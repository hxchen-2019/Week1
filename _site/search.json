[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "Welcome to  IS415 Geospatial Analytics and Applications \nThis is the course website of IS415. I am studying this term and I want to die."
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex2/Hands-on-Ex2.html",
    "href": "lessons/Hands-on/Hands-on-Ex2/Hands-on-Ex2.html",
    "title": "Hands-on-2",
    "section": "",
    "text": "pacman::p_load(sf, tidyverse)\n\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Hands-on\\Hands-on-Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Hands-on\\Hands-on-Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2248 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\npreschool = st_read(\"data/geospatial/preschools-location.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Hands-on\\Hands-on-Ex2\\data\\geospatial\\preschools-location.kml' \n  using driver `KML'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\n\nplot(st_geometry(mpsz))\n\n\n\n\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\nWarning: st_crs<- : replacing crs does not reproject data; use st_transform for\nthat\n\n\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\n\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\n\nRows: 4161 Columns: 75\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (26): listing_url, source, name, description, neighborhood_overview, pi...\ndbl  (37): id, scrape_id, host_id, host_listings_count, host_total_listings_...\nlgl   (7): host_is_superhost, host_has_profile_pic, host_identity_verified, ...\ndate  (5): last_scraped, host_since, calendar_last_scraped, first_review, la...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nlist(listings) \n\n[[1]]\n# A tibble: 4,161 × 75\n       id listing_url    scrap…¹ last_scr…² source name  descr…³ neigh…⁴ pictu…⁵\n    <dbl> <chr>            <dbl> <date>     <chr>  <chr> <chr>   <chr>   <chr>  \n 1  50646 https://www.a… 2.02e13 2022-09-23 previ… Plea… Fully … The se… https:…\n 2  71609 https://www.a… 2.02e13 2022-09-22 city … Ensu… For 3 … <NA>    https:…\n 3  71896 https://www.a… 2.02e13 2022-09-22 city … B&B … <b>The… <NA>    https:…\n 4  71903 https://www.a… 2.02e13 2022-09-22 city … Room… Like y… Quiet … https:…\n 5 275344 https://www.a… 2.02e13 2022-09-22 city … 15 m… Lovely… Bus st… https:…\n 6 289234 https://www.a… 2.02e13 2022-09-22 city … Book… This w… A quie… https:…\n 7 294281 https://www.a… 2.02e13 2022-09-22 city … 5 mi… I have… <NA>    https:…\n 8 324945 https://www.a… 2.02e13 2022-09-22 city … Cozy… <b>The… <NA>    https:…\n 9 330089 https://www.a… 2.02e13 2022-09-22 city … Cozy… A unit… <NA>    https:…\n10 330095 https://www.a… 2.02e13 2022-09-22 city … 10 m… Cosy, … Near I… https:…\n# … with 4,151 more rows, 66 more variables: host_id <dbl>, host_url <chr>,\n#   host_name <chr>, host_since <date>, host_location <chr>, host_about <chr>,\n#   host_response_time <chr>, host_response_rate <chr>,\n#   host_acceptance_rate <chr>, host_is_superhost <lgl>,\n#   host_thumbnail_url <chr>, host_picture_url <chr>, host_neighbourhood <chr>,\n#   host_listings_count <dbl>, host_total_listings_count <dbl>,\n#   host_verifications <chr>, host_has_profile_pic <lgl>, …\n\n\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\n\nglimpse(listings_sf)\n\nRows: 4,161\nColumns: 74\n$ id                                           <dbl> 50646, 71609, 71896, 7190…\n$ listing_url                                  <chr> \"https://www.airbnb.com/r…\n$ scrape_id                                    <dbl> 2.022092e+13, 2.022092e+1…\n$ last_scraped                                 <date> 2022-09-23, 2022-09-22, …\n$ source                                       <chr> \"previous scrape\", \"city …\n$ name                                         <chr> \"Pleasant Room along Buki…\n$ description                                  <chr> \"Fully furnished bedroom …\n$ neighborhood_overview                        <chr> \"The serenity & quiet sur…\n$ picture_url                                  <chr> \"https://a0.muscache.com/…\n$ host_id                                      <dbl> 227796, 367042, 367042, 3…\n$ host_url                                     <chr> \"https://www.airbnb.com/u…\n$ host_name                                    <chr> \"Sujatha\", \"Belinda\", \"Be…\n$ host_since                                   <date> 2010-09-08, 2011-01-29, …\n$ host_location                                <chr> \"Singapore\", \"Singapore\",…\n$ host_about                                   <chr> \"I am a working professio…\n$ host_response_time                           <chr> \"a few days or more\", \"wi…\n$ host_response_rate                           <chr> \"0%\", \"100%\", \"100%\", \"10…\n$ host_acceptance_rate                         <chr> \"N/A\", \"100%\", \"100%\", \"1…\n$ host_is_superhost                            <lgl> FALSE, FALSE, FALSE, FALS…\n$ host_thumbnail_url                           <chr> \"https://a0.muscache.com/…\n$ host_picture_url                             <chr> \"https://a0.muscache.com/…\n$ host_neighbourhood                           <chr> \"Bukit Timah\", \"Tampines\"…\n$ host_listings_count                          <dbl> 1, 6, 6, 6, 44, 6, 7, 44,…\n$ host_total_listings_count                    <dbl> 4, 15, 15, 15, 57, 15, 8,…\n$ host_verifications                           <chr> \"['email', 'phone', 'work…\n$ host_has_profile_pic                         <lgl> TRUE, TRUE, TRUE, TRUE, T…\n$ host_identity_verified                       <lgl> TRUE, TRUE, TRUE, TRUE, T…\n$ neighbourhood                                <chr> \"Singapore, Singapore\", N…\n$ neighbourhood_cleansed                       <chr> \"Bukit Timah\", \"Tampines\"…\n$ neighbourhood_group_cleansed                 <chr> \"Central Region\", \"East R…\n$ property_type                                <chr> \"Private room in rental u…\n$ room_type                                    <chr> \"Private room\", \"Private …\n$ accommodates                                 <dbl> 2, 6, 1, 2, 1, 4, 2, 1, 1…\n$ bathrooms                                    <lgl> NA, NA, NA, NA, NA, NA, N…\n$ bathrooms_text                               <chr> \"1 bath\", \"1 private bath…\n$ bedrooms                                     <dbl> 1, 2, 1, 1, 1, 3, 1, 1, N…\n$ beds                                         <dbl> 1, 3, 1, 2, 1, 5, 1, 1, 1…\n$ amenities                                    <chr> \"[\\\"Gym\\\", \\\"Washer\\\", \\\"…\n$ price                                        <chr> \"$80.00\", \"$145.00\", \"$85…\n$ minimum_nights                               <dbl> 92, 92, 92, 92, 60, 92, 9…\n$ maximum_nights                               <dbl> 730, 1125, 1125, 1125, 99…\n$ minimum_minimum_nights                       <dbl> 92, 92, 92, 92, 60, 92, 9…\n$ maximum_minimum_nights                       <dbl> 92, 92, 92, 92, 60, 92, 9…\n$ minimum_maximum_nights                       <dbl> 730, 1125, 1125, 1125, 99…\n$ maximum_maximum_nights                       <dbl> 730, 1125, 1125, 1125, 99…\n$ minimum_nights_avg_ntm                       <dbl> 92, 92, 92, 92, 60, 92, 9…\n$ maximum_nights_avg_ntm                       <dbl> 730, 1125, 1125, 1125, 99…\n$ calendar_updated                             <lgl> NA, NA, NA, NA, NA, NA, N…\n$ has_availability                             <lgl> TRUE, TRUE, TRUE, TRUE, T…\n$ availability_30                              <dbl> 30, 5, 0, 30, 0, 0, 30, 5…\n$ availability_60                              <dbl> 60, 35, 1, 60, 0, 0, 60, …\n$ availability_90                              <dbl> 90, 65, 1, 90, 21, 10, 90…\n$ availability_365                             <dbl> 365, 340, 265, 365, 296, …\n$ calendar_last_scraped                        <date> 2022-09-23, 2022-09-22, …\n$ number_of_reviews                            <dbl> 18, 20, 24, 47, 14, 12, 1…\n$ number_of_reviews_ltm                        <dbl> 0, 0, 0, 0, 1, 0, 0, 3, 2…\n$ number_of_reviews_l30d                       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ first_review                                 <date> 2014-04-18, 2011-12-19, …\n$ last_review                                  <date> 2014-12-26, 2020-01-17, …\n$ review_scores_rating                         <dbl> 4.56, 4.44, 4.16, 4.41, 4…\n$ review_scores_accuracy                       <dbl> 4.72, 4.37, 4.22, 4.39, 4…\n$ review_scores_cleanliness                    <dbl> 4.78, 4.00, 4.09, 4.52, 4…\n$ review_scores_checkin                        <dbl> 4.78, 4.63, 4.43, 4.63, 4…\n$ review_scores_communication                  <dbl> 4.94, 4.78, 4.43, 4.64, 4…\n$ review_scores_location                       <dbl> 4.72, 4.26, 4.17, 4.50, 4…\n$ review_scores_value                          <dbl> 4.50, 4.32, 4.04, 4.36, 4…\n$ license                                      <chr> NA, NA, NA, NA, \"S0399\", …\n$ instant_bookable                             <lgl> FALSE, FALSE, TRUE, FALSE…\n$ calculated_host_listings_count               <dbl> 1, 6, 6, 6, 44, 6, 7, 44,…\n$ calculated_host_listings_count_entire_homes  <dbl> 0, 0, 0, 0, 2, 0, 1, 2, 2…\n$ calculated_host_listings_count_private_rooms <dbl> 1, 6, 6, 6, 42, 6, 6, 42,…\n$ calculated_host_listings_count_shared_rooms  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reviews_per_month                            <dbl> 0.18, 0.15, 0.18, 0.34, 0…\n$ geometry                                     <POINT [m]> POINT (22646.02 351…\n\n\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\n\nsum(buffer_cycling$AREA)\n\n1556978 [m^2]\n\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    3.00    5.96    9.00   58.00 \n\n\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           58\n\n\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex3/Hands-on-Ex3.html",
    "href": "lessons/Hands-on/Hands-on-Ex3/Hands-on-Ex3.html",
    "title": "Hands-on-Ex3",
    "section": "",
    "text": "Installing Dependencies\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\nImporting Geospatial Data into R\n\nmpsz <- st_read(dsn = \"data/geospatial/\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Hands-on\\Hands-on-Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nImporting Aspatial Data\n\npopdata <- read_csv(\"data/aspatial/respopagesexfa2011to2020.csv\")\n\n\n\nWrangling Data\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\nJoining Datasets\n\n\n\n\n\n\nNote\n\n\n\nNeed to investigate on .funs as it is depreciated if necessary\n\n\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nConverting to RDS\n\n\n\n\n\n\nNote\n\n\n\nNeed to ensure that folder has been created\n\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\nMap Plotting\n\n\n\n\n\n\nNote\n\n\n\nCreated using qtm method\n\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCreated using tmap\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDrawing map using choropleth tm_polygons()\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDraw using tm_shape\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDraw using tm_shape but defining the border\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\nPlotting using tmap\n\n\n\n\n\n\nNote\n\n\n\nbuilt using built in classification method\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUsing different classes/\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSummary\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6540  0.7063  0.7712  0.7657 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nUsing different colour\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nChanging Appearance\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent Map Style\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCalling out different methods\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nResetting Style\n\n\n\ntmap_style(\"white\")\n\n\n\nDrawing Multiple Small Maps\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCreating Multiple Stand Alone Map\n\n\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMapping only a small portion of the map only\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html",
    "title": "Hands on Excerise 4",
    "section": "",
    "text": "pacman::p_load(maptools, sf, raster, spatstat, tmap)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#importing-spatial-data",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#importing-spatial-data",
    "title": "Hands on Excerise 4",
    "section": "2.1 Importing Spatial Data",
    "text": "2.1 Importing Spatial Data\n\nchildcare_sf <- st_read(\"data/geospatial/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Hands-on\\Hands-on-Ex4\\data\\geospatial\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf <- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\") %>%\n  st_transform(crs = 3414)\n\nReading layer `CostalOutline' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Hands-on\\Hands-on-Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Hands-on\\Hands-on-Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-to-spatial-class",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-to-spatial-class",
    "title": "Hands on Excerise 4",
    "section": "4.1 Converting to Spatial Class",
    "text": "4.1 Converting to Spatial Class\n\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>018989</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td>0</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td>0</td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>08F73931F4A691F4</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20200826094036</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \nmax values  : kml_999,                  <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>829646</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td>Child Care Services</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td>0</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td>0</td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>RAFFLES KIDZ @ PUNGGOL PTE LTD</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>379D017BF244B0FA</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20200826094036</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-to-generic-sp-format",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-to-generic-sp-format",
    "title": "Hands on Excerise 4",
    "section": "4.2 Converting to Generic SP Format",
    "text": "4.2 Converting to Generic SP Format\n\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-into-spatstatss-ppp-format",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-into-spatstatss-ppp-format",
    "title": "Hands on Excerise 4",
    "section": "4.3 Converting into spatstats’s ppp format",
    "text": "4.3 Converting into spatstats’s ppp format\n\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\nplot(childcare_ppp)\n\n\n\n\n\nsummary(childcare_ppp)\n\nPlanar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#viewing-the-duplicate",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#viewing-the-duplicate",
    "title": "Hands on Excerise 4",
    "section": "5.1 Viewing the Duplicate",
    "text": "5.1 Viewing the Duplicate\n\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\ntmap_mode('plot')"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#remove-duplicate-with-jittering-method",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#remove-duplicate-with-jittering-method",
    "title": "Hands on Excerise 4",
    "section": "5.2 Remove Duplicate with jittering method",
    "text": "5.2 Remove Duplicate with jittering method\n\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#creating-owin-object",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#creating-owin-object",
    "title": "Hands on Excerise 4",
    "section": "5.3 Creating Owin Object",
    "text": "5.3 Creating Owin Object\n\nsg_owin <- as(sg_sp, \"owin\")\n\n\nplot(sg_owin)\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#combining-point-events-and-own-obbject",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#combining-point-events-and-own-obbject",
    "title": "Hands on Excerise 4",
    "section": "5.4 Combining Point events and own obbject",
    "text": "5.4 Combining Point events and own obbject\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nsummary(childcareSG_ppp)\n\nPlanar point pattern:  1545 points\nAverage intensity 2.063463e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414\n\n\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#computing-kernal-density-automatically",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#computing-kernal-density-automatically",
    "title": "Hands on Excerise 4",
    "section": "6.1 Computing Kernal Density automatically",
    "text": "6.1 Computing Kernal Density automatically\n\nkde_childcareSG_bw <- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe density of the density is determined in meters, therefore it might be difficult to understand the value\n\n\n\nbw <- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n298.4095"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#rescalling-kde-value",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#rescalling-kde-value",
    "title": "Hands on Excerise 4",
    "section": "6.2 Rescalling KDE Value",
    "text": "6.2 Rescalling KDE Value\n\n\n\n\n\n\nNote\n\n\n\nRescaling help to make the number more understandable to the people.\n\n\n\nchildcareSG_ppp.km <- rescale(childcareSG_ppp, 1000, \"km\")\n\n\nkde_childcareSG.bw <- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#working-with-different-methods",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#working-with-different-methods",
    "title": "Hands on Excerise 4",
    "section": "6.3 Working with different methods",
    "text": "6.3 Working with different methods\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224898 1.450966 \n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\n\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.2984095 \n\n\n\nkde_childcareSG.ppl <- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#working-with-different-kernal-methods",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#working-with-different-kernal-methods",
    "title": "Hands on Excerise 4",
    "section": "6.4 Working with different kernal methods",
    "text": "6.4 Working with different kernal methods\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#computation-using-adaptive-bandwidth",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#computation-using-adaptive-bandwidth",
    "title": "Hands on Excerise 4",
    "section": "7.1 Computation using Adaptive Bandwidth",
    "text": "7.1 Computation using Adaptive Bandwidth\n\nkde_childcareSG_adaptive <- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nComparing\n\n\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-kde-to-grid-object",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-kde-to-grid-object",
    "title": "Hands on Excerise 4",
    "section": "7.2 Converting KDE to grid Object",
    "text": "7.2 Converting KDE to grid Object\n\n\n\n\n\n\nNote\n\n\n\nImages is practically useless, therefore we need to convert it into a KDE object.\n\n\n\ngridded_kde_childcareSG_bw <- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-to-raster-from-grid-object",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#converting-to-raster-from-grid-object",
    "title": "Hands on Excerise 4",
    "section": "7.3 Converting to Raster from Grid Object",
    "text": "7.3 Converting to Raster from Grid Object\n\nkde_childcareSG_bw_raster <- raster(gridded_kde_childcareSG_bw)\n\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -8.476185e-15, 28.51831  (min, max)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#assigning-projection-systems",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#assigning-projection-systems",
    "title": "Hands on Excerise 4",
    "section": "7.4 Assigning Projection Systems",
    "text": "7.4 Assigning Projection Systems\n\nprojection(kde_childcareSG_bw_raster) <- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +init=EPSG:3414 \nsource     : memory\nnames      : v \nvalues     : -8.476185e-15, 28.51831  (min, max)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#visualising-in-tmap",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#visualising-in-tmap",
    "title": "Hands on Excerise 4",
    "section": "7.5 Visualising in Tmap",
    "text": "7.5 Visualising in Tmap\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)##"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#comparing-spatial-point-pattern",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#comparing-spatial-point-pattern",
    "title": "Hands on Excerise 4",
    "section": "7.6 Comparing Spatial Point Pattern",
    "text": "7.6 Comparing Spatial Point Pattern\n\n7.6.1 Extracting the area\n\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n7.6.2 Converting to Generic SP Format\n\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n7.6.3 Creating Owin Object\n\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n7.6.4 Combining Points and Stydu area\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n\nchildcare_pg_ppp\n\nPlanar point pattern: 61 points\nwindow: polygonal boundary\nenclosing rectangle: [33952.59, 38889.96] x [40874.37, 44808.89] units\n\n\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n7.6.5 Computing KDE\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#testing-with-clar-and-evans",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#testing-with-clar-and-evans",
    "title": "Hands on Excerise 4",
    "section": "8.1 Testing with Clar and Evans",
    "text": "8.1 Testing with Clar and Evans\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 99 simulations of CSR with fixed n\n\ndata:  childcareSG_ppp\nR = 0.54756, p-value = 0.01\nalternative hypothesis: clustered (R < 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nPossible Conclusion that it might not be randomly distributed."
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#clark-and-evans-test-on-cck-planning-area",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#clark-and-evans-test-on-cck-planning-area",
    "title": "Hands on Excerise 4",
    "section": "8.2 Clark and Evans Test On CCK Planning Area",
    "text": "8.2 Clark and Evans Test On CCK Planning Area\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 999 simulations of CSR with fixed n\n\ndata:  childcare_ck_ppp\nR = 0.95457, p-value = 0.16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#clark-and-evans-test-on-tampinese-planning-area",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#clark-and-evans-test-on-tampinese-planning-area",
    "title": "Hands on Excerise 4",
    "section": "8.3 Clark and Evans Test On Tampinese Planning Area",
    "text": "8.3 Clark and Evans Test On Tampinese Planning Area\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 999 simulations of CSR with fixed n\n\ndata:  childcare_tm_ppp\nR = 0.77684, p-value = 0.002\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#choa-chu-kang-area",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#choa-chu-kang-area",
    "title": "Hands on Excerise 4",
    "section": "9.1 Choa Chu Kang Area",
    "text": "9.1 Choa Chu Kang Area\n\n9.1.1 Computing G Function estimation\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n9.1.2 Performing Complete Spatial Randomness Test\n\nG_CK.csr <- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60........\n.70.........80.........90.........100.........110.........120.........130......\n...140.........150.........160.........170.........180.........190.........200....\n.....210.........220.........230.........240.........250.........260.........270..\n.......280.........290.........300.........310.........320.........330.........340\n.........350.........360.........370.........380.........390.........400........\n.410.........420.........430.........440.........450.........460.........470......\n...480.........490.........500.........510.........520.........530.........540....\n.....550.........560.........570.........580.........590.........600.........610..\n.......620.........630.........640.........650.........660.........670.........680\n.........690.........700.........710.........720.........730.........740........\n.750.........760.........770.........780.........790.........800.........810......\n...820.........830.........840.........850.........860.........870.........880....\n.....890.........900.........910.........920.........930.........940.........950..\n.......960.........970.........980.........990........ 999.\n\nDone.\n\n\n\nplot(G_CK.csr)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#tampinese-planning-area",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#tampinese-planning-area",
    "title": "Hands on Excerise 4",
    "section": "9.2 Tampinese Planning Area",
    "text": "9.2 Tampinese Planning Area\n\n9.2.1 Computing G Function\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n9.2.2 Performing Complete Spatial Randomness Test\n\nG_tm.csr <- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60........\n.70.........80.........90.........100.........110.........120.........130......\n...140.........150.........160.........170.........180.........190.........200....\n.....210.........220.........230.........240.........250.........260.........270..\n.......280.........290.........300.........310.........320.........330.........340\n.........350.........360.........370.........380.........390.........400........\n.410.........420.........430.........440.........450.........460.........470......\n...480.........490.........500.........510.........520.........530.........540....\n.....550.........560.........570.........580.........590.........600.........610..\n.......620.........630.........640.........650.........660.........670.........680\n.........690.........700.........710.........720.........730.........740........\n.750.........760.........770.........780.........790.........800.........810......\n...820.........830.........840.........850.........860.........870.........880....\n.....890.........900.........910.........920.........930.........940.........950..\n.......960.........970.........980.........990........ 999.\n\nDone.\n\n\n\nplot(G_tm.csr)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#choa-chu-kang",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#choa-chu-kang",
    "title": "Hands on Excerise 4",
    "section": "10.1 Choa Chu Kang",
    "text": "10.1 Choa Chu Kang\n\n10.1.1 Computing F Function Estimate\n\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n10.1.2 Complete Spatial Randomness Test\n\nF_CK.csr <- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60........\n.70.........80.........90.........100.........110.........120.........130......\n...140.........150.........160.........170.........180.........190.........200....\n.....210.........220.........230.........240.........250.........260.........270..\n.......280.........290.........300.........310.........320.........330.........340\n.........350.........360.........370.........380.........390.........400........\n.410.........420.........430.........440.........450.........460.........470......\n...480.........490.........500.........510.........520.........530.........540....\n.....550.........560.........570.........580.........590.........600.........610..\n.......620.........630.........640.........650.........660.........670.........680\n.........690.........700.........710.........720.........730.........740........\n.750.........760.........770.........780.........790.........800.........810......\n...820.........830.........840.........850.........860.........870.........880....\n.....890.........900.........910.........920.........930.........940.........950..\n.......960.........970.........980.........990........ 999.\n\nDone.\n\n\n\nplot(F_CK.csr)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#tampines",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#tampines",
    "title": "Hands on Excerise 4",
    "section": "10.2 Tampines",
    "text": "10.2 Tampines\n\n10.2.1 Computing F Function Estimate\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n10.2.2 Complete Spatial Randomness Test\n\nF_tm.csr <- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60........\n.70.........80.........90.........100.........110.........120.........130......\n...140.........150.........160.........170.........180.........190.........200....\n.....210.........220.........230.........240.........250.........260.........270..\n.......280.........290.........300.........310.........320.........330.........340\n.........350.........360.........370.........380.........390.........400........\n.410.........420.........430.........440.........450.........460.........470......\n...480.........490.........500.........510.........520.........530.........540....\n.....550.........560.........570.........580.........590.........600.........610..\n.......620.........630.........640.........650.........660.........670.........680\n.........690.........700.........710.........720.........730.........740........\n.750.........760.........770.........780.........790.........800.........810......\n...820.........830.........840.........850.........860.........870.........880....\n.....890.........900.........910.........920.........930.........940.........950..\n.......960.........970.........980.........990........ 999.\n\nDone.\n\n\n\nplot(F_tm.csr)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#choa-chu-kang-1",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#choa-chu-kang-1",
    "title": "Hands on Excerise 4",
    "section": "11.1 Choa Chu Kang",
    "text": "11.1 Choa Chu Kang\n\n11.1.1 Computing K Function Estimate\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n11.1.2 Complete Spatial Randomness Test\n\nK_ck.csr <- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#tampines-1",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#tampines-1",
    "title": "Hands on Excerise 4",
    "section": "11.2 Tampines",
    "text": "11.2 Tampines\n\n11.2.1 Computing K Function Estimate\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n11.2.2 Complete Spatial Randomness Test\n\nK_tm.csr <- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#choa-chu-kang-2",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#choa-chu-kang-2",
    "title": "Hands on Excerise 4",
    "section": "12.1 Choa Chu Kang",
    "text": "12.1 Choa Chu Kang\n\n12.1.1 Computing L Function Estimate\n\n\n\n\n\n\nNote\n\n\n\nThis is a plot without simulation, therefore this is an estimate.\nCorrection extends the boundary so that we can see more the point at the edge.\n\n\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n12.1.2 Complete Spatial Randomness Test\n\n\n\n\n\n\nNote\n\n\n\nThe envelope method perform the monte carlo simulation of the point. the number of simulation is based on the confidence level.\n\n\n\nL_ck.csr <- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBased on the graph below. As long as the line is below boundary, it is statistically insignificant. In the Prof example, the distance between 480 and 580, is statistically significant to proof that there is clustering\n\n\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#tampines-2",
    "href": "lessons/Hands-on/Hands-on-Ex4/Hands-on-Ex4.html#tampines-2",
    "title": "Hands on Excerise 4",
    "section": "12.2 Tampines",
    "text": "12.2 Tampines\n\n12.2.1 Computing L Function Estimate\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n12.2.2 Complete Spatial Randomness Test\n\nL_tm.csr <- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98,  99.\n\nDone.\n\n\n\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html",
    "title": "Hands on Excerise 6",
    "section": "",
    "text": "pacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\n\n\n\nNote\n\n\n\nOne thing to note is that we are using in this package is the old package of spdep, refer to in class excercise for the more updated st packages"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#importing-geospatial-data",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#importing-geospatial-data",
    "title": "Hands on Excerise 6",
    "section": "2.1 Importing GeoSpatial Data",
    "text": "2.1 Importing GeoSpatial Data\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Hands-on\\Hands-on-Ex6\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#importing-the-csv-file",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#importing-the-csv-file",
    "title": "Hands on Excerise 6",
    "section": "2.2 Importing the CSV File",
    "text": "2.2 Importing the CSV File\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#performing-a-relational-join-on-the-dataset",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#performing-a-relational-join-on-the-dataset",
    "title": "Hands on Excerise 6",
    "section": "2.3 Performing a Relational Join on the dataset",
    "text": "2.3 Performing a Relational Join on the dataset\n\n\n\n\n\n\nNote\n\n\n\nIn this case we will be making use of the left_join packages to help us perform a left join. There is no need to indicate the name of the column to perform a join\n\n\n\nhunan <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#queen-based-neighbours",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#queen-based-neighbours",
    "title": "Hands on Excerise 6",
    "section": "4.1 Queen Based Neighbours",
    "text": "4.1 Queen Based Neighbours\n\nwm_q <- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n4.1.1 Accessing the different polygons\nThe code junk below helps us to identify which of the polygons are related to each other\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nHowever, it is kind of pointless in this case, as it is better for us to know the name of the polygons\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nNow that we know the name, we need to know the data that we have left joined before.\n\nnb1 <- wm_q[[1]]\nnb1 <- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nWhat if we want to view all the weight matrix.\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#rook-based",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#rook-based",
    "title": "Hands on Excerise 6",
    "section": "4.2 Rook Based",
    "text": "4.2 Rook Based\n\n\n\n\n\n\nNote\n\n\n\nThis is for root, there is almost no difference for the steps except at the start\n\n\n\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#visualizing-the-contiguity-weight",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#visualizing-the-contiguity-weight",
    "title": "Hands on Excerise 6",
    "section": "4.3 Visualizing the Contiguity Weight",
    "text": "4.3 Visualizing the Contiguity Weight\nTo Visualized we would need the coordinates of each of the maps, We would need to calculate the centroid of each of the maps, to do so we would need to make use of the longitude and latitude\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\n\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\ncoords <- cbind(longitude, latitude)\n\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n4.3.1 Plotting Neighbours map\nNow that we have the coordinates, we can convert it to a map for easy visualization\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\nWith that we can plot the Rook Based one as well\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\", main=\"Queen Contiguity\")\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\", main=\"Rook Contiguity\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#determine-the-cut-of-distance.",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#determine-the-cut-of-distance.",
    "title": "Hands on Excerise 6",
    "section": "5.1 Determine the cut of distance.",
    "text": "5.1 Determine the cut of distance.\nWell we need to determine the cutoff distance by using the steps below. (This is from Hands on Ex6)\n\n#coords <- coordinates(hunan)\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#computing-the-fixed-distance-weight-matrix",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#computing-the-fixed-distance-weight-matrix",
    "title": "Hands on Excerise 6",
    "section": "5.2 Computing the fixed distance weight matrix",
    "text": "5.2 Computing the fixed distance weight matrix\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nNote\n\n\n\nAverage Number of Links is the average number of neighbours each coordinate has.\n\n\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother weigh of displaying\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\nn_comp <- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\n\n\ntable(n_comp$comp.id)\n\n\n 1 \n88"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#plotting-fixed-distanc-eweight-matrix",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#plotting-fixed-distanc-eweight-matrix",
    "title": "Hands on Excerise 6",
    "section": "5.3 Plotting fixed distanc eweight matrix",
    "text": "5.3 Plotting fixed distanc eweight matrix\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternative methods can be used as well.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08, main=\"1st nearest neighbours\")\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6, main=\"Distance link\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#computing-adaptive-distance-weight-matrix",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#computing-adaptive-distance-weight-matrix",
    "title": "Hands on Excerise 6",
    "section": "5.4 Computing adaptive distance weight matrix",
    "text": "5.4 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nWell we can make use of the adaptive distance weight matrix to help us in this.\n\nknn6 <- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\""
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#plotting-distance-based-neighbours",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#plotting-distance-based-neighbours",
    "title": "Hands on Excerise 6",
    "section": "5.5 Plotting distance based neighbours",
    "text": "5.5 Plotting distance based neighbours\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#row-standardised-weigh-tmatrix",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#row-standardised-weigh-tmatrix",
    "title": "Hands on Excerise 6",
    "section": "6.1 Row Standardised weigh tmatrix",
    "text": "6.1 Row Standardised weigh tmatrix\nNow that we have the Weight Matrix we would need to standardised with it.\n\n\n\n\n\n\nNote\n\n\n\nStyle “W” is used for simplicity, but we can make used of other weights, notably style B as well.\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\n\n\n\nrswm_q <- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nWe can see that each of the neigbour is assigned a value of 0.125 of the total weight.\nWe can derived the row standardised distance of the weight matrix by using the code below.\n\nrswm_ids <- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#spatial-lag-with-row-standardized-weight",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#spatial-lag-with-row-standardized-weight",
    "title": "Hands on Excerise 6",
    "section": "7.1 Spatial lag with row standardized weight",
    "text": "7.1 Spatial lag with row standardized weight\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\nGDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nWe can retrieved the GDPPC with htis value\n\nnb1 <- wm_q[[1]]\nnb1 <- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nNow we will append the values into the sf dataframe\n\nlag.list <- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res <- as.data.frame(lag.list)\ncolnames(lag.res) <- c(\"NAME_3\", \"lag GDPPC\")\nhunan <- left_join(hunan,lag.res)\n\nNow we can see the values\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nWe can plot the GDPPC and spatial lag GDPPC for comparison\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_gdppc <- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#spatial-lag-as-a-sum-of-neighbouring-values.",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#spatial-lag-as-a-sum-of-neighbouring-values.",
    "title": "Hands on Excerise 6",
    "section": "7.2 Spatial Lag as a sum of neighbouring values.",
    "text": "7.2 Spatial Lag as a sum of neighbouring values.\nWe can calculate spatial lag as a sum of the neigbouring values by assigning binary weights\n\nb_weights <- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 <- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nNow that the weights are assigned we can compute the lag variable\n\nlag_sum <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res <- as.data.frame(lag_sum)\ncolnames(lag.res) <- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nWe will now append the calculated value into the hunan sf dataframe\n\nhunan <- left_join(hunan, lag.res)\n\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc <- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#spatial-window-average",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#spatial-window-average",
    "title": "Hands on Excerise 6",
    "section": "7.3 Spatial Window Average",
    "text": "7.3 Spatial Window Average\nThe spatial window average uses row-standardized weights and includes the diagonal element.\n\nwm_qs <- include.self(wm_q)\n\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nWe will now obtain the weights with nb2listw()\n\nwm_qs <- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAfterwards we need to assign weight balues and create the lag variable/\n\nlag_w_avg_gpdpc <- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nWe will now covnert the lag variable list object into a dataframe\n\nlag.list.wm_qs <- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res <- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) <- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nWe will now join with our hunan dataframe\n\nhunan <- left_join(hunan, lag_wm_qs.res)\n\nNow that we got both values why not we compare the difference in values\n\nhunan %>%\n  select(\"County\", \"lag GDPPC\", \"lag_window_avg GDPPC\") %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nAfter we can plot it for easier visualiztion.\n\nw_avg_gdppc <- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#spatial-sum",
    "href": "lessons/Hands-on/Hands-on-Ex6/Hands-on-Ex6.html#spatial-sum",
    "title": "Hands on Excerise 6",
    "section": "7.4 Spatial Sum",
    "text": "7.4 Spatial Sum\nThis is the opposuite to the window average, but use without row-standardised weights.\nWe need to include the diagonal neighbour this can be done with the weight queen\n\nwm_qs <- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext we will assign binary weights to the neighbour struvcture.\n\nb_weights <- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNow we can assignt he weight values\n\nb_weights2 <- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWe can now compute the lag variable\n\nw_sum_gdppc <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nOnce again we will convert it into a dataframe. We will join it with the hunan dataframe\n\nw_sum_gdppc.res <- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) <- c(\"NAME_3\", \"w_sum GDPPC\")\n\n\nhunan <- left_join(hunan, w_sum_gdppc.res)\n\nWhy not we compare the values as well\n\nhunan %>%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %>%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nWe will view it for easier viewing\n\nw_sum_gdppc <- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html",
    "title": "Hands on Excerise 7",
    "section": "",
    "text": "pacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#visualization",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#visualization",
    "title": "Hands on Excerise 7",
    "section": "2.1 Visualization",
    "text": "2.1 Visualization\n\nequal <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#computing-contiguity-spatial-weights",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#computing-contiguity-spatial-weights",
    "title": "Hands on Excerise 7",
    "section": "3.1 Computing Contiguity Spatial Weights",
    "text": "3.1 Computing Contiguity Spatial Weights\nWe will be making use of the Queen Contiguity Weight Matrix\n\nwm_q <- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#row-standardised-weight-matrix",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#row-standardised-weight-matrix",
    "title": "Hands on Excerise 7",
    "section": "3.2 Row Standardised weight matrix",
    "text": "3.2 Row Standardised weight matrix\nAfterwards we need to standardised the rows.\n\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#morans-i-test",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#morans-i-test",
    "title": "Hands on Excerise 7",
    "section": "4.1 Moran’s I Test",
    "text": "4.1 Moran’s I Test\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nWe can assume that they are highly idependant as the p value is smaller than 0.01.\n\n4.1.1 Computing Monte Carlo Morans.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nWe can assume that they are highly idependant as the p value is smaller than 0.01.\n\n\n4.1.2 Visualizing Monte Carlo Morans I\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\nThe Morans value resemble a Normal Distribution"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#geary-c-test",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#geary-c-test",
    "title": "Hands on Excerise 7",
    "section": "4.2 Geary C Test",
    "text": "4.2 Geary C Test\n\n4.2.1 The Geary C Test\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nWe can assume that they are highly idependant as the p value is smaller than 0.01.\n\n\n4.2.2 Computing Monte Carlo Geary’s C\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nWe can assume that they are highly idependant as the p value is smaller than 0.01.\n\n\n4.2.3 Visualising Monte Carlo Geary’s C\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\nThe Geary value resemble a Normal Distribution"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#spatial-correlogram",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#spatial-correlogram",
    "title": "Hands on Excerise 7",
    "section": "4.3 Spatial Correlogram",
    "text": "4.3 Spatial Correlogram\n\n4.3.1 Moran’s I\n\nMI_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nThe plot might not really give us the complete interpretation as such we need to see the full report;\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n4.3.2 Geary’s C\n\nGC_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#computing-local-morans-i",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#computing-local-morans-i",
    "title": "Hands on Excerise 7",
    "section": "5.1 Computing Local Moran’s I",
    "text": "5.1 Computing Local Moran’s I\n\nfips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\n\n\n\n\n\n\nNote\n\n\n\nlocalmoran() function returns calue\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\n\n\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#mapping-the-local-morans-i",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#mapping-the-local-morans-i",
    "title": "Hands on Excerise 7",
    "section": "5.2 Mapping the Local Moran’s I",
    "text": "5.2 Mapping the Local Moran’s I\nBefore mapping the local Morans’s I map, we would need to append it to the Hunan Dataframe\n\nhunan.localMI <- cbind(hunan,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#mapping-local-morans-i-p-values",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#mapping-local-morans-i-p-values",
    "title": "Hands on Excerise 7",
    "section": "5.3 Mapping local Moran’s I p-values",
    "text": "5.3 Mapping local Moran’s I p-values\nThe graph above shows evidence for both positive and negative li value, but we would need to use the local Moran’s p-values to tell.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nWe can then compare both the maps\n\nlocalMI.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#plotting-moran-scatter-plot",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#plotting-moran-scatter-plot",
    "title": "Hands on Excerise 7",
    "section": "6.1 Plotting Moran Scatter plot",
    "text": "6.1 Plotting Moran Scatter plot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\n\nnci <- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA Moran Scatter plot has 4 quadrants. The top right corners signifies the high GDPPC areas surrounded by those that are averaged."
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#plotting-moran-scatterplot-with-standardised-variable",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#plotting-moran-scatterplot-with-standardised-variable",
    "title": "Hands on Excerise 7",
    "section": "6.2 Plotting Moran Scatterplot with standardised variable",
    "text": "6.2 Plotting Moran Scatterplot with standardised variable\nWe will first need to scale the variables first. Afterwards, to ensure that the data type we get is a vector, we need to include the as.vector function().\n\nhunan$Z.GDPPC <- scale(hunan$GDPPC) %>% \n  as.vector \n\n\nnci2 <- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#preparing-lisa-map-cluster",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#preparing-lisa-map-cluster",
    "title": "Hands on Excerise 7",
    "section": "6.3 Preparing Lisa Map Cluster",
    "text": "6.3 Preparing Lisa Map Cluster\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\n\nWe will need to derived the spatially lagged variable of interest and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)\nDV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)   \n\n\nLM_I <- localMI[,1] - mean(localMI[,1])    \n\nWe will then set the statistical significance level for the local Moran\n\nsignif <- 0.05       \n\nThe followign command define the 4 categories into\n\nLow-Low\nLow-High\nHigh-Low\nHigh-High\n\n\nquadrant[DV <0 & LM_I>0] <- 1\nquadrant[DV >0 & LM_I<0] <- 2\nquadrant[DV <0 & LM_I<0] <- 3  \nquadrant[DV >0 & LM_I>0] <- 4      \n\nAnd we place those that are not significant in another category.\n\nquadrant[localMI[,5]>signif] <- 0\n\nAlternatively, we can combine them all into one."
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#plotting-lisa-map",
    "href": "lessons/Hands-on/Hands-on-Ex7/Hands-on-Ex7.html#plotting-lisa-map",
    "title": "Hands on Excerise 7",
    "section": "6.4 Plotting LISA Map",
    "text": "6.4 Plotting LISA Map\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nWe should plot the local Moran’s values and its corresponding p-values map next to each other to compare\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex2/In-class_Ex02.html",
    "href": "lessons/In-class/in-class_ex2/In-class_Ex02.html",
    "title": "In Class Excercise 2",
    "section": "",
    "text": "Note\n\n\n\nInstalling all the different packages\n\n\n\n\nShow the Code\npacman::p_load(sf, tidyverse, funModeling)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe want to import the RGA data file\n\n\n\n\nShow the Code\ngeoNGA <- st_read(\"data/geospatial/\", layer = \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\nSame Information being loaded but from different sources. They have the same number of observable records but different number of variable. This data is preferred as the data provide State Data and LGA as well.\n\n\n\n\nShow the Code\nNGA <- st_read(\"data/geospatial/\", layer = \"nga_admbnda_adm2_osgof_20190417\") %>%\n  st_transform(crs = 26392)\n\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is 2 function i the read_csv function tehre is 2\n\n\n\nwp_nga <- read_csv(\"data/aspatial/Wpdx.csv\") %>% \n  filter(`#clean_country_name` == \"Nigeria\")\n\n\n\n1) For\n\n\n\n\n\n\nNote\n\n\n\nYou can take the latitude degree and the longitude degree to convert sf point. (Please reference Hands on Excersie 2 for the code).\nNew Georeference Column store tb\n\n\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\nwp_nga\n\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nExtract from tibbler dataframe into a sf dataframe. Converting from tibbler to sf dataframe require the projection information. (\n\n\n\nwp_sf <- st_sf (wp_nga, crs=4326)\nwp_sf\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nTransform the projection to wgs84\n\n\n\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)\n\nwp_sf\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 28907.91 ymin: 33736.93 xmax: 1293293 ymax: 1092883\nProjected CRS: Minna / Nigeria Mid Belt\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …"
  },
  {
    "objectID": "lessons/In-class/in-class_ex2/In-class_Ex02.html#geospatial-data-cleaning",
    "href": "lessons/In-class/in-class_ex2/In-class_Ex02.html#geospatial-data-cleaning",
    "title": "In Class Excercise 2",
    "section": "2 Geospatial Data Cleaning",
    "text": "2 Geospatial Data Cleaning\n\n2.1 Excluding the redundent fields\n\n\n\n\n\n\nNote\n\n\n\nRetain column 3,4,8,9 only.\n\n\n\nNGA <- NGA %>%\n  select(c(3:4, 8:9))\n\n\n\n2.2 Check for duplicate name\n\n\n\n\n\n\nNote\n\n\n\nThis step is to check for the quality of the data. This is to check for if any field if there is any duplicate\n\n\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\n\nIn this case there appears to be 6 duplicated LGA.\n\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifellodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\ncharacter(0)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex2/In-class_Ex02.html#data-wrangling-for-water-data-point",
    "href": "lessons/In-class/in-class_ex2/In-class_Ex02.html#data-wrangling-for-water-data-point",
    "title": "In Class Excercise 2",
    "section": "3 Data Wrangling for Water Data Point",
    "text": "3 Data Wrangling for Water Data Point\n\nfreq(data = wp_sf, input = \"#status_clean\")\n\n\n\n\n                     #status_clean frequency percentage cumulative_perc\n1                       Functional     45883      48.29           48.29\n2                   Non-Functional     29385      30.93           79.22\n3                             <NA>     10656      11.22           90.44\n4      Functional but needs repair      4579       4.82           95.26\n5 Non-Functional due to dry season      2403       2.53           97.79\n6        Functional but not in use      1686       1.77           99.56\n7         Abandoned/Decommissioned       234       0.25           99.81\n8                        Abandoned       175       0.18           99.99\n9 Non functional due to dry season         7       0.01          100.00\n\n\n\n\n\n\n\n\nNote\n\n\n\nmutate allow us to perform data preprocessing\n\n\n\nwp_sf_nga <- wp_sf %>%\n  rename(status_clean = \"#status_clean\") %>%\n  select(status_clean) %>%\n  mutate(status_clean = replace_na(status_clean, \"unknown\"))\n\n\n3.1 Extracting water Point Data\n\n3.1.1 Functional\n\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n        c(\"Functional\",\n          \"Functional but not in use\",\n          \"Functional but needs repair\"))\n\n\n\n3.1.2 Non Functional\n\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n        c(\"Abandoned/Decommissioned\",\n          \"Abandoned\",\n          \"Non-Functional due to dry season\",\n          \"Non-Functional\",\n          \"Non functional due to dry season\"))\n\n\n\n3.1.3 Unknown\n\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"unknown\"))\n\n\n\n3.1.4 Combining into a single dataframe\n\n\n\n\n\n\nNote\n\n\n\nThis give the total number of waterpoints that falls inside each NGA by comparing the dataset with theorginal NGA dataset and appending it into a new field in NGA_wp\n\n\n\nNGA_wp <- NGA %>%\n  mutate(`total_wp` = lengths( \n    st_intersects(NGA, wp_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(NGA, wp_functional))) %>%\n  mutate(`non_functional` = lengths(\n    st_intersects(NGA, wp_nonfunctional))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(NGA, wp_unknown)))\n\n\n\n\n3.2 Saving the analytical data in RDS format\n\nwrite_rds(NGA_wp,\"data/rds/NGA_wp.rds\")\n\n\n\n3.3 Plotting\n\nggplot(data = NGA_wp, aes(x= total_wp)) +\n  geom_histogram(bins=20, color=\"black\", fill = \"light blue\") +\n  geom_vline(aes(xintercept=mean(total_wp, na.rm=T)), color =\"red\",linetype=\"dashed\", size=0.8) + \n  ggtitle(\"Distribution of total water points by LGA\")+\n  xlab(\"No. of water points\") + \n  ylab(\"No of \\nLGAs\") + \n  theme(axis.title.y=element_text(angle = 0))"
  },
  {
    "objectID": "lessons/In-class/in-class_ex3/In-class_Ex03.html",
    "href": "lessons/In-class/in-class_ex3/In-class_Ex03.html",
    "title": "In Class Excercise 3",
    "section": "",
    "text": "pacman::p_load(tmap,tidyverse,sf)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex3/In-class_Ex03.html#importing-the-data",
    "href": "lessons/In-class/in-class_ex3/In-class_Ex03.html#importing-the-data",
    "title": "In Class Excercise 3",
    "section": "2 Importing the data",
    "text": "2 Importing the data\n\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "lessons/In-class/in-class_ex3/In-class_Ex03.html#basic-mapping",
    "href": "lessons/In-class/in-class_ex3/In-class_Ex03.html#basic-mapping",
    "title": "In Class Excercise 3",
    "section": "3 Basic Mapping",
    "text": "3 Basic Mapping\n\n3.1 Visualisization\n\n\n\n\n\n\nNote\n\n\n\ntm_shape is to access shape, it will not give a picture of the map. tm_fill and tm_border is used to draw the shape og the map\n\n\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = TRUE)\n\np1\n\n\n\n\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Greens\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total water point by LGAs\",\n            legend.outside = TRUE)\n\np2\n\n\n\n\n\ntmap_arrange(p2, p1, nrow=1)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex3/In-class_Ex03.html#plotting-map-of-rate",
    "href": "lessons/In-class/in-class_ex3/In-class_Ex03.html#plotting-map-of-rate",
    "title": "In Class Excercise 3",
    "section": "4 Plotting Map of Rate",
    "text": "4 Plotting Map of Rate\n\n\n\n\n\n\nNote\n\n\n\nCalculating Rate\n\n\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = non_functional/total_wp)\n\n\np3 <- tm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Purples\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate Map of water point by LGAs\",\n            legend.outside = TRUE)\n\np3\n\n\n\n\n\np4 <- tm_shape(NGA_wp) +\n  tm_fill(\"pct_nonfunctional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Reds\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate Map of Non Functional water point by LGAs\",\n            legend.outside = TRUE)\n\np4\n\n\n\n\n\ntmap_arrange(p4, p3, nrow=1)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex3/In-class_Ex03.html#extreme-values-maps",
    "href": "lessons/In-class/in-class_ex3/In-class_Ex03.html#extreme-values-maps",
    "title": "In Class Excercise 3",
    "section": "5 Extreme Values Maps",
    "text": "5 Extreme Values Maps\n\n5.1 Percentile Map\n\n\n\n\n\n\nNote\n\n\n\nThis is a map with sizx specific catergories. Breakpoints can be dervied bym means of the base R quanbtile command, passing an explicit vector of cumulative probabilities. the start and end point must be included\n\n\n\n5.1.1 Data Preperation\nStep 1: Exclude records with NA\n\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\nStep 2: Creating customised classification and extracting the values\n\n\n\n\n\n\nNote\n\n\n\nst_set_geometry must be set to null otherwise there will be an issue with the code chunk\n\n\n\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n5.1.2 Creating Functions\n\n\n\n\n\n\nNote\n\n\n\nFunction to get name\n\n\n\nget.var <- function(vname, df){\n  v <- df[vname] %>%\n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n\n\n\n\nNote\n\n\n\nCreating Functions to draw maps out\n\n\n\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam,df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n    tm_polygons() +\n    tm_shape(df) +\n    tm_fill(vnam,\n            title= legtitle,\n            breaks = bperc,\n            palette=\"Spectral\",\n            labels = c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% to 99%\", \"99% to 100%\")) +\n    tm_borders() +\n    tm_layout(main.title = mtitle,\n              title.position = c(\"right\", \"bottom\"),\n              legend.outside = TRUE)\n  \n}\n\n\n\n5.1.3 Plotting the Map\n\npercentmap(\"wp_functional\",NGA_wp)\n\n\n\n\n\npercentmap(\"non_functional\",NGA_wp)\n\n\n\n\n\n\n\n5.2 Box Plot\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = non_functional)) +\n  geom_boxplot()\n\n\n\n\n\n\n5.3 Box Break Function\n\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\n\n5.4 Create Get Var Function\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\nvar <- get.var(\"non_functional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n5.5 Box Map Function\n\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"non_functional\", NGA_wp)\n\n\n\n\n\n\n5.6 Recode Zero\n\nNGA_wp <- NGA_wp %>%\n  mutate(wp_functional = na_if(\n    total_wp, total_wp < 0))"
  },
  {
    "objectID": "lessons/In-class/in-class_ex4/In-class_Ex04.html",
    "href": "lessons/In-class/in-class_ex4/In-class_Ex04.html",
    "title": "In Class Exercise 4",
    "section": "",
    "text": "1 Loading the package\n\npacman::p_load(maptools, sf, raster, spatstat, tmap)\n\nThings to learn outside from this code chunk.\n\n\n2 Load Dataset\n\nchildcare_sf <- st_read(\"data/geospatial/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex4\\data\\geospatial\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThis is already a projected coordinate system. This is converted to meters.\n\n\n\n\n\n\nNote\n\n\n\nFor all spatial analysis, all data must be projected coordinate system.\n\n\n\nsg_sf <- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nNote\n\n\n\ntmap_mode(“plot”) will give a static map\ntmap_mode(“view”) will give a interactive map\ntmap is an extension of leaflet\nalpha is to introduce a certain level of opacity\ntm_shape is needed to create a new layer. It needs to be at the start of everyplot\n\n\n\ntmap_mode('view')\ntm_shape(childcare_sf) +\n  tm_dots(alph = 0.5, size=0.01) \n\n\n\n\n\n\n\n2.0.1 Converting to Spatial Dataframe\n\n\n\n\n\n\nNote\n\n\n\nSpatial Polygon has a data table\n\n\n\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\n\n2.0.2 Converting to Spatial Object\n\n\n\n\n\n\nNote\n\n\n\nThis only retains the geometric\n\n\n\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\n\n2.0.3 Converting to PPP format\n\n\n\n\n\n\nNote\n\n\n\nThis drops everything else into a list of coordinates that can only be read by the ST package\n\n\n\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\n\n\n3 Additional Map\n\ntmap_mode('view')\ntm_shape(childcare_sf) +\n  tm_dots(alph = 0.5, size=0.01) +\n  tm_view(set.zoom.limits = c(11,14), set.bounds = TRUE)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex5/data/stores.html",
    "href": "lessons/In-class/in-class_ex5/data/stores.html",
    "title": "IS415-GAA-HX",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     \n\n\n        0 0     false"
  },
  {
    "objectID": "lessons/In-class/in-class_ex5/data/study_area.html",
    "href": "lessons/In-class/in-class_ex5/data/study_area.html",
    "title": "IS415-GAA-HX",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "lessons/In-class/in-class_ex5/In-class_Ex05.html",
    "href": "lessons/In-class/in-class_ex5/In-class_Ex05.html",
    "title": "In Class Exercise 5",
    "section": "",
    "text": "1 Getting Started\n\npacman::p_load(tidyverse, tmap, sf, sfdep)\n\n\n\n2 Importing Data\n\n\n\n\n\n\nNote\n\n\n\nTaiwan has 2 projection system\n\n\n\nstudyArea <- st_read(dsn = \"data\",\n                     layer=\"study_area\") %>%\n  st_transform (crs =3829)\n\nReading layer `study_area' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex5\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 121.4836 ymin: 25.00776 xmax: 121.592 ymax: 25.09288\nGeodetic CRS:  TWD97\n\n\n\nstores <- st_read(dsn = \"data\",\n                     layer=\"stores\") %>%\n  st_transform (crs =3829)\n\nReading layer `stores' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex5\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1409 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 121.4902 ymin: 25.01257 xmax: 121.5874 ymax: 25.08557\nGeodetic CRS:  TWD97\n\n\n\ntmap_mode(\"view\")\ntm_shape(studyArea)+\n  tm_polygons() +\n  tm_shape(stores) +\n  tm_dots(col=\"Name\",\n          size = 0.01,\n          border.col=\"black\",\n          border.lwd=0.5)+\n  tm_view(set.zoom.limits = c(12,16))\n\n\n\n\n\n\n\n\n3 Local Colocation Quotients (LCLQ)\n\n\n\n\n\n\nNote\n\n\n\n6 nearest neighbour is used. Include Self is needed as it help to boost the number to an odd number.\nThis is an adaptive method. Weights of the stores will be stored when st_knernel_weights is used\n\n\n\nnb <- include_self(\n  st_knn(st_geometry(stores),6)\n)\n\n\nwt <- st_kernel_weights(nb,\n                        stores,\n                        \"gaussian\",\n                        adaptive = TRUE)\n\n\nFamilyMart <- stores %>%\n  filter(Name == \"Family Mart\")\nA <- FamilyMart$Name\n\n\nSevenEleven <- stores %>%\n  filter(Name == \"7-Eleven\")\nB <- SevenEleven$Name\n\n\n\n\n\n\n\nNote\n\n\n\nA is the target\nB is the neightbour\nnb is a list of nearest neighbour\nwt is the weight\n49 is the simulation runs\nThe simulation will run 50 Times, and will generate a different p-value\nNA is common in local_colocation()\n\n\n\nLCLQ <- local_colocation (A, B, nb, wt, 49)\n\n\n\n\n\n\n\nNote\n\n\n\nThere is no unique idenitfier therefore, the only code is cbind. cbind cannot work if the results is sorted.\n\n\n\nLCLQ_stores <- cbind(stores, LCLQ)\n\n\ntmap_mode(\"view\")+\n  tm_shape(studyArea)+\n  tm_polygons() +\n  tm_shape(LCLQ_stores) +\n  tm_dots(col=\"X7.Eleven\",\n          size = 0.01,\n          border.col=\"black\",\n          border.lwd=0.5)+\n  tm_shape(stores)+\n  tm_dots(col=\"Name\",\n          size = 0.01,\n          border.col=\"black\",\n          border.lwd=0.5)\n\n\n\n\n\n  tm_view(set.zoom.limits = c(12,16))\n\n$tm_layout\n$tm_layout$set.zoom.limits\n[1] 12 16\n\n$tm_layout$style\n[1] NA\n\n\nattr(,\"class\")\n[1] \"tm\""
  },
  {
    "objectID": "lessons/In-class/in-class_ex6/In-class_Ex06.html",
    "href": "lessons/In-class/in-class_ex6/In-class_Ex06.html",
    "title": "In Class Exercise 6",
    "section": "",
    "text": "Important\n\n\n\nWe will be using sfdep in In-Class-Excercise instead of spded in Hands On Excercise"
  },
  {
    "objectID": "lessons/In-class/in-class_ex6/In-class_Ex06.html#importing-the-geospatial-data",
    "href": "lessons/In-class/in-class_ex6/In-class_Ex06.html#importing-the-geospatial-data",
    "title": "In Class Exercise 6",
    "section": "2.1 Importing the Geospatial data",
    "text": "2.1 Importing the Geospatial data\n\n\n\n\n\n\nNote\n\n\n\nThis file is loaded as a sf dataframe\n\n\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex6\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "lessons/In-class/in-class_ex6/In-class_Ex06.html#importing-geospatial",
    "href": "lessons/In-class/in-class_ex6/In-class_Ex06.html#importing-geospatial",
    "title": "In Class Exercise 6",
    "section": "2.2 Importing GeoSpatial",
    "text": "2.2 Importing GeoSpatial\n\n\n\n\n\n\nNote\n\n\n\nThis input is recorded as tibble dataframe\n\n\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n2.2.1 Combining both data frame by using left join\n\n\n\n\n\n\nNote\n\n\n\nIn order to retain the geospatial properties, the left data frame must be the sf data frame.\nWe need to make sure that the name is the same, it is Case-Sensitive!!\n\n\n\nhunan_GDPPC <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)\n\n\n\n2.2.2 Plotting a Choropleth map\n\ntmap_mode(\"plot\") \ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\", \n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex6/In-class_Ex06.html#identify-area-neighbours",
    "href": "lessons/In-class/in-class_ex6/In-class_Ex06.html#identify-area-neighbours",
    "title": "In Class Exercise 6",
    "section": "2.3 Identify area Neighbours",
    "text": "2.3 Identify area Neighbours\n\n2.3.1 Calculating Contiguit neighbour methods\n\n\n\n\n\n\nNote\n\n\n\nThe code junk below uses st_contiguity is used to derive a contiguity neighbour list using Queen’s method.\nThe code chunk below will store the results into a new sf data frame at the first column (.before = 1_\n\n\n\n\n2.3.2 Queens Methods\n\nnb_queen <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\n\nsummary(nb_queen$nb)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nWe can view the first 10 values\n\nnb_queen\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb   NAME_2  ID_3    NAME_3   ENGTYPE_3\n1                 2, 3, 4, 57, 85  Changde 21098   Anxiang      County\n2               1, 57, 58, 78, 85  Changde 21100   Hanshou      County\n3                     1, 4, 5, 85  Changde 21101    Jinshi County City\n4                      1, 3, 5, 6  Changde 21102        Li      County\n5                     3, 4, 6, 85  Changde 21103     Linli      County\n6                4, 5, 69, 75, 85  Changde 21104    Shimen      County\n7                  67, 71, 74, 84 Changsha 21109   Liuyang County City\n8       9, 46, 47, 56, 78, 80, 86 Changsha 21110 Ningxiang      County\n9           8, 66, 68, 78, 84, 86 Changsha 21111 Wangcheng      County\n10 16, 17, 19, 20, 22, 70, 72, 73 Chenzhou 21112     Anren      County\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\nLets review the name of the neighbouring polygons\n\nnb_queen$County[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\n\n\n2.3.3 Rooks Methods\n\nnb_rook <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         .before = 1)\n\n\n\n2.3.4 Indentifying Higher order neighbours\n\nnb2_queen <-  hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\n\n\nnb2_queen\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                        nb2\n1                                     2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\n2                           1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\n3                                                 1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\n4                                                             1, 2, 3, 5, 6, 57, 69, 75, 85\n5                                                 1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\n6                                         1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\n7                                                     9, 19, 66, 67, 71, 73, 74, 76, 84, 86\n8  2, 9, 19, 21, 31, 32, 34, 35, 36, 41, 45, 46, 47, 56, 58, 66, 68, 74, 78, 80, 84, 85, 86\n9               2, 7, 8, 19, 21, 35, 46, 47, 56, 58, 66, 67, 68, 74, 76, 78, 80, 84, 85, 86\n10               11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 70, 71, 72, 73, 74, 82, 83, 86\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "lessons/In-class/in-class_ex6/In-class_Ex06.html#computing-contiguity-weights",
    "href": "lessons/In-class/in-class_ex6/In-class_Ex06.html#computing-contiguity-weights",
    "title": "In Class Exercise 6",
    "section": "2.4 Computing contiguity weights",
    "text": "2.4 Computing contiguity weights\n\n2.4.1 Queens Method\n\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n2.4.2 Rooks Method\n\nwm_r <- hunan %>%\n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         wt = st_weights(nb),\n         .before = 1)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex6/In-class_Ex06.html#deriving-fixed-distance-weight",
    "href": "lessons/In-class/in-class_ex6/In-class_Ex06.html#deriving-fixed-distance-weight",
    "title": "In Class Exercise 6",
    "section": "3.1 Deriving fixed distance weight",
    "text": "3.1 Deriving fixed distance weight\n\ngeo <- sf::st_geometry(hunan_GDPPC)\nnb <- st_knn(geo, longlat = TRUE)\ndists <- unlist(st_nb_dists(geo, nb))\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_nb_dists() of sfdep is used to calculate the nearest neighbour distance. The output is a list of distances for each observation’s neighbors list.\nunlist() of Base R is then used to return the output as a vector so that the summary statistics of the nearest neighbour distances can be derived\n\n\n\n\nsummary(dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.56   29.11   36.89   37.34   43.21   65.80 \n\n\nSince the max value is 65.80, we can use the threshold value of 66km.\nIn the function there is 2 things to note:\n\nst_dists_band() of sfdep is used to identify neighbors based on a distance band (i.e. 66km). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors.\n\n\n\nwm_fd <- hunan_GDPPC %>%\n  mutate(nb = st_dist_band(geometry,\n                           upper = 66),\n               wt = st_weights(nb),\n               .before = 1)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex6/In-class_Ex06.html#deriving-adaptive-distance-weight",
    "href": "lessons/In-class/in-class_ex6/In-class_Ex06.html#deriving-adaptive-distance-weight",
    "title": "In Class Exercise 6",
    "section": "3.2 Deriving adaptive distance weight",
    "text": "3.2 Deriving adaptive distance weight\nThere is a few things to note in the function used to compute the adaptive distance weighjt\n\nst_knn() of sfdep is used to identify neighbors based on k (i.e. k = 8 indicates the nearest eight neighbours). The output is a list of neighbours (i.e. nb).\nst_weights() is then used to calculate polygon spatial weights of the nb list. Note that:\n\nthe default style argument is set to “W” for row standardized weights, and\nthe default allow_zero is set to TRUE, assigns zero as lagged value to zone without neighbors\n\n\n\nwm_ad <- hunan_GDPPC %>% \n  mutate(nb = st_knn(geometry,\n                     k=8),\n         wt = st_weights(nb),\n               .before = 1)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex6/In-class_Ex06.html#calculate-inverse-distance-weight",
    "href": "lessons/In-class/in-class_ex6/In-class_Ex06.html#calculate-inverse-distance-weight",
    "title": "In Class Exercise 6",
    "section": "3.3 Calculate inverse distance weight",
    "text": "3.3 Calculate inverse distance weight\nWhen calculatinf inverse distance weight , this are variable to note.\n\nst_contiguity() of sfdep is used to identify the neighbours by using contiguity criteria. The output is a list of neighbours (i.e. nb).\nst_inverse_distance() is then used to calculate inverse distance weights of neighbours on the nb list.\n\n\nwm_idw <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html",
    "title": "In Class Exercise 7",
    "section": "",
    "text": "Important\n\n\n\nWe will be using sfdep in In-Class-Excercise instead of spded in Hands On Excercise"
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#importing-the-geospatial-data",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#importing-the-geospatial-data",
    "title": "In Class Exercise 7",
    "section": "2.1 Importing the Geospatial data",
    "text": "2.1 Importing the Geospatial data\n\n\n\n\n\n\nNote\n\n\n\nThis file is loaded as a sf dataframe\n\n\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex7\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#importing-geospatial",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#importing-geospatial",
    "title": "In Class Exercise 7",
    "section": "2.2 Importing GeoSpatial",
    "text": "2.2 Importing GeoSpatial\n\n\n\n\n\n\nNote\n\n\n\nThis input is recorded as tibble dataframe\n\n\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\n2.2.1 Combining both data frame by using left join\n\n\n\n\n\n\nNote\n\n\n\nIn order to retain the geospatial properties, the left data frame must be the sf data frame.\nWe need to make sure that the name is the same, it is Case-Sensitive!!\n\n\n\nhunan_GDPPC <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)\n\n\n\n2.2.2 Plotting a Choropleth map\n\ntmap_mode(\"plot\") \ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by district, Hunan Province\", \n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#identify-area-neighbours",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#identify-area-neighbours",
    "title": "In Class Exercise 7",
    "section": "2.3 Identify area Neighbours",
    "text": "2.3 Identify area Neighbours\n\n2.3.1 Calculating Contiguit neighbour methods\n\n\n\n\n\n\nNote\n\n\n\nThe code junk below uses st_contiguity is used to derive a contiguity neighbour list using Queen’s method.\nThe code chunk below will store the results into a new sf data frame at the first column (.before = 1_\n\n\n\n\n2.3.2 Queens Methods\n\nnb_queen <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         .before = 1)\n\n\nsummary(nb_queen$nb)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nWe can view the first 10 values\n\nnb_queen\n\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb   NAME_2  ID_3    NAME_3   ENGTYPE_3\n1                 2, 3, 4, 57, 85  Changde 21098   Anxiang      County\n2               1, 57, 58, 78, 85  Changde 21100   Hanshou      County\n3                     1, 4, 5, 85  Changde 21101    Jinshi County City\n4                      1, 3, 5, 6  Changde 21102        Li      County\n5                     3, 4, 6, 85  Changde 21103     Linli      County\n6                4, 5, 69, 75, 85  Changde 21104    Shimen      County\n7                  67, 71, 74, 84 Changsha 21109   Liuyang County City\n8       9, 46, 47, 56, 78, 80, 86 Changsha 21110 Ningxiang      County\n9           8, 66, 68, 78, 84, 86 Changsha 21111 Wangcheng      County\n10 16, 17, 19, 20, 22, 70, 72, 73 Chenzhou 21112     Anren      County\n      County GDPPC                       geometry\n1    Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\nLets review the name of the neighbouring polygons\n\nnb_queen$County[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\n\n\n2.3.3 Rooks Methods\n\nnb_rook <- hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry,\n                            queen = FALSE),\n         .before = 1)\n\n\n\n2.3.4 Indentifying Higher order neighbours\n\nnb2_queen <-  hunan_GDPPC %>% \n  mutate(nb = st_contiguity(geometry),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\n\n\nnb2_queen\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                        nb2\n1                                     2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\n2                           1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\n3                                                 1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\n4                                                             1, 2, 3, 5, 6, 57, 69, 75, 85\n5                                                 1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\n6                                         1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\n7                                                     9, 19, 66, 67, 71, 73, 74, 76, 84, 86\n8  2, 9, 19, 21, 31, 32, 34, 35, 36, 41, 45, 46, 47, 56, 58, 66, 68, 74, 78, 80, 84, 85, 86\n9               2, 7, 8, 19, 21, 35, 46, 47, 56, 58, 66, 67, 68, 74, 76, 78, 80, 84, 85, 86\n10               11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 70, 71, 72, 73, 74, 82, 83, 86\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#computing-contiguity-weights",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#computing-contiguity-weights",
    "title": "In Class Exercise 7",
    "section": "2.4 Computing contiguity weights",
    "text": "2.4 Computing contiguity weights\n\n2.4.1 Queens Method\n\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\nwm_q\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#computing-global-morans-i",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#computing-global-morans-i",
    "title": "In Class Exercise 7",
    "section": "2.5 Computing Global Moran’s I",
    "text": "2.5 Computing Global Moran’s I\n\n\n\n\n\n\nNote\n\n\n\nWe usually do not perform a Moran Step by itself, we usually perform the global Moran Test\n\n\n\nmoranI <- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#performing-global-morani-test",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#performing-global-morani-test",
    "title": "In Class Exercise 7",
    "section": "2.6 Performing Global Moran’I test",
    "text": "2.6 Performing Global Moran’I test\n\nglobal_moran_test(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nWe have enough statistical evidence to reject the null hypothesis that the GDP is spatially independence. If we refer to the Moran I value is positive, therefore it shows signs that is it 00.\n\n2.6.1 Peroforming Global Moran’s I permutation Test\n\n\n\n\n\n\nNote\n\n\n\nMust perform Set seed to ensure reproducibility.\n\n\n\nset.seed(1234)\n\nIf the dataset is small, we can increase teh number of simulation for stability.\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#computing-local-morans-i",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#computing-local-morans-i",
    "title": "In Class Exercise 7",
    "section": "2.7 Computing Local Moran’s I",
    "text": "2.7 Computing Local Moran’s I\nWithout the unnest, if you tried to plot it, we will not be able to plot it.\n\nlisa <- wm_q %>%\n  mutate(local_moran = local_moran(GDPPC,nb,wt,nsim=99),\n         .before = 1) %>%\n  unnest(local_moran)\nlisa\n\nSimple feature collection with 88 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 21\n         ii        eii   var_ii    z_ii    p_ii p_ii_…¹ p_fol…² skewn…³ kurtosis\n      <dbl>      <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n 1 -0.00147  0.00177    4.18e-4 -0.158  0.874      0.82    0.41  -0.812  0.652  \n 2  0.0259   0.00641    1.05e-2  0.190  0.849      0.96    0.48  -1.09   1.89   \n 3 -0.0120  -0.0374     1.02e-1  0.0796 0.937      0.76    0.38   0.824  0.0461 \n 4  0.00102 -0.0000349  4.37e-6  0.506  0.613      0.64    0.32   1.04   1.61   \n 5  0.0148  -0.00340    1.65e-3  0.449  0.654      0.5     0.25   1.64   3.96   \n 6 -0.0388  -0.00339    5.45e-3 -0.480  0.631      0.82    0.41   0.614 -0.264  \n 7  3.37    -0.198      1.41e+0  3.00   0.00266    0.08    0.04   1.46   2.74   \n 8  1.56    -0.265      8.04e-1  2.04   0.0417     0.08    0.04   0.459 -0.519  \n 9  4.42     0.0450     1.79e+0  3.27   0.00108    0.02    0.01   0.746 -0.00582\n10 -0.399   -0.0505     8.59e-2 -1.19   0.234      0.28    0.14  -0.685  0.134  \n# … with 78 more rows, 12 more variables: mean <fct>, median <fct>,\n#   pysal <fct>, nb <nb>, wt <list>, NAME_2 <chr>, ID_3 <int>, NAME_3 <chr>,\n#   ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>, geometry <POLYGON [°]>, and\n#   abbreviated variable names ¹​p_ii_sim, ²​p_folded_sim, ³​skewness\n\n\nIn General the Mean and the Pysal is the same, Use either Mean or Pysal. If we are using R, we use Mean. Pysal is the python method of processing.\nIt is the safest if we use mean. Median might be a better measure to use if the data does not follow a normal distribution."
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#visualising-local-morans-i",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#visualising-local-morans-i",
    "title": "In Class Exercise 7",
    "section": "2.8 Visualising Local Morans’ I",
    "text": "2.8 Visualising Local Morans’ I\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") +\n  tm_borders(alpha = 0.5) + \n  tm_view(set.zoom.limits=c(6,8))\n\n\n\n\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") +\n  tm_borders(alpha = 0.5) + \n  tm_view(set.zoom.limits=c(6,8))\n\n\n\n\nWe should use p_ii_sim or p_folded_sim, to plot the simulation, as those values are where the simulation are used.\n\nlisa_sig <- lisa %>%\n  filter(p_ii <0.05)\n\nThis is not a good way to do so, can make use of the Hands-On Exercise Code to show how the grey area as insignificant areas.\nThere is no need to use LISA.\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(lisa_sig) +\n  tm_fill(\"mean\") +\n  tm_borders(alpha=0.4)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#peroforming-emerging-hotspot-analysis",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#peroforming-emerging-hotspot-analysis",
    "title": "In Class Exercise 7",
    "section": "3.1 Peroforming Emerging Hotspot Analysis",
    "text": "3.1 Peroforming Emerging Hotspot Analysis\nWe need to arrange to something similar to the Hunan_GDPPC for performing Emerging Hotspot Analysis.\n\npacman::p_load(sf, sfdep, tmap, plotly, tidyverse, zoo, Kendall)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#importing-the-darta",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#importing-the-darta",
    "title": "In Class Exercise 7",
    "section": "3.2 Importing the Darta",
    "text": "3.2 Importing the Darta\n\nGDPPC <- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")\n\n\nGDPPC_st <- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\n\n3.2.1 Analysis\n\nGDPPC_nb <- GDPPC_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt=st_weights(nb)\n         ) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\n\ngi_stars <-GDPPC_nb %>%\n          group_by(Year) %>%\n  mutate(gi_star = local_gstar_perm(GDPPC, nb, wt, nsim=99)) %>%\n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex7/In-class_Ex07.html#mann_kendall-test",
    "href": "lessons/In-class/in-class_ex7/In-class_Ex07.html#mann_kendall-test",
    "title": "In Class Exercise 7",
    "section": "3.3 Mann_Kendall Test",
    "text": "3.3 Mann_Kendall Test\n\ncbg <- gi_stars %>%\n  ungroup() %>%\n  filter(County==\"Changsha\") |>\n  select(County, Year, gi_star)\n\n\nehsa <- emerging_hotspot_analysis(\n  x=GDPPC_st,\n  .var = \"GDPPC\",\n  k =1,\n  nsim = 99\n)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex8/In-class_Ex08.html",
    "href": "lessons/In-class/in-class_ex8/In-class_Ex08.html",
    "title": "In Class Exercise 8",
    "section": "",
    "text": "Important\n\n\n\nWe will be using sfdep in In-Class-Excercise instead of spded in Hands On Excercise"
  },
  {
    "objectID": "lessons/In-class/in-class_ex8/In-class_Ex08.html#importing-the-geospatial-data",
    "href": "lessons/In-class/in-class_ex8/In-class_Ex08.html#importing-the-geospatial-data",
    "title": "In Class Exercise 8",
    "section": "2.1 Importing the Geospatial data",
    "text": "2.1 Importing the Geospatial data\n\n\n\n\n\n\nNote\n\n\n\nThis file is loaded as a sf dataframe\n\n\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\In-class\\in-class_ex8\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz_svy21 <- st_transform(mpsz, 3414)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex8/In-class_Ex08.html#update-the-crs-model",
    "href": "lessons/In-class/in-class_ex8/In-class_Ex08.html#update-the-crs-model",
    "title": "In Class Exercise 8",
    "section": "2.2 Update the CRS Model",
    "text": "2.2 Update the CRS Model\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nst_bbox(mpsz) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "lessons/In-class/in-class_ex8/In-class_Ex08.html#reading-the-aspatial-data",
    "href": "lessons/In-class/in-class_ex8/In-class_Ex08.html#reading-the-aspatial-data",
    "title": "In Class Exercise 8",
    "section": "2.3 Reading the Aspatial Data",
    "text": "2.3 Reading the Aspatial Data\n\ncondo_resale = readr::read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\ndplyr::glimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             <dbl> 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            <dbl> 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             <dbl> 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n2.3.1 Converting aspatial dataframe into a sf object\n\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)"
  },
  {
    "objectID": "lessons/In-class/in-class_ex8/In-class_Ex08.html#multiple-histogram-plots",
    "href": "lessons/In-class/in-class_ex8/In-class_Ex08.html#multiple-histogram-plots",
    "title": "In Class Exercise 8",
    "section": "3.1 Multiple Histogram Plots",
    "text": "3.1 Multiple Histogram Plots\n\n## Need to fix all of t\n\n\n3.1.1 Drawing Statistical Point Map\n\ntmap_mode(\"view\")\n\n\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tmap_options(check.and.fix = TRUE)+\n  tm_dots(\n          col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "lessons/In-class/in-class_ex8/In-class_Ex08.html#simple-linear-regression",
    "href": "lessons/In-class/in-class_ex8/In-class_Ex08.html#simple-linear-regression",
    "title": "In Class Exercise 8",
    "section": "4.1 Simple Linear Regression",
    "text": "4.1 Simple Linear Regression\n\ncondo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nWe will plot it to see\n\n## Need to fix thios"
  },
  {
    "objectID": "lessons/In-class/in-class_ex8/In-class_Ex08.html#multiple-linear-regression",
    "href": "lessons/In-class/in-class_ex8/In-class_Ex08.html#multiple-linear-regression",
    "title": "In Class Exercise 8",
    "section": "4.2 Multiple Linear Regression",
    "text": "4.2 Multiple Linear Regression\nWe need to check the correlation first\n\ncorrplot::corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")"
  },
  {
    "objectID": "lessons/In-class/in-class_ex8/In-class_Ex08.html#building-a-hedonic-pricing-model-using-multiple-linear-regression-method",
    "href": "lessons/In-class/in-class_ex8/In-class_Ex08.html#building-a-hedonic-pricing-model-using-multiple-linear-regression-method",
    "title": "In Class Exercise 8",
    "section": "4.3 Building a hedonic pricing model using multiple linear regression method",
    "text": "4.3 Building a hedonic pricing model using multiple linear regression method\n\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  < 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  < 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  < 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  < 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: < 2.2e-16\n\n\n\n4.3.1 Preparing Publication Quality Table\n\ncondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                             Model Summary                               \n------------------------------------------------------------------------\nR                       0.807       RMSE                     755957.289 \nR-Squared               0.651       Coef. Var                    43.168 \nAdj. R-Squared          0.647       MSE                571471422208.591 \nPred R-Squared          0.638       MAE                      414819.628 \n------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n4.3.2 Preparing using GT SUmamry\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n4.3.3 Regression\n\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = <0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n4.3.4 CHecking for multicolinearity\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "Important\n\n\n\nThis context is taken from the IS415 Take Home Exercise 1\nAll rights belong to Dr Kam Tin Seong.\n\n\nWater is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis Objectives are taken from the IS415 Take Home Exercise 1\nAll rights belong to Dr Kam Tin Seong.\n\n\n\n\n\nDerive kernel density maps of functional and non-functional water points. Using appropriate tmap functions,\nDisplay the kernel density maps on openstreetmap of Osub State, Nigeria.\nDescribe the spatial patterns revealed by the kernel density maps. Highlight the advantage of kernel density map over point map.\n\n\n\n\nWith reference to the spatial point patterns observed in ESDA:\n\nFormulate the null hypothesis and alternative hypothesis and select the confidence level.\nPerform the test by using appropriate Second order spatial point patterns analysis technique.\nWith reference to the analysis results, draw statistical conclusions.\n\n\n\n\nIn this section, you are required to confirm statistically if the spatial distribution of functional and non-functional water points are independent from each other.\n\nFormulate the null hypothesis and alternative hypothesis and select the confidence level.\nPerform the test by using appropriate Second order spatial point patterns analysis technique.\nWith reference to the analysis results, draw statistical conclusions."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#packages",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#packages",
    "title": "Take Home Exercise 1",
    "section": "Packages",
    "text": "Packages\n\nsf: used for importing, managing, and processing geospatial data\ntidyverse: for performing data science tasks such as importing, wrangling and visualising data.\ntmap: used for creating thematic maps, such as choropleth and bubble maps\nspatstat: used for point pattern analysis\nraster: reads, writes, manipulates, analyses and models gridded spatial data (i.e. raster-based geographical data)\nmaptools: a set of tools for manipulating geographic data\nfunModeling: contains a set of functions related to exploratory data analysis, data preparation, and model performance"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#installing-and-loading-the-packages",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#installing-and-loading-the-packages",
    "title": "Take Home Exercise 1",
    "section": "Installing and Loading the Packages",
    "text": "Installing and Loading the Packages\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(maptools, sf, raster, spatstat, tmap, tidyverse, funModeling)\n\nThis prepares all the tools necessary for us to start or spatial analysis."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#dataset-used",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#dataset-used",
    "title": "Take Home Exercise 1",
    "section": "Dataset used",
    "text": "Dataset used\n2 datasets are used for this excercise\n\nThe First Dataset used would be the Level 2 Administrative Boundary which can be found either from Geoboundaries or Humanitarian Data Exchange\nWaterpoint Data Repositories is the dataset for the waterpoint"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#importing-geospatial-dataframe",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#importing-geospatial-dataframe",
    "title": "Take Home Exercise 1",
    "section": "Importing Geospatial Dataframe",
    "text": "Importing Geospatial Dataframe\n\n\n\n\n\n\nNote\n\n\n\nNeed to double check the CRS as it is depending on the system used by the country.\nSince the country we are focusing on is Nigeria. The EPSG code is 26392., and it encompasses the entire area of Nigeria.\n\n\nWe will be using the st_read() function from the sf package to read the data set. More information on st_read() can be found here..\nHowever, as the polygon data is not in the correct format, there will be a need to convert the geometric data to the correct form. st_transform from the sf package is used to so. More information on st_transform() can be found here\n\ngeoBoundaries data set\nThis dataset loads the boundaries of Nigeria from geoBoundaries\n\ngeoNGA <- st_read(\"data/geospatial/\",\n                  layer = \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Take-home\\Take-home_ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\ngeoNGA contains the following data:\n\n\n\nColumns Name\nDescription\n\n\n\n\nshapeName\nName of the Level 2 Boundaries\n\n\npcode\nUnique Code\n\n\nlevel\nADM2 (Indicating this is a Level 2 Boundaries)\n\n\nshapeID\nUnique Code of the Shape\n\n\nshapeGroup\nNGA (Indicating Nigeria)\n\n\nshapeType\nADM2 (Indicating this is a Level 2 Boundaries)\n\n\ngeometry\nPolygon Data\n\n\n\n\n\nNGA Data set (Humanitarian Data Exchange)\n\n\n\n\n\n\nNote\n\n\n\nThe NGA Dataset is essentially the same as geoBoundaries dataset with the exception that the dataset in geoBoundaries is more condense.\n\n\n\nNGA <- st_read(\"data/geospatial/\",\n               layer = \"nga_admbnda_adm2_osgof_20190417\") %>%\n  st_transform(crs = 26392)\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Take-home\\Take-home_ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nNGA contains the following data:\n\n\n\nColumn Name\nDescription\n\n\n\n\nShape_Leng\nLength of the Shape\n\n\nShape_Area\nArea of the Shape\n\n\nADM2_EN\nEnglish Name of ADM2\n\n\nADM2_PCODE\nUnique ID of the ADM2\n\n\nADM2_REF\nA Reference to ADM2_EN\n\n\nADM2ALT1EN\nAlternative English Name\n\n\nADM2ALT2EN\nAlternative English Name\n\n\nADM1_EN\nADM1 English Name\n\n\nADM1_PCODE\nUnique ID of ADM1\n\n\nADM0_EN\nADM0 English Name\n\n\nADM0_PCODE\nUnique ID of ADM0\n\n\ndate\nDate of the boundaries\n\n\nvalidOn\nValid Date of the Boundaries\n\n\nvalidTo\nEnd of Valid Date of the Boundaries\n\n\nSD_EN\nSenatorial District\n\n\nSD_PCODE\nUnique Code of the Senatorial District\n\n\ngeometry\nPolygon Data\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAs NGA seems to offer a more richer data set the rest of the analysis will be done on the NGA Data set"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#importing-aspatial-data",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#importing-aspatial-data",
    "title": "Take Home Exercise 1",
    "section": "Importing Aspatial Data",
    "text": "Importing Aspatial Data\n\nLoading the dataset from CSV\nThe next dataset that we will be loading would be the waterpoint dataset. As the dataset is found in the CSV another function read_csv(), which will import the csv as a tibble dataset. Read more about read_csv() from readr here.\n\n\n\n\n\n\nNote\n\n\n\nAs the CSV contain almost 70 variables and more than 10000 observations it would be better to filter the dataset to the country of interest, in this case, Nigeria. Read more about filter() from dplyr here.\n\n\n\nwp_nga <- read_csv(\"data/aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nRows: 406566 Columns: 70\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (43): #source, #report_date, #status_id, #water_source_clean, #water_sou...\ndbl (23): row_id, #lat_deg, #lon_deg, #install_year, #fecal_coliform_value, ...\nlgl  (4): #rehab_year, #rehabilitator, is_urban, latest_record\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nConverting the water point data into sf point feature.\nDespite loading the aspatial dataframe into a tibble data frame, we would need to convert the dataframe into an sf data frame for us to perform Geospatial Analysis.\nThe column “New Georeferenced Column” contain the spatial data is a well-known text representation of geometry, as the such the fuction st_as_sfc() can be used to convert that into a sfc object. Read more about st_as_sfc() from sf here. We will append the sfc object into a new Column called “Geometry”.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\nwp_nga\n\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nNow than we have a tibble data frame we would need to convert the data frame into a sf object using st_sf(). Read more about st_sf() here.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that the sfc object in the Geometry column does not contain the correct referencing system. There is a need to transform the projection into a WGS 84. The EPSG code is 4326.\n\n\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nMuch like the Handling of the Geospatial data above, there is a need to conver the WGS84 projection to the projection coordinate system of Nigeria as well.\n\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#excluding-redundent-fields",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#excluding-redundent-fields",
    "title": "Take Home Exercise 1",
    "section": "Excluding Redundent Fields",
    "text": "Excluding Redundent Fields\nTaking a look at the columns (NGA Data set (Humanitarian Data Exchange))of the NGA sf dataframe, we could identify most of the redundent fields. The only field that really matters would be the columns\n\n\n\n\n\n\n\nColumns to Keep\nReasons\n\n\n\n\nADM2_EN\nThis is the English Name of the ADM2. This is where the Local Government Area.\n\n\nADM2_PCODE\nThis is the unique identifier of ADM2\n\n\nADM1_EN\nThis is the English Name of the ADM1. This is where the States of Nigeria is.\n\n\nADM1_PCODE\nThis is the unique identifier of ADM1\n\n\n\n\nNGA <- NGA %>%\n  select(c(3:4, 8:9))"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#checking-for-duplicate-name",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#checking-for-duplicate-name",
    "title": "Take Home Exercise 1",
    "section": "Checking for Duplicate Name",
    "text": "Checking for Duplicate Name\nWe need to ensure that there is no duplicate name in the data. In this case, we only really care about checking for duplicate names in Local Government Area or ADM2. One method we can used to check for duplicated name would the used of the duplicated() function. Find out about the duplicated() from base R here.\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\n\nNow that we know that there are similarities in the name we would need to examine the duplicate field more closely. One method we can used to check if the duplicated data are the same would be to take a look at their unique pcode.\n\nNGA$ADM2_PCODE[duplicated(NGA$ADM2_PCODE)==TRUE]\n\ncharacter(0)\n\n\nNow that we have establised that each of ADM2_PCODE is different, we can determined that the ADM2 names are the same but are referencing different area. In this case, there will be a need to correct the names of the ADM2_EN so that there will be no duplicate data.\n\n\n\n\n\n\nTip\n\n\n\nA Google Search can be performed as well to double check they are indeed different area.\n\n\n\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nNow, we would need to confirm that the duplicated name issues has been addressed already.\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\ncharacter(0)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#extracting-the-water-point-data",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#extracting-the-water-point-data",
    "title": "Take Home Exercise 1",
    "section": "Extracting the Water Point Data",
    "text": "Extracting the Water Point Data\nWith some basic understanding of the water point, we can now categories the water point data. From the graph above we can categories it into the following method.\n\nThis is to extract functional water point\n\n\nWaterpoint Category\nStatus\n\n\n\n\nFunctional\nFunctional\n\n\nFunctional\nFunctional but not in use\n\n\nFunctional\nFunctional but needs repair\n\n\nNon Functional\nAbandoned/Decommissioned\n\n\nNon Functional\nAbandoned\n\n\nNon Functional\nNon-Functional due to dry season\n\n\nNon Functional\nNon-Functional\n\n\nNon Functional\nNon functional due to dry season\n\n\nUnknown\nUnknown\n\n\n\n\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\n\nwp_functional\n\nSimple feature collection with 52148 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 29322.63 ymin: 33758.37 xmax: 1218553 ymax: 1092629\nProjected CRS: Minna / Nigeria Mid Belt\n# A tibble: 52,148 × 2\n   status_clean            Geometry\n * <chr>                <POINT [m]>\n 1 Functional   (128394.3 330487.9)\n 2 Functional   (464684.4 94532.59)\n 3 Functional   (588792.3 74102.03)\n 4 Functional     (459153.3 171705)\n 5 Functional   (586703.9 75701.92)\n 6 Functional      (612461.7 87149)\n 7 Functional     (503439 87320.23)\n 8 Functional   (599467.7 92205.82)\n 9 Functional   (651470.8 101586.9)\n10 Functional   (650819.3 104796.9)\n# … with 52,138 more rows\n\n\nThis is to extract nonfunctional water point.\n\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Abandoned/Decommissioned\",\n             \"Abandoned\",\n             \"Non-Functional due to dry season\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\"))\n\n\nwp_nonfunctional\n\nSimple feature collection with 32204 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 28907.91 ymin: 33736.93 xmax: 1209690 ymax: 1092883\nProjected CRS: Minna / Nigeria Mid Belt\n# A tibble: 32,204 × 2\n   status_clean                        Geometry\n * <chr>                            <POINT [m]>\n 1 Abandoned/Decommissioned (578642.2 141523.1)\n 2 Abandoned/Decommissioned (571655.4 70856.98)\n 3 Abandoned/Decommissioned   (571629.5 143544)\n 4 Abandoned/Decommissioned (608748.8 141693.1)\n 5 Abandoned/Decommissioned (576876.2 66860.76)\n 6 Abandoned/Decommissioned   (698288 224655.8)\n 7 Abandoned/Decommissioned (698293.1 224809.4)\n 8 Abandoned/Decommissioned (341287.7 459644.6)\n 9 Abandoned/Decommissioned (402193.2 89488.33)\n10 Abandoned/Decommissioned (589410.8 147917.3)\n# … with 32,194 more rows\n\n\nThis is to extract unknown water point\n\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\n\nwp_unknown\n\nSimple feature collection with 10656 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 29143.21 ymin: 36660.5 xmax: 1293293 ymax: 965811.9\nProjected CRS: Minna / Nigeria Mid Belt\n# A tibble: 10,656 × 2\n   status_clean            Geometry\n * <chr>                <POINT [m]>\n 1 unknown      (297874.6 441473.8)\n 2 unknown      (607559.4 274905.5)\n 3 unknown      (576523.1 301556.6)\n 4 unknown      (578321.7 307339.8)\n 5 unknown      (590994.2 326738.8)\n 6 unknown      (597909.2 333608.5)\n 7 unknown      (724171.9 367609.1)\n 8 unknown      (737994.1 350616.5)\n 9 unknown      (749790.1 354304.6)\n10 unknown      (728109.9 367079.1)\n# … with 10,646 more rows"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#eda-on-waterpoints",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#eda-on-waterpoints",
    "title": "Take Home Exercise 1",
    "section": "EDA on waterpoints",
    "text": "EDA on waterpoints\nNow that we have extracted the water point data, we can have a better look at water points from each category.\nThis is for Functional Water point\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\n\n\n\n                 status_clean frequency percentage cumulative_perc\n1                  Functional     45883      87.99           87.99\n2 Functional but needs repair      4579       8.78           96.77\n3   Functional but not in use      1686       3.23          100.00\n\n\nThis is for Non Functional Water Point\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\n\n\n\n                      status_clean frequency percentage cumulative_perc\n1                   Non-Functional     29385      91.25           91.25\n2 Non-Functional due to dry season      2403       7.46           98.71\n3         Abandoned/Decommissioned       234       0.73           99.44\n4                        Abandoned       175       0.54           99.98\n5 Non functional due to dry season         7       0.02          100.00\n\n\nThis is Unknown Water Point\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n\n  status_clean frequency percentage cumulative_perc\n1      unknown     10656        100             100"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#performing-point-in-polygon-count.",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#performing-point-in-polygon-count.",
    "title": "Take Home Exercise 1",
    "section": "Performing Point In Polygon Count.",
    "text": "Performing Point In Polygon Count.\nWhile knowing the number of total functional and nonfunctional and unknown water point is important, it would be better if we are able to see the status of each water point in each of the LGA.\nTo do that we would need to perform a series of steps.\n\nWe would need to make use of the st_intersects() function of sf package to identify the functional water points in each LGA. Find out more about st_intersects() function of sf package here\nWe would need to make use of lengths() function from base r to calculate the number of functional water points that fall inside each LGA. Find out more about lengths() function of base r here.\n\nAll of this is added to a new sf data frame “NGA_wp” to be used in subsequent steps.\n\nNGA_wp <- NGA %>% \n  mutate(`total_wp` = lengths(\n    st_intersects(NGA, wp_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(NGA, wp_functional))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(NGA, wp_nonfunctional))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(NGA, wp_unknown)))"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#visualising-attributes",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#visualising-attributes",
    "title": "Take Home Exercise 1",
    "section": "Visualising Attributes",
    "text": "Visualising Attributes\nNow that we have the point in polygon count we can reveal the distribution of the waterpoint.\nWe can make use of the ggplot2 packages to help plot. ggplot2 is part of tidyverse, and more information can be found here.\nDistribution of Total Water point\n\nggplot(data = NGA_wp,\n       aes(x = wp_functional)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nDistribution of Functional Water point\n\nggplot(data = NGA_wp,\n       aes(x = wp_nonfunctional)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n\n\nDistribution of Non Functional Water point\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n\n\nDistribution of Unknown Water point\n\nggplot(data = NGA_wp,\n       aes(x = wp_unknown)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-sf-data-to-sps-spatial-class",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-sf-data-to-sps-spatial-class",
    "title": "Take Home Exercise 1",
    "section": "Converting sf data to sp’s Spatial Class",
    "text": "Converting sf data to sp’s Spatial Class\nAs sp’s Spatial Class is still commonly used by most of the spatial analysis packages as such we would need to convert the data frame into Spatial Class.\nThis can be done with the function as_Spatial() function of the sf package. Find out more about as_Spatial() from sf package here.\n\nwp_functional_spatial <- as_Spatial(wp_functional)\nwp_nonfunctional_spatial <- as_Spatial(wp_nonfunctional)\nNGA_spatial <- as_Spatial(NGA)\n\n\nViewing each of the sp Spatial Class\nHere we will view the Spatial Class of the converted data frame.\nFunctional\n\nwp_functional_spatial \n\nclass       : SpatialPointsDataFrame \nfeatures    : 52148 \nextent      : 29322.63, 1218553, 33758.37, 1092629  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :              status_clean \nmin values  :                Functional \nmax values  : Functional but not in use \n\n\nNon Functional\n\nwp_nonfunctional_spatial\n\nclass       : SpatialPointsDataFrame \nfeatures    : 32204 \nextent      : 28907.91, 1209690, 33736.93, 1092883  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :                     status_clean \nmin values  :                        Abandoned \nmax values  : Non functional due to dry season \n\n\nNGA\n\nNGA_spatial\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 774 \nextent      : 26662.71, 1344157, 30523.38, 1096029  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       :   ADM2_EN, ADM2_PCODE, ADM1_EN, ADM1_PCODE \nmin values  : Aba North,   NG001001,    Abia,      NG001 \nmax values  :      Zuru,   NG037014, Zamfara,      NG037"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "Take Home Exercise 1",
    "section": "Converting the Spatial Class into generic sp format",
    "text": "Converting the Spatial Class into generic sp format\nThe main package that we are using to analyse out data would be spatstat. However, this requires our analytical data to be in the ppp object form.\nHowever, there is no direct way for us to convert a Spatial class into a ppp object. It would need to be converted into a Spatial object first before we can convert it into a ppp object.\nWe can make use of the as() function from basic R to convert it into a Spatial Object. Find out more about as() function here.\n\n\n\n\n\n\nNote\n\n\n\nNote that we are converting the point into Spatial Points, while we are converting the boundaries into “Spatial Polygons”\n\n\n\nwp_functional_sp <- as(wp_functional_spatial, \"SpatialPoints\")\nwp_nonfunctional_sp <- as(wp_nonfunctional_spatial, \"SpatialPoints\")\nNGA_sp <- as(NGA_spatial, \"SpatialPolygons\")\n\n\nViewing each of the Spatial Points\nFunctional\n\nwp_functional_sp \n\nclass       : SpatialPoints \nfeatures    : 52148 \nextent      : 29322.63, 1218553, 33758.37, 1092629  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \n\n\nNon Functional\n\nwp_nonfunctional_sp\n\nclass       : SpatialPoints \nfeatures    : 32204 \nextent      : 28907.91, 1209690, 33736.93, 1092883  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \n\n\nNGA\n\nNGA_sp\n\nclass       : SpatialPolygons \nfeatures    : 774 \nextent      : 26662.71, 1344157, 30523.38, 1096029  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-into-spatstas-ppp-format",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-into-spatstas-ppp-format",
    "title": "Take Home Exercise 1",
    "section": "Converting into spatstas ppp format",
    "text": "Converting into spatstas ppp format\nWe can now convert the Spatial Object into a ppp format\n\nwp_functional_ppp <- as(wp_functional_sp , \"ppp\")\nwp_nonfunctional_ppp <- as(wp_nonfunctional_sp , \"ppp\")\n\n\nViewing each ppp format\nFunctional\n\nwp_functional_ppp\n\nPlanar point pattern: 52148 points\nwindow: rectangle = [29322.6, 1218553.3] x [33758.4, 1092628.9] units\n\n\nNon Functional\n\nwp_nonfunctional_ppp\n\nPlanar point pattern: 32204 points\nwindow: rectangle = [28907.9, 1209690] x [33736.9, 1092882.6] units\n\n\n\n\nChecking for Duplicate Points\n\n\n\n\n\n\nNote\n\n\n\nIt is important to check for Duplicate Points, and there are many methods of handling duplicate points such as rjitter().\n\n\nWe would now need to perform a check to make sure that there is no duplicated points. which we can check using the\n\nany(duplicated(wp_functional_ppp))\n\n[1] FALSE\n\n\n\nany(duplicated(wp_nonfunctional_ppp))\n\n[1] FALSE\n\n\nSince the result is False, it seems that there is no special need to perform other actions to fix the duplicated data."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#creating-owin-object",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#creating-owin-object",
    "title": "Take Home Exercise 1",
    "section": "Creating Owin Object",
    "text": "Creating Owin Object\n\n\n\n\n\n\nNote\n\n\n\nAn owin object is used to define the polygonal region of the Region of interest.\n\n\nWe will now wish to confine the geographical area boundary to that of Nigeria, as such we can convert the Spatial Polygon object into owin to help us represent this polygonal region.\n\nNGA_owin <- as(NGA_sp, \"owin\")\n\n\nPlotting the Owin\n\nplot(NGA_owin)\n\n\n\n\n\n\nCombining Non Functional Water Point and Functional Water Point with Owin\nNow then we can extract the water point events and combine it with the owin data.\n\nwp_functional_NGA_ppp = wp_functional_ppp[NGA_owin]\n\n\nwp_nonfunctional_NGA_ppp = wp_nonfunctional_ppp[NGA_owin]\n\n\n\nPlotting the Owin Object with Water Points\nNow we will plot the owin object to see if we it was successful.\nFunctional\n\nplot(wp_functional_NGA_ppp)\n\n\n\n\nNon Functional\n\nplot(wp_nonfunctional_NGA_ppp)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#rescaling-the-kde-values",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#rescaling-the-kde-values",
    "title": "Take Home Exercise 1",
    "section": "Rescaling the KDE values",
    "text": "Rescaling the KDE values\nOne issue with the ppp data is that the unit of measurement is in meters, which will cause out density values to be very small.\nAs such there is a needed to convert the unit measurement into kilometer in order for the density to make better sense.\n\nwp_functional_NGA_ppp.km <- rescale(wp_functional_NGA_ppp, 1000, \"km\")\nwp_nonfunctional_NGA_ppp.km <- rescale(wp_nonfunctional_NGA_ppp, 1000, \"km\")"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#plotting-the-kernal-density-map",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#plotting-the-kernal-density-map",
    "title": "Take Home Exercise 1",
    "section": "Plotting the Kernal Density Map",
    "text": "Plotting the Kernal Density Map\nNow that we have rescaled the data set we can perform Kernel Density Estimation on the datasets.\nWe can make use of the density() function from the spatstat package to help us generate the Kernal Density map. Find out more about density() from spatstat here., We will making use of the automatic bandwidth methods here.\n\n\n\n\n\n\nNote\n\n\n\nOne thing to note is that there are a 3 different spatstat function for us to use to determine the bandwidth, called bw.CvL, bw.scott and bw.ppl. And also kernel needs to be determined as well (I will be using the default “Gaussian”.\nAccording to research by Prof, it was suggested that bw.ppl() is more appropriate to use, when patterns consist predominantly of tight cluster. bw.diggle() is used when the we are trying to detect a single tight cluster in the midst of random noise. However, both are more commonly use.\nI have chosen to use diggle(), but results might vary if bw.ppl() is used.\n\n\nFunctional Kernal Density Map\n\nwp_functional_NGA.bw <- density(wp_functional_NGA_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(wp_functional_NGA.bw)\n\n\n\n\nNon Functional Kernal Density Map\n\nwp_nonfunctional_NGA.bw <- density(wp_nonfunctional_NGA_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(wp_nonfunctional_NGA.bw)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-to-grid-object",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-to-grid-object",
    "title": "Take Home Exercise 1",
    "section": "Converting to Grid Object",
    "text": "Converting to Grid Object\nWe would need to convert the KDE into a suitable format for mapping purpose. The format we are converting would be a Grid Object\nFunctional\n\ngridded_kde_wp_functional_NG_bw <- as.SpatialGridDataFrame.im(wp_functional_NGA.bw)\nspplot(gridded_kde_wp_functional_NG_bw)\n\n\n\n\nNon Functional\n\ngridded_kde_wp_nonfunctional_NG_bw <- as.SpatialGridDataFrame.im(wp_nonfunctional_NGA.bw)\nspplot(gridded_kde_wp_nonfunctional_NG_bw)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-to-raster",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-to-raster",
    "title": "Take Home Exercise 1",
    "section": "Converting to Raster",
    "text": "Converting to Raster\nAfter we have converted the data into a Grid Object we would need to convert that into a Raster Layer. We can make use of raster() function of raster package to help us convert. Find out more about raster package here.\nRaster object is accepted by tmap as one of the layer for plotting. as such we will be converting it into a raster object\n\nkde_wp_functional_NG_bw_raster <- raster(gridded_kde_wp_functional_NG_bw)\nkde_wp_nonfunctional_NG_raster <- raster(gridded_kde_wp_nonfunctional_NG_bw)\n\n\n\n\n\n\n\nNote\n\n\n\nHowever, a raster object does not have any CRS information. We would need to project the CRS information into the raster layer. In this case the EPSG code is 26392.\n\n\n\nprojection(kde_wp_functional_NG_bw_raster) <- CRS(\"+init=EPSG:26392\")\nprojection(kde_wp_nonfunctional_NG_raster) <- CRS(\"+init=EPSG:26392\")\n\n\nViewing Raster Object\nFunctional\n\nkde_wp_functional_NG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 10.29292, 8.324266  (x, y)\nextent     : 26.66271, 1344.157, 30.52338, 1096.029  (xmin, xmax, ymin, ymax)\ncrs        : +init=EPSG:26392 \nsource     : memory\nnames      : v \nvalues     : -4.383912e-16, 4.843545  (min, max)\n\n\nNon Functional\n\nkde_wp_nonfunctional_NG_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 10.29292, 8.324266  (x, y)\nextent     : 26.66271, 1344.157, 30.52338, 1096.029  (xmin, xmax, ymin, ymax)\ncrs        : +init=EPSG:26392 \nsource     : memory\nnames      : v \nvalues     : -2.410108e-16, 1.517255  (min, max)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#extracting-osun-state-from-nga",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#extracting-osun-state-from-nga",
    "title": "Take Home Exercise 1",
    "section": "Extracting Osun State from NGA",
    "text": "Extracting Osun State from NGA\nWe can now perform the same steps as above to generate the Kernal Density Map of Osun Area.\n\nosun = NGA_spatial[NGA_spatial@data$ADM1_EN == \"Osun\",]\n\n\nplot(osun)\n\n\n\n\n\nosun_sp = as(osun, \"SpatialPolygons\")\n\n\nosun_owin = as(osun_sp, \"owin\")"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#merging-water-data-point-with-osun-owin",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#merging-water-data-point-with-osun-owin",
    "title": "Take Home Exercise 1",
    "section": "Merging Water Data point with Osun Owin",
    "text": "Merging Water Data point with Osun Owin\n\nwp_functional_osun_ppp = wp_functional_ppp[osun_owin]\nwp_nonfunctional_osun_ppp = wp_nonfunctional_ppp[osun_owin]\n\n\nRescaling Data Points\n\nwp_functional_osun_ppp.km = rescale(wp_functional_osun_ppp, 1000, \"km\")\nwp_nonfunctional_osun_ppp.km = rescale(wp_nonfunctional_osun_ppp, 1000, \"km\")\n\nWe will plot a map of the functional water point of osun to take a look.\n\nplot(wp_functional_osun_ppp.km)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#generating-kde",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#generating-kde",
    "title": "Take Home Exercise 1",
    "section": "Generating KDE",
    "text": "Generating KDE\n\nwp_nonfunctional_osun.bw <- density(wp_nonfunctional_osun_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(wp_nonfunctional_osun.bw)\n\n\n\n\n\nwp_functional_osun.bw <- density(wp_functional_osun_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(wp_functional_osun.bw)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-kde-to-raster",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#converting-kde-to-raster",
    "title": "Take Home Exercise 1",
    "section": "Converting KDE to Raster",
    "text": "Converting KDE to Raster\n\ngridded_kde_wp_functional_osun_bw <- as.SpatialGridDataFrame.im(wp_functional_osun.bw)\nspplot(gridded_kde_wp_functional_osun_bw)\n\n\n\n\n\ngridded_kde_wp_nonfunctional_osun_bw <- as.SpatialGridDataFrame.im(wp_nonfunctional_osun.bw)\nspplot(gridded_kde_wp_nonfunctional_osun_bw)\n\n\n\n\n\nkde_wp_functional_osun_bw_raster <- raster(gridded_kde_wp_functional_osun_bw)\nkde_wp_nonfunctional_osun_bw_raster <- raster(gridded_kde_wp_nonfunctional_osun_bw)\n\n\nprojection(kde_wp_functional_osun_bw_raster) <- CRS(\"+init=EPSG:26393\")\nprojection(kde_wp_nonfunctional_osun_bw_raster) <- CRS(\"+init=EPSG:26393\")\n\n\nkde_wp_functional_osun_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.8948485, 0.9616045  (x, y)\nextent     : 176.5032, 291.0438, 331.4347, 454.5201  (xmin, xmax, ymin, ymax)\ncrs        : +init=EPSG:26393 \nsource     : memory\nnames      : v \nvalues     : -4.876249e-15, 25.49435  (min, max)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#plotting-kde-of-osun",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#plotting-kde-of-osun",
    "title": "Take Home Exercise 1",
    "section": "Plotting KDE of Osun",
    "text": "Plotting KDE of Osun\n\ntm_shape(kde_wp_functional_osun_bw_raster) + \n    tm_layout(main.title = \"KDE of Functional Water Point (Osun)\") +\n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\nVariable(s) \"v\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\ntm_shape(kde_wp_nonfunctional_osun_bw_raster) + \n    tm_layout(main.title = \"KDE of Non Functional Water Point (Osun)\") +\n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\nVariable(s) \"v\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#testing-for-distribution-clarks-and-evens",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#testing-for-distribution-clarks-and-evens",
    "title": "Take Home Exercise 1",
    "section": "Testing for Distribution Clarks and Evens",
    "text": "Testing for Distribution Clarks and Evens\n\n\n\n\n\n\nNote\n\n\n\nClarks and Evants Test is a Test of Aggregation, Further Testing would need to be performed in order for us to determine if the Hypothesis is True. Find out more about clarksevans.test() here.\n\n\nThe test hypotheses are:\nHo = The distribution of water points in Osun are randomly distributed.\nH1= The distribution of water points in Osun are not randomly distributed.\nThe 95% confident interval will be used.\n\nclarkevans.test(wp_functional_osun_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 99 simulations of CSR with fixed n\n\ndata:  wp_functional_osun_ppp\nR = 0.44767, p-value = 0.01\nalternative hypothesis: clustered (R < 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe therefore reject the null Hypothesis that the Functional water point are randomly distributed as the p-value is greater than 0.01.\n\n\n\nclarkevans.test(wp_nonfunctional_osun_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 99 simulations of CSR with fixed n\n\ndata:  wp_nonfunctional_osun_ppp\nR = 0.44327, p-value = 0.01\nalternative hypothesis: clustered (R < 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe therefore reject the null Hypothesis that the non Functional water point are randomly distributed as the p-value is greater than 0.01."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#analysis-of-functional-and-non-functional-water-point",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#analysis-of-functional-and-non-functional-water-point",
    "title": "Take Home Exercise 1",
    "section": "Analysis of Functional and Non Functional Water point",
    "text": "Analysis of Functional and Non Functional Water point\nWhen looking at the 2 Kernal Density Map, it is easy to assume that there seems to be almost no pattern on the density of the water point, however, one can easily assume that there is no pattern in where all the water points are located.\nHowever upon further inspection, there seems to be 2 trends that can be spotted based in the difference in map density. In other to help out in the spotting of the trend, a map of Osun from Google map was include below for the trend to be more easily view able.\n\nAreas with High Functional Water Point and Non Functional Water Point are clustered together in the cities in Osun.\n\n\n\nCopyrighted: Taken from Google Maps\n\n\nTaking a look at the map above and comparing it with the KDE Map, areas with high density of functional and non functional water points seems to coincide with where the cities are located. This seems to make the most sense as cities are where most people lives and need the access to water, as such it make sense most of the water point to concentrate itself in the cities.\nIn this regards, Large cities tend to have a larger concentration of functional and non functional water points as compared to smaller cities. On the other hand, areas of very low human density such as the Both Nature Reserve seem to have almost no Water Points whether they are functional or not.\n\n\n\n\n\n\nImportant\n\n\n\nBased on this observations, we can hypothesis that both the Functional Water Point and Non Functional Water Point are clustered together in the state of Osun. Further Validation using Second Spatial Point Pattern Analysis would need to be performed.\n\n\n\n\nFunctional and Non Functional Water Point are clustered together in cities of Osun\n\n\n\nFunctional Water Point KDE\n\n\n\n\n\nNon Functional Water Point KDE\n\n\nWe have already establish in the previous observation that Most of the Water Points in the Cities. When we observed the density of the Functional Water Points and Non Functional Water Points in the state, we come to a hypothesis that the water points are clustered together in the State of Osun. However, if we were to inspect the KDE of both water points more closely focusing on the cities, we have reason to believe that the functional water and non functional water points are clustered together among themselves in the cities as well.\nThe 2 images above are captures of the cities of Ede. Despite both being in the same city, the water points appeared to be clustered together as well. This can be observed in other points in the Functional Water Point KDE and Non Functional Water Point KDE as well.\n\n\n\n\n\n\nImportant\n\n\n\nBased on this observations, we can hypothesis that both the Functional Water Point and Non Functional Water Point are clustered together in the cities of the state of Osun. Further Validation using Second Spatial Point Pattern Analysis would need to be performed."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#comparison-between-kernel-density-map-and-point-map.",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#comparison-between-kernel-density-map-and-point-map.",
    "title": "Take Home Exercise 1",
    "section": "Comparison between Kernel Density Map and Point Map.",
    "text": "Comparison between Kernel Density Map and Point Map.\n\nKernel Density Map are better at spotting trends as compared to Point Map\nKernal Density Map are much better at spotting trends as compared to that of the point map. A Point map will highlight each individual water point out and this can be difficult for us to determine any trend at all.\nWith reference to the map in Mapping the Functional and Non Functional Points, there seems to be almost no difference in the density of the water points in the area, or at the very least it is difficult to tell. A Kernel Density Map solve this by smoothing over the points and providing a density number, whereas for Point Map, we can only observed the areas based on the number of points. T\nKernel Density Map provides a Quantitative Value as compared to the Qualitative observation of Point Map in detecting trends.\n\n\nKernal Density Map are less computationally intensive to display as compared to Point Map in an interactive map\nA Point Map with it thousands of points is more computationally intensive to display are compared to Kernal Density Map. This is because the computer would need to take note of every individual point and plot it out which would be an issue for computers with low computational power.\n\n\nKernel Density Map is able to provide an estimation of the concentration of points.\nKernel Density takes into account the inverse-distance weighted counts of the existing point to estimate the concentration of points in an given area. This means that the estimated cells values are derived based on the weights of the existing points, with the furthest points having the lowest weights. This process in turns smooth the density generated and can help to give a more pronounced gradient.\nFind out more about Inverse Distance Weighted here."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#defining-the-areas",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#defining-the-areas",
    "title": "Take Home Exercise 1",
    "section": "Defining the Areas",
    "text": "Defining the Areas\n\niwo = NGA_spatial[NGA_spatial@data$ADM2_EN == \"Iwo\",] \nosogbo = NGA_spatial[NGA_spatial@data$ADM2_EN == \"Osogbo\",]\nede = NGA_spatial[NGA_spatial@data$ADM2_EN %in% c(\"Ede North\",\"Ede South\"),]\n\n\niwo_sp = as(iwo, \"SpatialPolygons\")\nosogbo_sp = as(osogbo, \"SpatialPolygons\")\nede_sp = as(ede, \"SpatialPolygons\")\n\n\niwo_owin = as(iwo, \"owin\")\nosogbo_owin = as(osogbo, \"owin\")\nede_owin = as(ede, \"owin\")\n\n\nwp_functional_iwo_ppp = wp_functional_ppp[iwo_owin]\nwp_nonfunctional_iwo_ppp = wp_nonfunctional_ppp[iwo_owin]\n\nwp_functional_osogbo_ppp = wp_functional_ppp[osogbo_owin]\nwp_nonfunctional_osogbo_ppp = wp_nonfunctional_ppp[osogbo_owin]\n\nwp_functional_ede_ppp = wp_functional_ppp[ede_owin]\nwp_nonfunctional_ede_ppp = wp_nonfunctional_ppp[ede_owin]\n\n\nwp_functional_iwo_ppp.km = rescale(wp_functional_iwo_ppp, 1000, \"km\")\nwp_nonfunctional_iwo_ppp.km = rescale(wp_nonfunctional_iwo_ppp, 1000, \"km\")\n\nwp_functional_osogbo_ppp.km = rescale(wp_functional_osogbo_ppp, 1000, \"km\")\nwp_nonfunctional_osogbo_ppp.km = rescale(wp_nonfunctional_osogbo_ppp, 1000, \"km\")\n\nwp_functional_ede_ppp.km = rescale(wp_functional_ede_ppp, 1000, \"km\")\nwp_nonfunctional_ede_ppp.km = rescale(wp_nonfunctional_ede_ppp, 1000, \"km\")"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#computing-g-function",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#computing-g-function",
    "title": "Take Home Exercise 1",
    "section": "Computing G Function",
    "text": "Computing G Function\nNow that we have extracted the dataset for the 3 cities and converted it into ppp format we can now perform our 2nd Order Spatial Analysis on them.\nFor this we have chose to use G Function to test our Hypothesis. As stated by our Professor, G Function is a measure of the distribution from an arbitrary event to its nearest event. It estimates the nearest neighbour distance distribution from a point pattern in a window of arbitrary shape. It is recommended that G Function is a useful statistic summarising one aspect of the ``clustering’’ of points. As such we will make use of G Function to test our Hypothesis on whether the points are randomly distributed or not.\nFor us to perform G Function Analysis we will be making use of Gest() Function from spatstat. Find out more about Gest() from spatstat here. To further confirm our Hypothesis we would need to perform a Monte Carlo Simulation Test with G Function to test our Hypothesis.\n\nIwo\n\nComputing G_Function Estimation for Functional Water Point\n\nG_iwo = Gest(wp_functional_iwo_ppp.km, correction = \"border\")\nplot(G_iwo)\n\n\n\n\n\n\nTesting the Hypothesis of Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Functional Water Points in Iwo are randomly distributed\nH1: The distribution of the Functional Water Points in Iwo are not randomly distributed\nConfidence level : 99%\nSignificance level : 0.01\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.01.\n\n\nG_iwo_fuctional.csr <- envelope(wp_functional_iwo_ppp.km, Gest, nsim=100)\n\nGenerating 100 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,  100.\n\nDone.\n\n\n\nplot(G_iwo_fuctional.csr)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed G(r) is far above the G(theo) as well as the envelope - indicating that Functional Water Point in the Iwo area are clustered. Hence, we reject the null hypothesis that Functional Water Point in the Iwo area are randomly distributed at 99% confident interval.\n\n\n\n\nComputing G_Fuction Estimate for Non Functional Water Point\n\nG_iwo_non = Gest(wp_nonfunctional_iwo_ppp.km, correction = \"border\")\nplot(G_iwo_non)\n\n\n\n\n\n\nTesting the Hypothesis of Non Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Non Functional Water Points in Iwo are randomly distributed\nH1: The distribution of the Non Functional Water Points in Iwo are not randomly distributed\nConfidence level : 99%\nSignificance level : 0.01\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.01.\n\n\nG_iwo_nonfuctional.csr <- envelope(wp_nonfunctional_iwo_ppp.km, Gest, nsim=100)\n\nGenerating 100 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,  100.\n\nDone.\n\n\n\nplot(G_iwo_nonfuctional.csr)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed G(r) is far above the G(theo) as well as the envelope - indicating that Non Functional Water Point in the Iwo area are clustered. Hence, we reject the null hypothesis that Non Functional Water Point in the Iwo area are randomly distributed at 99% confident interval.\n\n\n\n\n\nOsogbo\n\nComputing G-Fuction Estimate of Functional Water Point\n\nG_osogbo = Gest(wp_functional_osogbo_ppp.km, correction = \"border\")\nplot(G_osogbo)\n\n\n\n\n\n\nTesting the Hypothesis of Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Functional Water Points in Osogbo are randomly distributed\nH1: The distribution of the Functional Water Points in Osogbo are not randomly distributed\nConfidence level : 99%\nSignificance level : 0.01\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.01.\n\n\nG_osogbo_fuctional.csr <- envelope(wp_functional_osogbo_ppp.km, Gest, nsim=100)\n\nGenerating 100 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,  100.\n\nDone.\n\n\n\nplot(G_osogbo_fuctional.csr)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed G(r) is far above the G(theo) as well as the envelope - indicating that Functional Water Point in the Osogbo area are clustered. Hence, we reject the null hypothesis that Functional Water Point in the Osogbo area are randomly distributed at 99% confident interval.\n\n\n\n\nComputing G_Function Estimate for Non Functional Water Point\n\nG_osogbo_non = Gest(wp_nonfunctional_osogbo_ppp.km, correction = \"border\")\nplot(G_osogbo_non)\n\n\n\n\n\n\nTesting the Hypothesis of Non Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Non Functional Water Points in Osogbo are randomly distributed\nH1: The distribution of the Non Functional Water Points in Osogbo are not randomly distributed\nConfidence level : 99%\nSignificance level : 0.01\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.01.\n\n\nG_osogbo_nonfuctional.csr <- envelope(wp_nonfunctional_osogbo_ppp.km, Gest, nsim=100)\n\nGenerating 100 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,  100.\n\nDone.\n\n\n\nplot(G_osogbo_nonfuctional.csr)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed G(r) is far above the G(theo) as well as the envelope - indicating that Non Functional Water Point in the Osobo area are clustered. Hence, we reject the null hypothesis that Non Functional Water Point in the Osobo area are randomly distributed at 99% confident interval.\n\n\n\n\n\nEde\n\nComputing G_Function Estimate for Functional Water Point\n\nG_ede = Gest(wp_functional_ede_ppp.km, correction = \"border\")\nplot(G_ede)\n\n\n\n\n\n\nTesting the Hypothesis of Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Functional Water Points in Ede are randomly distributed\nH1: The distribution of the Functional Water Points in Ede are not randomly distributed\nConfidence level : 99%\nSignificance level : 0.01\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.01.\n\n\nG_ede_fuctional.csr <- envelope(wp_functional_ede_ppp.km, Gest, nsim=100)\n\nGenerating 100 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,  100.\n\nDone.\n\n\n\nplot(G_ede_fuctional.csr)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed G(r) is far above the G(theo) as well as the envelope - indicating that Functional Water Point in the Ede area are clustered. Hence, we reject the null hypothesis that Functional Water Point in the Ede area are randomly distributed at 99% confident interval.\n\n\n\n\nComputing G_Function Estimate for Non Functional Water Point\n\nG_ede_non = Gest(wp_nonfunctional_ede_ppp.km, correction = \"border\")\nplot(G_ede_non)\n\n\n\n\n\n\nTesting the Hypothesis of Non Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Non Functional Water Points in Ede are randomly distributed\nH1: The distribution of the Non Functional Water Points in Ede are not randomly distributed\nConfidence level : 99%\nSignificance level : 0.01\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.01.\n\n\nG_ede_nonfuctional.csr <- envelope(wp_nonfunctional_ede_ppp.km, Gest, nsim=100)\n\nGenerating 100 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,  100.\n\nDone.\n\n\n\nplot(G_ede_nonfuctional.csr)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed G(r) is far above the G(theo) as well as the envelope - indicating that Non Functional Water Point in the Ede area are clustered. Hence, we reject the null hypothesis that Non Functional Water Point in the Ede area are randomly distributed at 99% confident interval.\n\n\n\n\n\nOsun\n\nComputing G_Function Estimate for Functional Water Point\n\nG_osun = Gest(wp_functional_osun_ppp.km, correction = \"border\") \nplot(G_osun)\n\n\n\n\n\n\nTesting the Hypothesis for Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Functional Water Points in Osun are randomly distributed\nH1: The distribution of the Functional Water Points in Osun are not randomly distributed\nConfidence level : 99%\nSignificance level : 0.01\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.01.\n\n\nG_osun_fuctional.csr <- envelope(wp_functional_osun_ppp.km, Gest, nsim=100)\n\nGenerating 100 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,  100.\n\nDone.\n\n\n\nplot(G_osun_fuctional.csr)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed G(r) is far above the G(theo) as well as the envelope - indicating that Functional Water Point in the Osun area are clustered. Hence, we reject the null hypothesis that Functional Water Point in the Osun area are randomly distributed at 99% confident interval.\n\n\n\n\nComputing G_Function Estimate for Non Functional Water Point\n\nG_osun_non = Gest(wp_nonfunctional_osun_ppp.km, correction = \"border\") \nplot(G_osun_non)\n\n\n\n\n\n\nHypothesis Testing For Non Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Non Functional Water Points in Osun are randomly distributed\nH1: The distribution of the Non Functional Water Points in Osun are not randomly distributed\nConfidence level : 99%\nSignificance level : 0.01\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.01.\n\n\nG_osun_nonfuctional.csr <- envelope(wp_nonfunctional_osun_ppp.km, Gest, nsim=100)\n\nGenerating 100 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,  100.\n\nDone.\n\n\n\nplot(G_osun_nonfuctional.csr)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed G(r) is far above the G(theo) as well as the envelope - indicating that Non Functional Water Point in the Osun area are clustered. Hence, we reject the null hypothesis that Functional Water Point in the Osun area are randomly distributed at 99% confident interval."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#conclusion-for-2nd-order-spatial-point-analysis",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#conclusion-for-2nd-order-spatial-point-analysis",
    "title": "Take Home Exercise 1",
    "section": "Conclusion for 2nd Order Spatial Point Analysis",
    "text": "Conclusion for 2nd Order Spatial Point Analysis\n\nAreas with High Functional Water Point and Non Functional Water Point are clustered together in the cities in Osun\nBased on our G Function Analysis, since we have determined that both the Functional Water Point and the Non Functional Point are not randomly distributed at 99% Confidence interval in Osun. Hence, we can determine that they are clusted together\n\n\nFunctional and Non Functional Water Point are clustered together in cities of Osun\nBased on our G Function Analysis, since we have determined that both the Functional Water Point and the Non Functional Point are not randomly distributed at 99% Confidence interval in 3 main cities Iwo, Osogbo and Ede. Hence, we can determine that they are clusted together"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#computing-l-function",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#computing-l-function",
    "title": "Take Home Exercise 1",
    "section": "Computing L Function",
    "text": "Computing L Function\nIn order to test for Correlation we would need to perform a K Function. K function is a popular technique for analyzing spatial correlation in point patterns. It measures the number of events found up to a given distance of any particular event, according to Prof. This makes it perfect for testing for Spatial Correlation Analysis.\nHowever,K Function makes it difficult for us to discern difference between the Theortical K and Predicted K at lower values, as such we will be making use of the L Function instead. L Function is a transformation of K Function before applying a square root transformation, which theoretical stabilised the variance of the estimator.\nFor us to perform L Function Analysis we will be making use of Lest() Function from spatstat. Find out more about Lest() Function from spatstat here. To further confirm our Hypothesis we would need to perform a Monte Carlo Simulation Test with G Function to test our Hypothesis.\n\nFunctional Water Point\n\nComputing L Function Estimate\n\nL_fun_osun = Lest(wp_functional_osun_ppp.km, correction = \"Ripley\")\nplot(L_fun_osun, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d\")\n\n\n\n\nTesting the Hypothesis on Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Functional Water Points in Osun are independent of each other\nH1: The distribution of the Functional Water Points in Osun are not independent of each other\nConfidence level : 95%\nSignificance level : 0.05\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.05.\n\n\nL_fun_osun.csr <- envelope(wp_functional_osun_ppp.km, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\n\nplot(L_fun_osun.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed L(r)-r(obs) is far above the L(r)-r(theo) as well as the envelope for almost all distance of d- indicating that Functional Water Point in the Osun are not independent of each other. Hence, we reject the null hypothesis that Functional Water Point in the Osun are independent of each other at all distance of d at 95% confidence interval.\n\n\n\n\n\nNon Functional Water Point\n\nComputing L Function Estimate\n\nL_non_fun_osun = Lest(wp_nonfunctional_osun_ppp.km, correction = \"Ripley\")\nplot(L_non_fun_osun, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d\")\n\n\n\n\nTesting the Hypothesis of Non Functional Water Point\nOur Hypothesis:\n\nH0: The distribution of the Non Functional Water Points in Osun are independent of each other\nH1: The distribution of the Non Functional Water Points in Osun are not independent of each other\nConfidence level : 95%\nSignificance level : 0.05\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.05.\n\n\nL_non_fun_osun.csr <- envelope(wp_nonfunctional_osun_ppp.km, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\n\nplot(L_non_fun_osun.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConclusion: The observed L(r)-r(obs) is far above the L(r)-r(theo) as well as the envelope for almost all distance of d- indicating that Non Functional Water Point in the Osun are not independent of each other. Hence, we reject the null hypothesis that Non Functional Water Point in the Osun are independent of each other at all distance of d at 95% confidence interval."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#conclusion",
    "href": "lessons/Take-home/Take-home_ex1/Take-home_ex1.html#conclusion",
    "title": "Take Home Exercise 1",
    "section": "Conclusion",
    "text": "Conclusion\n\nFunctional Water Point are not independent of each other in the State of Osun\nBased on the Conclusion of L Function Test, we need to reject the null hypothesis that Functional Water Point in the Osun are independent of each other at all distance of d at 95% confidence interval, therefore we can conclude that Functional Water Point in the Osun area are not independent of each other.\n\n\nNon Functional Water Point are not independent of each other in the State of Osun\nBased on the Conclusion of L Function Test, we need to reject the null hypothesis that Non Functional Water Point in the Osun are independent of each other at all distance of d at 95% confidence interval, therefore we can conclude that Functional Water Point in the Osun area are not independent of each other."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Important\n\n\n\nThis part is taken from IS415 Take Home Exercise 2. All rights belong to Dr Kam Tin Seong.\n\n\nSince late December 2019, an outbreak of a novel coronavirus disease (COVID-19; previously known as 2019-nCoV) was reported in Wuhan, China, which had subsequently affected 210 countries worldwide. In general, COVID-19 is an acute resolved disease but it can also be deadly, with a 2% case fatality rate.\nThe COVID-19 vaccination in Indonesia is an ongoing mass immunisation in response to the COVID-19 pandemic in Indonesia. On 13 January 2021, the program commenced when President Joko Widodo was vaccinated at the presidential palace. In terms of total doses given, Indonesia ranks third in Asia and fifth in the world.\nAccording to wikipedia, as of 5 February 2023 at 18:00 WIB (UTC+7), 204,266,655 people had received the first dose of the vaccine and 175,131,893 people had been fully vaccinated; 69,597,474 of them had been inoculated with the booster or the third dose, while 1,585,164 had received the fourth dose. Jakarta has the highest percentage of population fully vaccinated with 103.46%, followed by Bali and Special Region of Yogyakarta with 85.45% and 83.02% respectively.\nDespite its compactness, the cumulative vaccination rate are not evenly distributed within DKI Jakarta. The question is where are the sub-districts with relatively higher number of vaccination rate and how they changed over time."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#getting-the-data",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#getting-the-data",
    "title": "Take Home Exercise 2",
    "section": "Getting the Data",
    "text": "Getting the Data\nIn this Take Home Exercise, the main dataset we will be using would be indicated here:\n\nAspatial Data: We will be making use of the data from Riwayat File Vaksinasi DKI Jakarta. This website the vaccination data set for Jakarta. Based on the task, I have downloaded the data set from the month of July 2021 to June 2022. For this analysis, I have decided to pick the 1st Day of the Month.\nGeospatial Data: We will be making use of the DKI Jakarta administration boundary from 2019 in this case. The data can be found here."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#analyzing-the-data",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#analyzing-the-data",
    "title": "Take Home Exercise 2",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\n\nAspatial Data\nNow that we have gotten the data set, lets take a look at the data set. We will start off by loading the dataset from month of July 2021 first.\n\nJuly_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Juli 2021).xlsx\")\n\n\nJuly_2021\n\n# A tibble: 268 × 21\n   KODE KELURA…¹ WILAY…² KECAM…³ KELUR…⁴ SASARAN BELUM…⁵ JUMLA…⁶ JUMLA…⁷ TOTAL…⁸\n   <chr>         <chr>   <chr>   <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 <NA>          <NA>    <NA>    TOTAL   7739060 5041111 2696017 1181740 3877757\n 2 3172051003    JAKART… PADEMA… ANCOL     20393   13272    7114    3287   10401\n 3 3173041007    JAKART… TAMBORA ANGKE     25785   16477    9299    3221   12520\n 4 3175041005    JAKART… KRAMAT… BALE K…   25158   18849    6301    2644    8945\n 5 3175031003    JAKART… JATINE… BALI M…    8683    5743    2937    1517    4454\n 6 3175101006    JAKART… CIPAYU… BAMBU …   22768   15407    7357    3985   11342\n 7 3174031002    JAKART… MAMPAN… BANGKA    18930   12503    6421    2704    9125\n 8 3175051002    JAKART… PASAR … BARU      20267   11268    8982    4674   13656\n 9 3175041004    JAKART… KRAMAT… BATU A…   41389   30358   11020    5254   16274\n10 3171071002    JAKART… TANAH … BENDUN…   19008   11502    7499    3566   11065\n# … with 258 more rows, 12 more variables: `LANSIA\\r\\nDOSIS 1` <dbl>,\n#   `LANSIA\\r\\nDOSIS 2` <dbl>, `LANSIA TOTAL \\r\\nVAKSIN DIBERIKAN` <dbl>,\n#   `PELAYAN PUBLIK\\r\\nDOSIS 1` <dbl>, `PELAYAN PUBLIK\\r\\nDOSIS 2` <dbl>,\n#   `PELAYAN PUBLIK TOTAL\\r\\nVAKSIN DIBERIKAN` <dbl>,\n#   `GOTONG ROYONG\\r\\nDOSIS 1` <dbl>, `GOTONG ROYONG\\r\\nDOSIS 2` <dbl>,\n#   `GOTONG ROYONG TOTAL\\r\\nVAKSIN DIBERIKAN` <dbl>,\n#   `TENAGA KESEHATAN\\r\\nDOSIS 1` <dbl>, `TENAGA KESEHATAN\\r\\nDOSIS 2` <dbl>, …\n\n\nBased on what we can see, we have noticed that the data set is written in Indonesian. In this exercise, I am only interested in the first 6 columns, of which a rough translation of what each columns means is given in the table below.\n\n\n\nIndonesian Name\nRough Translation\n\n\n\n\nKODE KELURAHAN\nVillage Code\n\n\nWILAYAH KOTA\nCity Area (ADM2)\n\n\nKECAMATAN\nDistrict (ADM3)\n\n\nKELURAHAN\nUrban Village\n\n\nSASARAN\nTarget\n\n\nBELUM VAKSIN\nNot Vaccinated\n\n\n\nFurthermore, note that there is actually a row in the data frame that provides only the total count, we would need to drop the row as well. I have check through the rest of the data set and found that they are all in the same format as above, as such we can perform the same code for all the data.\nThe Code Chunk below show how we can perform all the filtering and the removal of the first row in a single code chunk\n\nJuly_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Juli 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nWe can then verify that it has been removed already.\n\nJuly_2021\n\n# A tibble: 267 × 6\n   `KODE KELURAHAN` `WILAYAH KOTA`  KECAMATAN        KELURAHAN   SASARAN BELUM…¹\n   <chr>            <chr>           <chr>            <chr>         <dbl>   <dbl>\n 1 3172051003       JAKARTA UTARA   PADEMANGAN       ANCOL         20393   13272\n 2 3173041007       JAKARTA BARAT   TAMBORA          ANGKE         25785   16477\n 3 3175041005       JAKARTA TIMUR   KRAMAT JATI      BALE KAMBA…   25158   18849\n 4 3175031003       JAKARTA TIMUR   JATINEGARA       BALI MESTER    8683    5743\n 5 3175101006       JAKARTA TIMUR   CIPAYUNG         BAMBU APUS    22768   15407\n 6 3174031002       JAKARTA SELATAN MAMPANG PRAPATAN BANGKA        18930   12503\n 7 3175051002       JAKARTA TIMUR   PASAR REBO       BARU          20267   11268\n 8 3175041004       JAKARTA TIMUR   KRAMAT JATI      BATU AMPAR    41389   30358\n 9 3171071002       JAKARTA PUSAT   TANAH ABANG      BENDUNGAN …   19008   11502\n10 3175031002       JAKARTA TIMUR   JATINEGARA       BIDARA CINA   32331   23395\n# … with 257 more rows, and abbreviated variable name ¹​`BELUM VAKSIN`\n\n\n\n\nGeospatial Data\nNow that we had taken a look at the Aspatial Data, we can take a look at the Geospatial Data.\nWe will import the Jakarta into a sf data frame with the code chunk below. In order to ensure that the spatial data is accurate, we would need to use the apply the correct crs information to the sf data frame through st_transform. As we are focusing on Jakarta, the CRS code is 23845. Find out more about Indonesia CRS here.\n\nJakarta_Area <- st_read(dsn = \"data/geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\") %>%\n  st_transform(crs = 23845)\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Take-home\\Take-home_ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nLets take a look at the data frame first. One thing you have noted is that there is a total of 162 columns in the sf data frame, and most of which is redundant in our current analysis. In this case, we are only interested in the first 9 columns of the sf data frame. The details of the data frame is as provided below:\n\n\n\nColumn Name\nRough Translation\n\n\n\n\nOBJECT_ID\nobject id\n\n\nKODE_DESA\nVillage Code\n\n\nDESA\nVillage - Similar to Urban Village\n\n\nKODE\nCode\n\n\nPROVINSI\nProvince\n\n\nKAB_KOTA\nCity District (ADM2) - Similar to City Area\n\n\nKECAMATAN\nDistrict (ADM3)\n\n\nDESA_KELAR\nVillage\n\n\nJUMLAH_PEN\nPopulations\n\n\n\nAnother important point to note is that while our Vaccination Data Frame only has 267 observations as compared to our sf data frame having 269 observations, this would mean that there is 2 extra areas in the sf data frame that we need to handle.\nUpon closer look at the data, we found out that there are 2 areas in the sf data frame that has no data, those areas are:\n\nDANAU SUNTER\nDANAU SUNTER DLL\n\nWe could just drops the rows as they have no meaningful data, but we might want to take a look more closely first. To solve the issue first we will extract filter the relevant data from the sf data set first, through the code chunk below.\n\nJakarta_Area <- st_read(dsn = \"data/geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\") %>%\n  select(0:9)\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Take-home\\Take-home_ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nWe will then perform a left join on this both of the data frame in order to find out which rows has empty data. We will make use of DESA and KELURAHAN as the common identifier.\n\njakarta_07_2021 <- left_join(Jakarta_Area, July_2021,\n                          by = c(\"DESA\" = \"KELURAHAN\"))\n\nOnce we join the data we have notice that that are 4 different rows with NA columns, 2 of them are expected, but the other 2 are a surprised:\n\nDANAU SUNTER\nDANAU SUNTER DLL\nJATIPULO\nKRENDANG\n\nI went to take a look at the data for the vaccination table and compare the results, and found that there is a slight difference in the name. The Jakarta sf Data frame record JATI PULO as JATIPULO, while for KERENDANG, it was recorded as KRENDANG instead.\nThere are 2 options to solve this issue. I can replace the name of the those two, but I choose to use the KODE_DESA and KODE KELURAHAN instead.\n\njakarta_07_2021 <- left_join(Jakarta_Area, July_2021,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\nWhen we look at the data, we spotted 2 different rows with NA.\n\nDANAU SUNTER\nDANAU SUNTER DLL\n\nWe can confirm that we can drop the columns already. We can perform the step with the code chunk below.\n\njakarta_07_2021 <- jakarta_07_2021 %>% drop_na()\n\nJust to give us a piece of mind, lets plot out the border\n\ntm_shape(jakarta_07_2021)+\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\nLooking at the map, notice how the map includes all the outer islands. This is not what this exercise is about as such we would need to remove all those data points.\nI have identified that all the outer islands belong district KEPULAUAN SERIBU. All we need to do is to filter out the data.\n\njakarta_07_2021_removed <- jakarta_07_2021 %>% \n  drop_na() %>%\n  filter(KAB_KOTA != \"KEPULAUAN SERIBU\")\n\nWe can plot again to make sure that we are correct.\n\ntm_shape(jakarta_07_2021_removed)+\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#calculating-the-vaccination-rate",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#calculating-the-vaccination-rate",
    "title": "Take Home Exercise 2",
    "section": "Calculating the Vaccination Rate",
    "text": "Calculating the Vaccination Rate\n\n\n\n\n\n\nDanger\n\n\n\nNote that in this case we are using the Target as the vaccination rate as it is a better representative values of the vaccination rate. This is because COVID 19 vaccine is given in 2 doses, it is possible that that the targeted will be greater than the population of Jakarta.\n\n\nNow that we have cleaned up the data we would need to map out the Monthly Vaccination Rate.\nTo Calculate Vaccination Rate, we would need to make some calculation as the vaccination data frame does not provide the vaccination rate directly, what was given however, is the amount of people not vaccination and the target vaccination. In this case we can use the formula:\nVaccination Rate = (Target - Not Vaccinated) / Target\nWe will be performing this calculation on the vaccination data frame. The code chunk below shows calculation of the vaccination rate into a new column\n\nvaccinated_july <- July_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nNow we can take a look at the data.\n\nvaccinated_july\n\n# A tibble: 267 × 7\n   `KODE KELURAHAN` `WILAYAH KOTA`  KECAMATAN    KELUR…¹ SASARAN BELUM…² vacci…³\n   <chr>            <chr>           <chr>        <chr>     <dbl>   <dbl>   <dbl>\n 1 3172051003       JAKARTA UTARA   PADEMANGAN   ANCOL     20393   13272   0.349\n 2 3173041007       JAKARTA BARAT   TAMBORA      ANGKE     25785   16477   0.361\n 3 3175041005       JAKARTA TIMUR   KRAMAT JATI  BALE K…   25158   18849   0.251\n 4 3175031003       JAKARTA TIMUR   JATINEGARA   BALI M…    8683    5743   0.339\n 5 3175101006       JAKARTA TIMUR   CIPAYUNG     BAMBU …   22768   15407   0.323\n 6 3174031002       JAKARTA SELATAN MAMPANG PRA… BANGKA    18930   12503   0.340\n 7 3175051002       JAKARTA TIMUR   PASAR REBO   BARU      20267   11268   0.444\n 8 3175041004       JAKARTA TIMUR   KRAMAT JATI  BATU A…   41389   30358   0.267\n 9 3171071002       JAKARTA PUSAT   TANAH ABANG  BENDUN…   19008   11502   0.395\n10 3175031002       JAKARTA TIMUR   JATINEGARA   BIDARA…   32331   23395   0.276\n# … with 257 more rows, and abbreviated variable names ¹​KELURAHAN,\n#   ²​`BELUM VAKSIN`, ³​vaccinated_rate\n\n\nNotice that we have created a new column with the Vaccination Rate of the Month. With all that done we can perform our Task."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#importing-aspatial-data",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#importing-aspatial-data",
    "title": "Take Home Exercise 2",
    "section": "Importing Aspatial Data",
    "text": "Importing Aspatial Data\nThere is 12 datasets to be imported and we will be importing all of them in the code chunk below. At the same time we will also be filtering out the first row and selecting the relevant columns.\n\n\n\n\n\n\nNote\n\n\n\nFor the March Data, the data from the website reference 2 March instead.\n\n\n\njuly_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Juli 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\naug_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (1 Agustus 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nsept_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 September 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\noct_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Oktober 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nnov_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 November 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\ndec_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Desember 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\njan_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Januari 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nfeb_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Februari 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nmar_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (02 Maret 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\napr_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 April 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nmay_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Mei 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\njun_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Juni 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#calculating-vaccination-rate",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#calculating-vaccination-rate",
    "title": "Take Home Exercise 2",
    "section": "Calculating Vaccination Rate",
    "text": "Calculating Vaccination Rate\nNow we will be calculating the Monthly vaccination rate for each of the months. We will append the calculated rate to the end of the data frame.\n\nvaccinated_july <- july_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_aug <- aug_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_sept <- sept_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_oct <- oct_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_nov <- nov_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_dec <- dec_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_jan <- jan_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_feb <- feb_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_mar <- mar_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_apr <- apr_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_may <- may_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_jun <- jun_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#importing-geospatial-data",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#importing-geospatial-data",
    "title": "Take Home Exercise 2",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\nNow that we have finish calculating the vaccination rate, we can now import the sf data frame.\nIn the code chunk below, we will be filtering the relevant columns. We will also be removing the 2 outer islands with NA in the rows. We also will be applying st_transform() to transform the data into the correct coordinate system.\n\njakarta_sf <- st_read(dsn = \"data/geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\") %>%\n  select(0:9) %>%\n  drop_na() %>% \n  filter(KAB_KOTA != \"KEPULAUAN SERIBU\") %>%\n  st_transform(crs = 23845)\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Take-home\\Take-home_ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#merging-the-sf-data-frame",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#merging-the-sf-data-frame",
    "title": "Take Home Exercise 2",
    "section": "Merging the SF data frame",
    "text": "Merging the SF data frame\nNow that we can do that we can now perform the merging of the sf data frame with the vaccination data frame. We will be joining by KODE_DESA and KODE KELURAHAN for simplicity.\n\njakarta_july <- left_join(jakarta_sf, vaccinated_july,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_aug <- left_join(jakarta_sf, vaccinated_aug,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_sept <- left_join(jakarta_sf, vaccinated_sept,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_oct <- left_join(jakarta_sf, vaccinated_oct,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_nov <- left_join(jakarta_sf, vaccinated_nov,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_dec <- left_join(jakarta_sf, vaccinated_dec,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_jan <- left_join(jakarta_sf, vaccinated_jan,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_feb <- left_join(jakarta_sf, vaccinated_feb,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_mar <- left_join(jakarta_sf, vaccinated_mar,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_apr <- left_join(jakarta_sf, vaccinated_apr,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_may <- left_join(jakarta_sf, vaccinated_may,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_jun <- left_join(jakarta_sf, vaccinated_jun,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#choropleth-maps-july-2021-to-june-2022",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#choropleth-maps-july-2021-to-june-2022",
    "title": "Take Home Exercise 2",
    "section": "Choropleth Maps July 2021 to June 2022",
    "text": "Choropleth Maps July 2021 to June 2022\n\n\n\n\n\n\nNote\n\n\n\nWe will be setting the style of classification into “quantile” and setting the bins to 5.\n\n\nJuly 2021\n\njuly_choro <- tm_shape(jakarta_july)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\njuly_choro\n\n\n\n\nAugust 2021\n\naug_choro <- tm_shape(jakarta_aug)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Aug 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\naug_choro\n\n\n\n\nSeptember 2021\n\nsept_choro <- tm_shape(jakarta_sept)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Sept 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nsept_choro\n\n\n\n\nOctober 2021\n\noct_choro <- tm_shape(jakarta_oct)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Oct 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\noct_choro\n\n\n\n\nNovember 2021\n\nnov_choro <- tm_shape(jakarta_nov)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Nov 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\nnov_choro\n\n\n\n\nDecember 2021\n\ndec_choro <- tm_shape(jakarta_dec)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Dec 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\ndec_choro\n\n\n\n\nJanuary 2022\n\njan_choro <- tm_shape(jakarta_jan)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Jan 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\njan_choro\n\n\n\n\nFebruary 2022\n\nfeb_choro <- tm_shape(jakarta_feb)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Feb 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_grid(alpha =0.2)\n\nfeb_choro\n\n\n\n\nMarch 2022\n\nmar_choro <- tm_shape(jakarta_mar)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Mar 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nmar_choro\n\n\n\n\nApril 2022\n\napr_choro <- tm_shape(jakarta_apr)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Apr 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\napr_choro\n\n\n\n\nMay 2022\n\nmay_choro <- tm_shape(jakarta_may)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate May 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\nmay_choro\n\n\n\n\nJune 2022\n\njune_choro <- tm_shape(jakarta_jun)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Jun 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\njune_choro"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#analysis-of-choropleth-map",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#analysis-of-choropleth-map",
    "title": "Take Home Exercise 2",
    "section": "Analysis of Choropleth Map",
    "text": "Analysis of Choropleth Map\nTo make things easier for us to analyse, we will be arranging the maps into a grid of maps for us to see the pattern better.\n\ntmap_arrange(july_choro, aug_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(sept_choro, oct_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(nov_choro, dec_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(jan_choro, feb_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(mar_choro, apr_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(may_choro, june_choro, asp = 1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on the choropleths map plotted for the region of Jakarta, one pattern that was reviewed was that throughout the study period, the monthly vaccination rate of the entire region increases. The Monthly vaccination rate increases sharply up till the month of December before it slowed down significantly. Overall the vaccination rate throughout the region of Jakarta increases with the minimum rate increasing from 0.227 to 0781.\nHowever, one thing to note is the rate of change of vaccination rate differs significantly for each sub district. A district can fluctuate between upper quantile and the lower quantile throughout the period. This can be seen from the 2 district in the north-western region having the highest vaccination rates, but progress to a lower quantile throughout the month. The districts located at the south of Jakarta and a few districts near the north-east remains largely stable at the highest quantile throughout the period."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#step-1-deriving-contiguity-weights.",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#step-1-deriving-contiguity-weights.",
    "title": "Take Home Exercise 2",
    "section": "Step 1: Deriving contiguity weights.",
    "text": "Step 1: Deriving contiguity weights.\nBefore we calculate the Local Gi* Analysis, we would need to calculate the contiguity weights. This can be done with the code chunk below. The code chunk add 2 new columns, one of which is the idenitify nearest neighbors and the contain the weights matrix.\nWe will make use of the following functions from sfdep package:\n\nst_contiguity(): Identify the nearest neighbors. Find out more here.\nst_inverse_distance(): Calculate the inverse distance weighs. Find out more about here.\n\nThe code chunk below will compute the weight matrix for all the months:\n\nwm_jul <- jakarta_july %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_aug <- jakarta_aug %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_sept <- jakarta_sept %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_oct <- jakarta_oct %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_nov <- jakarta_nov %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_dec <- jakarta_dec %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_jan <- jakarta_jan %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_feb <- jakarta_feb %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_mar <- jakarta_mar %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_apr <- jakarta_apr %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_may <- jakarta_may %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_jun <- jakarta_jun %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#step-2-calculating-gi-statistic",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#step-2-calculating-gi-statistic",
    "title": "Take Home Exercise 2",
    "section": "Step 2: Calculating Gi* Statistic",
    "text": "Step 2: Calculating Gi* Statistic\nNow that we have the computed the weights of each for each of the vaccination rate, we can calculate the local Gi* as well. In this step, we will be performing a monte carlo simulation on the values as well.\nWe will be making use of the following function from the sfdep package:\n\nlocal_gstar_perm(): Calculate the Gi* values with simulation. It takes the target variable, nearest neighbor and weight matrix and the number of simulation. Find out more here.\n\nThe code chunk below will compute the weight matrix for all the months:\n\nHCSA_jul <- wm_jul %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_aug <- wm_aug %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_sept <- wm_sept %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_oct <- wm_oct %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_nov <- wm_nov %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_dec <- wm_dec %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_jan <- wm_jan %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_feb <- wm_feb %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_mar <- wm_mar %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_apr <- wm_apr %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_may <- wm_may %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_jun <- wm_jun %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#step-3-visualization-gi",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#step-3-visualization-gi",
    "title": "Take Home Exercise 2",
    "section": "Step 3: Visualization Gi*",
    "text": "Step 3: Visualization Gi*\nNow that we have calculated the Gi* values we can visualize it\n\nLooking at the Visualisation\n\nmap1 <- tm_shape(HCSA_jul)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"Gi* of July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\nmap1\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nJust looking at the map above does not seems to indicate, but that is not enough for us to make a conclusion. We would need to compare it with the p-value. Lets compare it with the p-value map\n\np_map <- tm_shape(HCSA_jul)+\n    tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\ntmap_arrange(map1, p_map, ncol = 2)\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nWarning: Values have found that are less than the lowest break\n\n\nWarning: Values have found that are higher than the highest break\n\n\nVariable(s) \"p_value\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nWe will need to plot out the areas that are of significant p-value. In this case we are selecting 95% significant level as selecting 99% will give an error.\n\nHCSA_jul_sig <- HCSA_jul  %>%\n  filter(p_sim < 0.05)\n\n\nHCSA_jul_map <- tm_shape(HCSA_jul)+\n  tm_polygons()+\n  tm_shape(HCSA_jul_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_jul_map\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nNow that we are able to better see the Cold Spot and Hot Spot, we can better come to a conclusion.\n\n\nFiltering of significant value\nThe code chunk below will filter out the value of significant value less than 0.05.\n\nHCSA_aug_sig <- HCSA_aug  %>%\n  filter(p_sim < 0.05)\n\nHCSA_sept_sig <- HCSA_sept  %>%\n  filter(p_sim < 0.05)\n\nHCSA_oct_sig <- HCSA_oct  %>%\n  filter(p_sim < 0.05)\n\nHCSA_nov_sig <- HCSA_nov  %>%\n  filter(p_sim < 0.05)\n\nHCSA_dec_sig <- HCSA_dec  %>%\n  filter(p_sim < 0.05)\n\nHCSA_jan_sig <- HCSA_jan  %>%\n  filter(p_sim < 0.05)\n\nHCSA_feb_sig <- HCSA_feb  %>%\n  filter(p_sim < 0.05)\n\nHCSA_mar_sig <- HCSA_mar  %>%\n  filter(p_sim < 0.05)\n\nHCSA_apr_sig <- HCSA_apr  %>%\n  filter(p_sim < 0.05)\n\nHCSA_may_sig <- HCSA_may  %>%\n  filter(p_sim < 0.05)\n\nHCSA_jun_sig <- HCSA_jun  %>%\n  filter(p_sim < 0.05)\n\n\n\nVisualizing Hot Spot and Cold Spot Area\n\n\n\n\n\n\nNote\n\n\n\nAll the maps here are images and not rendered as the map changes due to the use of monte carlo simulation. The map you generated might be different.\n\n\nJuly 2021\n\nHCSA_jul_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 5 cold spot areas and 5 hot spot areas for July 2021.\n\n\nAugust 2021\n\nHCSA_aug_map <- tm_shape(HCSA_aug)+\n  tm_polygons()+\n  tm_shape(HCSA_aug_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA August 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_aug_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 3 hot spot areas for August 2021\n\n\nSeptember 2021\n\nHCSA_sept_map <- tm_shape(HCSA_sept)+\n  tm_polygons()+\n  tm_shape(HCSA_sept_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA September 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_sept_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 5 cold spot areas and 4 hot spot areas for September 2021\n\n\nOctober 2021\n\nHCSA_oct_map <- tm_shape(HCSA_oct)+\n  tm_polygons()+\n  tm_shape(HCSA_oct_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA October 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_oct_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 6 cold spot areas and 6 hot spot areas for October 2021\n\n\nNovember 2021\n\nHCSA_nov_map <- tm_shape(HCSA_nov)+\n  tm_polygons()+\n  tm_shape(HCSA_nov_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA November 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_nov_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 5 cold spot areas and 5 hot spot areas for November 2021.\n\n\nDecember 2021\n\nHCSA_dec_map <- tm_shape(HCSA_dec)+\n  tm_polygons()+\n  tm_shape(HCSA_dec_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA December 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_dec_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 3 hot spot areas for December 2021\n\n\nJanuary 2022\n\nHCSA_jan_map <- tm_shape(HCSA_jan)+\n  tm_polygons()+\n  tm_shape(HCSA_jan_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA January 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_jan_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 4 hot spot areas for January 2022\n\n\nFebruary 2022\n\nHCSA_feb_map <- tm_shape(HCSA_feb)+\n  tm_polygons()+\n  tm_shape(HCSA_feb_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA February 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_feb_map\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 6 hot spot areas for February 2022.\n\n\nMarch 2022\n\nHCSA_mar_map <- tm_shape(HCSA_mar)+\n  tm_polygons()+\n  tm_shape(HCSA_mar_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA March 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_mar_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 4 hot spot areas for March 2022\n\n\nApril 2022\n\nHCSA_apr_map <- tm_shape(HCSA_apr)+\n  tm_polygons()+\n  tm_shape(HCSA_apr_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA April 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_apr_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 6 hot spot areas for April 2022.\n\n\nMay 2022\n\nHCSA_may_map <- tm_shape(HCSA_may)+\n  tm_polygons()+\n  tm_shape(HCSA_may_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA May 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_may_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 5 hot spot areas for May 2022.\n\n\nJune 2022\n\nHCSA_jun_map <- tm_shape(HCSA_jun)+\n  tm_polygons()+\n  tm_shape(HCSA_jun_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA June 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_jun_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 6 hot spot areas for June 2022."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#preparing-the-data-for-ehsa",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#preparing-the-data-for-ehsa",
    "title": "Take Home Exercise 2",
    "section": "Preparing the Data for EHSA",
    "text": "Preparing the Data for EHSA\nAs our Vaccination Rate Data is found in a different files we would need to combine them together to form a data set that we can use.\n\ndf <- tibble(\"month\" = as.Date(\"01/07/2021\", \"%m/%d/%Y\"), \n             \"DESA\" = jakarta_july %>% pull(DESA),\n             \"vaccinated_rate\" = jakarta_july %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/08/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_aug %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_aug %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/09/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_sept %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_sept %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/10/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_oct %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_oct %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/11/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_nov %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_nov %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/12/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_dec %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_dec %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/01/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_jan %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_jan %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/02/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_feb %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_feb %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"02/03/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_mar %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_mar %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/04/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_apr %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_apr %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/05/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_may %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_may %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/06/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_jun %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_jun %>% pull(vaccinated_rate))\n\nLets take a look at our new data frame with all the information.\n\ndf\n\n# A tibble: 3,132 × 3\n   month      DESA               vaccinated_rate\n   <date>     <chr>                        <dbl>\n 1 2021-01-07 KEAGUNGAN                    0.327\n 2 2021-01-07 GLODOK                       0.505\n 3 2021-01-07 HARAPAN MULIA                0.318\n 4 2021-01-07 CEMPAKA BARU                 0.326\n 5 2021-01-07 PASAR BARU                   0.479\n 6 2021-01-07 KARANG ANYAR                 0.348\n 7 2021-01-07 MANGGA DUA SELATAN           0.351\n 8 2021-01-07 PETOJO UTARA                 0.347\n 9 2021-01-07 SENEN                        0.396\n10 2021-01-07 BUNGUR                       0.374\n# … with 3,122 more rows\n\n\nAs we have not filter out the Outer Islands in the original sf data frame we can perform the task here:\n\nJakarta_Area_cleaned <- Jakarta_Area %>%\n  filter(KAB_KOTA != \"KEPULAUAN SERIBU\")"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#performing-man-kendall-test",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#performing-man-kendall-test",
    "title": "Take Home Exercise 2",
    "section": "Performing Man-Kendall Test",
    "text": "Performing Man-Kendall Test\n\nStep 1: Calculating Space Time Cube\nOnce we have both the time series data of the Vaccination Rate and the filtered sf data frame, we can create a space time cube using the spacetime() function. Find out more here.\n\nvaccinated_rate_st <- spacetime(df, Jakarta_Area_cleaned,\n                      .loc_col = \"DESA\",\n                      .time_col = \"month\")\n\nWe will then verified that the time series cube is correct.\n\nis_spacetime_cube(vaccinated_rate_st)\n\n[1] TRUE\n\n\n\n\nStep 2: Calculating the Spatial Weights\nIn the code chunk below, multiple actions are being done. Firstly, we will need to activate the geometry context of the space time cube, and we will create 2 new rows for the nearest neighbors and weight matrix (similar to what was done above). The difference is that because this exist as a space time cube, we will need to incorporate the weight matrix and nearest neighbors to each time slice. The following new functions are used:\n\nactivate(): activate the geometry context\nset_nbs() and set_wts(): create a new column in the data context with the same name as the column in the geometry context. Find out more here.\n\n\nvaccinated_rate_nb <- vaccinated_rate_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\nWarning in st_point_on_surface.sfc(geometry): st_point_on_surface may not give\ncorrect results for longitude/latitude data\n\n\n\n\nStep 3: Computing Gi*\nThis code chunk below calculate calculate the local Gi* for each district and month. It is similar in code to that of computing Gi* individually.\n\ngi_stars <- vaccinated_rate_nb %>% \n  group_by(month) %>% \n  mutate(gi_star = local_gstar_perm(\n    vaccinated_rate, nb, wt)) %>% \n  tidyr::unnest(gi_star)\n\n\n\nStep 4: Performing Man-Kendall Test\nIn this step, we will be performing Man-Kendall Test on each of sub district. In this case we are using the MannKendall() from the Kendall package.\n\nMannKendall(): It is a test for monotonic trend in a time series z[t] based on the Kendall rank correlation of z[t] and t. Find out more here.\n\n\nehsa <- gi_stars %>%\n  group_by(DESA) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#temporal-trends",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#temporal-trends",
    "title": "Take Home Exercise 2",
    "section": "Temporal Trends",
    "text": "Temporal Trends\nIn this section, we will be performing a deep dive into 3 different sub district and we will be analyzing the temporal trends in the area. In this case 3 sub district were selected:\n\nGEDONG: A sub district of PASAR REBO, which became a hot spot from November 2011\nKOJA: A sub district of KOJA, which is consistently a cold spot\nGELORA: A sub district of TANAH ABANG, central Jakarta with a lot of fluctuation\n\nWe will be plotting the various Graphs of the Gi* value for each of the region.\nFor this Kendall-Mann Test, we are evaluating the following Hypothesis\n\nNull hypothesis: There is no monotonic trend in the series.\nAlternate hypothesis: A trend exists. This trend can be positive, negative, or non-null.\n\n\nTemporal Trends of GEDONG sub district\n\ngedong <- gi_stars %>% \n  ungroup() %>% \n  filter(DESA == \"GEDONG\") |> \n  select(DESA, month, gi_star)\n\n\ngedong_plot <- ggplot(data = gedong, \n       aes(x = month, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(gedong_plot)\n\n\n\n\n\n\ngedong %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n    tau      sl     S     D  varS\n  <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 0.727 0.00127    48  66.0  213.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the above result, the sI is the p-value, while the S value represents the direction of the trend.\nAs the p-value < 0.05, we must reject the null Hypothesis. we can conclude that is a significant upward trend, based with a confidence of 95%. In other words, this is a sign of emerging hot spot.\n\n\n\n\nTemporal Trends of KOJA sub district\n\nkoja <- gi_stars %>% \n  ungroup() %>% \n  filter(DESA == \"KOJA\") |> \n  select(DESA, month, gi_star)\n\n\nkoja_plot <- ggplot(data = koja, \n       aes(x = month, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(koja_plot)\n\n\n\n\n\n\nkoja %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau    sl     S     D  varS\n   <dbl> <dbl> <dbl> <dbl> <dbl>\n1 0.0303 0.945     2  66.0  213.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the above result, the sI is the p-value, while the S value represents the direction of the trend.\nAs the p-value > 0.05, we must not reject the null Hypothesis. We can conclude that is a upward but insignificant trend, based with a confidence of 95%.\n\n\n\n\nTemporal Trends of GELORA sub district\n\ngelora <- gi_stars %>% \n  ungroup() %>% \n  filter(DESA == \"GELORA\") |> \n  select(DESA, month, gi_star)\n\n\ngelora_plot <- ggplot(data = gelora, \n       aes(x = month, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(gelora_plot)\n\n\n\n\n\n\ngelora %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau       sl     S     D  varS\n   <dbl>    <dbl> <dbl> <dbl> <dbl>\n1 -0.818 0.000279   -54  66.0  213.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the above result, the sI is the p-value, while the S value represents the direction of the trend.\nAs the p-value < 0.05, we must reject the null Hypothesis. We can conclude that is a significant downward trend, based with a confidence of 95%. In other words, this is an emerging cold spot."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#plotting-an-ehsa-map",
    "href": "lessons/Take-home/Take-home_ex2/Take-home_ex2.html#plotting-an-ehsa-map",
    "title": "Take Home Exercise 2",
    "section": "Plotting an EHSA Map",
    "text": "Plotting an EHSA Map\nNow that we can infer the trend from some smaller district lets look at the entire Jakarta as a whole.\n\nViewing Classification Hot and Cold Spots\nWe can make use of the emerging_hotspot_analysis() function. It takes the space time cube and combines the Gi* statistic with the Mann-Kendall test to determine if there is a temporal trend associated with local clustering of hot and cold spots. Find out more here.\n\nehsa_analysis <- emerging_hotspot_analysis(\n  x = vaccinated_rate_st, \n  .var = \"vaccinated_rate\", \n  k = 1, \n  nsim = 99\n)\n\nAfter we have completed that, lets take a look at the data.\n\nehsa_analysis\n\n# A tibble: 261 × 4\n   location             tau  p_value classification    \n   <chr>              <dbl>    <dbl> <chr>             \n 1 KEAGUNGAN          0.576 0.0112   oscilating hotspot\n 2 GLODOK             0.606 0.00749  sporadic coldspot \n 3 HARAPAN MULIA      0.636 0.00493  sporadic coldspot \n 4 CEMPAKA BARU       0.636 0.00493  sporadic coldspot \n 5 PASAR BARU         0.848 0.000162 oscilating hotspot\n 6 KARANG ANYAR       0.576 0.0112   oscilating hotspot\n 7 MANGGA DUA SELATAN 0.848 0.000162 oscilating hotspot\n 8 PETOJO UTARA       0.606 0.00749  sporadic coldspot \n 9 SENEN              0.273 0.244    sporadic coldspot \n10 BUNGUR             0.636 0.00493  sporadic coldspot \n# … with 251 more rows\n\n\nNow, we can see a bar graph of the different classification.\n\nehsa_plot <- ggplot(data = ehsa_analysis,\n       aes(x = classification)) +\n  geom_bar()\n\nggplotly(ehsa_plot)\n\n\n\n\n\n\n\nPlotting EHSA Data\n\n\n\n\n\n\nNote\n\n\n\nOne Assumption I made in the plotting the EHSA data is that a Mann-Kendall Test is usually done to determine if the area is an Emerging Hot Spot or Cold Spot. Mann-Kendall Test uses the G* values to calculate the\n\n\nBefore we can plot the EHSA data, we once again need to combine the data with a sf data frame.\n\njakarta_ehsa <- left_join(Jakarta_Area_cleaned, ehsa_analysis,\n                          by = c(\"DESA\" = \"location\"))\n\nNow that we have the necessary data, we will need to filter out the classification that are not significant.\n\nehsa_sig <- jakarta_ehsa  %>%\n  filter(p_value < 0.05)\n\nWe can now plot the graph.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(jakarta_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAt a 95% confidence level, based on the Heat Map above, we can determine that most of the areas in Jakarta are oscillating hot spot. This is followed by most areas being sporadic cold spot and then oscillating cold spot and finally no pattern being the detected.\nThe gray out area are areas that are insignificant.\nAn oscillating hot spot is expected to dominate most of the areas of the region as it make sense that most of the area have an history of being a cold spot most of the time, as government have not ramp up vaccination efforts there. An oscillating cold spot likewise is expected to dominate with opposite taking placing, with the government slowing down their efforts there. What is surprising is that there are sporadic cold spot takes second place, as this indicate, that the effort spend there is constantly shifting, most likely to the areas of oscillating hot spot as all sporadic cold spot borders at least one oscillating hot spot."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Important\n\n\n\nThis part is taken from IS415 Take Home Exercise 2. All rights belong to Dr Kam Tin Seong.\n\n\nSince late December 2019, an outbreak of a novel coronavirus disease (COVID-19; previously known as 2019-nCoV) was reported in Wuhan, China, which had subsequently affected 210 countries worldwide. In general, COVID-19 is an acute resolved disease but it can also be deadly, with a 2% case fatality rate.\nThe COVID-19 vaccination in Indonesia is an ongoing mass immunisation in response to the COVID-19 pandemic in Indonesia. On 13 January 2021, the program commenced when President Joko Widodo was vaccinated at the presidential palace. In terms of total doses given, Indonesia ranks third in Asia and fifth in the world.\nAccording to wikipedia, as of 5 February 2023 at 18:00 WIB (UTC+7), 204,266,655 people had received the first dose of the vaccine and 175,131,893 people had been fully vaccinated; 69,597,474 of them had been inoculated with the booster or the third dose, while 1,585,164 had received the fourth dose. Jakarta has the highest percentage of population fully vaccinated with 103.46%, followed by Bali and Special Region of Yogyakarta with 85.45% and 83.02% respectively.\nDespite its compactness, the cumulative vaccination rate are not evenly distributed within DKI Jakarta. The question is where are the sub-districts with relatively higher number of vaccination rate and how they changed over time."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#getting-the-data",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#getting-the-data",
    "title": "Take Home Exercise 2",
    "section": "Getting the Data",
    "text": "Getting the Data\nIn this Take Home Exercise, the main dataset we will be using would be indicated here:\n\nAspatial Data: We will be making use of the data from Riwayat File Vaksinasi DKI Jakarta. This website the vaccination data set for Jakarta. Based on the task, I have downloaded the data set from the month of July 2021 to June 2022. For this analysis, I have decided to pick the 1st Day of the Month.\nGeospatial Data: We will be making use of the DKI Jakarta administration boundary from 2019 in this case. The data can be found here."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#analyzing-the-data",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#analyzing-the-data",
    "title": "Take Home Exercise 2",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\n\nAspatial Data\nNow that we have gotten the data set, lets take a look at the data set. We will start off by loading the dataset from month of July 2021 first.\n\nJuly_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Juli 2021).xlsx\")\n\n\nJuly_2021\n\n# A tibble: 268 × 21\n   KODE KELURA…¹ WILAY…² KECAM…³ KELUR…⁴ SASARAN BELUM…⁵ JUMLA…⁶ JUMLA…⁷ TOTAL…⁸\n   <chr>         <chr>   <chr>   <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 <NA>          <NA>    <NA>    TOTAL   7739060 5041111 2696017 1181740 3877757\n 2 3172051003    JAKART… PADEMA… ANCOL     20393   13272    7114    3287   10401\n 3 3173041007    JAKART… TAMBORA ANGKE     25785   16477    9299    3221   12520\n 4 3175041005    JAKART… KRAMAT… BALE K…   25158   18849    6301    2644    8945\n 5 3175031003    JAKART… JATINE… BALI M…    8683    5743    2937    1517    4454\n 6 3175101006    JAKART… CIPAYU… BAMBU …   22768   15407    7357    3985   11342\n 7 3174031002    JAKART… MAMPAN… BANGKA    18930   12503    6421    2704    9125\n 8 3175051002    JAKART… PASAR … BARU      20267   11268    8982    4674   13656\n 9 3175041004    JAKART… KRAMAT… BATU A…   41389   30358   11020    5254   16274\n10 3171071002    JAKART… TANAH … BENDUN…   19008   11502    7499    3566   11065\n# … with 258 more rows, 12 more variables: `LANSIA\\r\\nDOSIS 1` <dbl>,\n#   `LANSIA\\r\\nDOSIS 2` <dbl>, `LANSIA TOTAL \\r\\nVAKSIN DIBERIKAN` <dbl>,\n#   `PELAYAN PUBLIK\\r\\nDOSIS 1` <dbl>, `PELAYAN PUBLIK\\r\\nDOSIS 2` <dbl>,\n#   `PELAYAN PUBLIK TOTAL\\r\\nVAKSIN DIBERIKAN` <dbl>,\n#   `GOTONG ROYONG\\r\\nDOSIS 1` <dbl>, `GOTONG ROYONG\\r\\nDOSIS 2` <dbl>,\n#   `GOTONG ROYONG TOTAL\\r\\nVAKSIN DIBERIKAN` <dbl>,\n#   `TENAGA KESEHATAN\\r\\nDOSIS 1` <dbl>, `TENAGA KESEHATAN\\r\\nDOSIS 2` <dbl>, …\n\n\nBased on what we can see, we have noticed that the data set is written in Indonesian. In this exercise, I am only interested in the first 6 columns, of which a rough translation of what each columns means is given in the table below.\n\n\n\nIndonesian Name\nRough Translation\n\n\n\n\nKODE KELURAHAN\nVillage Code\n\n\nWILAYAH KOTA\nCity Area (ADM2)\n\n\nKECAMATAN\nDistrict (ADM3)\n\n\nKELURAHAN\nUrban Village\n\n\nSASARAN\nTarget\n\n\nBELUM VAKSIN\nNot Vaccinated\n\n\n\nFurthermore, note that there is actually a row in the data frame that provides only the total count, we would need to drop the row as well. I have check through the rest of the data set and found that they are all in the same format as above, as such we can perform the same code for all the data.\nThe Code Chunk below show how we can perform all the filtering and the removal of the first row in a single code chunk\n\nJuly_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Juli 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nWe can then verify that it has been removed already.\n\nJuly_2021\n\n# A tibble: 267 × 6\n   `KODE KELURAHAN` `WILAYAH KOTA`  KECAMATAN        KELURAHAN   SASARAN BELUM…¹\n   <chr>            <chr>           <chr>            <chr>         <dbl>   <dbl>\n 1 3172051003       JAKARTA UTARA   PADEMANGAN       ANCOL         20393   13272\n 2 3173041007       JAKARTA BARAT   TAMBORA          ANGKE         25785   16477\n 3 3175041005       JAKARTA TIMUR   KRAMAT JATI      BALE KAMBA…   25158   18849\n 4 3175031003       JAKARTA TIMUR   JATINEGARA       BALI MESTER    8683    5743\n 5 3175101006       JAKARTA TIMUR   CIPAYUNG         BAMBU APUS    22768   15407\n 6 3174031002       JAKARTA SELATAN MAMPANG PRAPATAN BANGKA        18930   12503\n 7 3175051002       JAKARTA TIMUR   PASAR REBO       BARU          20267   11268\n 8 3175041004       JAKARTA TIMUR   KRAMAT JATI      BATU AMPAR    41389   30358\n 9 3171071002       JAKARTA PUSAT   TANAH ABANG      BENDUNGAN …   19008   11502\n10 3175031002       JAKARTA TIMUR   JATINEGARA       BIDARA CINA   32331   23395\n# … with 257 more rows, and abbreviated variable name ¹​`BELUM VAKSIN`\n\n\n\n\nGeospatial Data\nNow that we had taken a look at the Aspatial Data, we can take a look at the Geospatial Data.\nWe will import the Jakarta into a sf data frame with the code chunk below. In order to ensure that the spatial data is accurate, we would need to use the apply the correct crs information to the sf data frame through st_transform. As we are focusing on Jakarta, the CRS code is 23845. Find out more about Indonesia CRS here.\n\nJakarta_Area <- st_read(dsn = \"data/geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\") %>%\n  st_transform(crs = 23845)\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Take-home\\Take-home_ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nLets take a look at the data frame first. One thing you have noted is that there is a total of 162 columns in the sf data frame, and most of which is redundant in our current analysis. In this case, we are only interested in the first 9 columns of the sf data frame. The details of the data frame is as provided below:\n\n\n\nColumn Name\nRough Translation\n\n\n\n\nOBJECT_ID\nobject id\n\n\nKODE_DESA\nVillage Code\n\n\nDESA\nVillage - Similar to Urban Village\n\n\nKODE\nCode\n\n\nPROVINSI\nProvince\n\n\nKAB_KOTA\nCity District (ADM2) - Similar to City Area\n\n\nKECAMATAN\nDistrict (ADM3)\n\n\nDESA_KELAR\nVillage\n\n\nJUMLAH_PEN\nPopulations\n\n\n\nAnother important point to note is that while our Vaccination Data Frame only has 267 observations as compared to our sf data frame having 269 observations, this would mean that there is 2 extra areas in the sf data frame that we need to handle.\nUpon closer look at the data, we found out that there are 2 areas in the sf data frame that has no data, those areas are:\n\nDANAU SUNTER\nDANAU SUNTER DLL\n\nWe could just drops the rows as they have no meaningful data, but we might want to take a look more closely first. To solve the issue first we will extract filter the relevant data from the sf data set first, through the code chunk below.\n\nJakarta_Area <- st_read(dsn = \"data/geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\") %>%\n  select(0:9)\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Take-home\\Take-home_ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nWe will then perform a left join on this both of the data frame in order to find out which rows has empty data. We will make use of DESA and KELURAHAN as the common identifier.\n\njakarta_07_2021 <- left_join(Jakarta_Area, July_2021,\n                          by = c(\"DESA\" = \"KELURAHAN\"))\n\nOnce we join the data we have notice that that are 4 different rows with NA columns, 2 of them are expected, but the other 2 are a surprised:\n\nDANAU SUNTER\nDANAU SUNTER DLL\nJATIPULO\nKRENDANG\n\nI went to take a look at the data for the vaccination table and compare the results, and found that there is a slight difference in the name. The Jakarta sf Data frame record JATI PULO as JATIPULO, while for KERENDANG, it was recorded as KRENDANG instead.\nThere are 2 options to solve this issue. I can replace the name of the those two, but I choose to use the KODE_DESA and KODE KELURAHAN instead.\n\njakarta_07_2021 <- left_join(Jakarta_Area, July_2021,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\nWhen we look at the data, we spotted 2 different rows with NA.\n\nDANAU SUNTER\nDANAU SUNTER DLL\n\nWe can confirm that we can drop the columns already. We can perform the step with the code chunk below.\n\njakarta_07_2021 <- jakarta_07_2021 %>% drop_na()\n\nJust to give us a piece of mind, lets plot out the border\n\ntm_shape(jakarta_07_2021)+\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\nLooking at the map, notice how the map includes all the outer islands. This is not what this exercise is about as such we would need to remove all those data points.\nI have identified that all the outer islands belong district KEPULAUAN SERIBU. All we need to do is to filter out the data.\n\njakarta_07_2021_removed <- jakarta_07_2021 %>% \n  drop_na() %>%\n  filter(KAB_KOTA != \"KEPULAUAN SERIBU\")\n\nWe can plot again to make sure that we are correct.\n\ntm_shape(jakarta_07_2021_removed)+\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#calculating-the-vaccination-rate",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#calculating-the-vaccination-rate",
    "title": "Take Home Exercise 2",
    "section": "Calculating the Vaccination Rate",
    "text": "Calculating the Vaccination Rate\n\n\n\n\n\n\nDanger\n\n\n\nNote that in this case we are using the Target as the vaccination rate as it is a better representative values of the vaccination rate. This is because COVID 19 vaccine is given in 2 doses, it is possible that that the targeted will be greater than the population of Jakarta.\n\n\nNow that we have cleaned up the data we would need to map out the Monthly Vaccination Rate.\nTo Calculate Vaccination Rate, we would need to make some calculation as the vaccination data frame does not provide the vaccination rate directly, what was given however, is the amount of people not vaccination and the target vaccination. In this case we can use the formula:\nVaccination Rate = (Target - Not Vaccinated) / Target\nWe will be performing this calculation on the vaccination data frame. The code chunk below shows calculation of the vaccination rate into a new column\n\nvaccinated_july <- July_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nNow we can take a look at the data.\n\nvaccinated_july\n\n# A tibble: 267 × 7\n   `KODE KELURAHAN` `WILAYAH KOTA`  KECAMATAN    KELUR…¹ SASARAN BELUM…² vacci…³\n   <chr>            <chr>           <chr>        <chr>     <dbl>   <dbl>   <dbl>\n 1 3172051003       JAKARTA UTARA   PADEMANGAN   ANCOL     20393   13272   0.349\n 2 3173041007       JAKARTA BARAT   TAMBORA      ANGKE     25785   16477   0.361\n 3 3175041005       JAKARTA TIMUR   KRAMAT JATI  BALE K…   25158   18849   0.251\n 4 3175031003       JAKARTA TIMUR   JATINEGARA   BALI M…    8683    5743   0.339\n 5 3175101006       JAKARTA TIMUR   CIPAYUNG     BAMBU …   22768   15407   0.323\n 6 3174031002       JAKARTA SELATAN MAMPANG PRA… BANGKA    18930   12503   0.340\n 7 3175051002       JAKARTA TIMUR   PASAR REBO   BARU      20267   11268   0.444\n 8 3175041004       JAKARTA TIMUR   KRAMAT JATI  BATU A…   41389   30358   0.267\n 9 3171071002       JAKARTA PUSAT   TANAH ABANG  BENDUN…   19008   11502   0.395\n10 3175031002       JAKARTA TIMUR   JATINEGARA   BIDARA…   32331   23395   0.276\n# … with 257 more rows, and abbreviated variable names ¹​KELURAHAN,\n#   ²​`BELUM VAKSIN`, ³​vaccinated_rate\n\n\nNotice that we have created a new column with the Vaccination Rate of the Month. With all that done we can perform our Task."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#importing-aspatial-data",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#importing-aspatial-data",
    "title": "Take Home Exercise 2",
    "section": "Importing Aspatial Data",
    "text": "Importing Aspatial Data\nThere is 12 datasets to be imported and we will be importing all of them in the code chunk below. At the same time we will also be filtering out the first row and selecting the relevant columns.\n\n\n\n\n\n\nNote\n\n\n\nFor the March Data, the data from the website reference 2 March instead.\n\n\n\njuly_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Juli 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\naug_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (1 Agustus 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nsept_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 September 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\noct_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Oktober 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nnov_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 November 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\ndec_2021 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Desember 2021).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\njan_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Januari 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nfeb_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Februari 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nmar_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (02 Maret 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\napr_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 April 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\nmay_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Mei 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))\n\njun_2022 <- read_excel(\"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 Juni 2022).xlsx\") %>% \n  select(0:6) %>%\n  filter(!row_number() %in% c(1))"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#calculating-vaccination-rate",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#calculating-vaccination-rate",
    "title": "Take Home Exercise 2",
    "section": "Calculating Vaccination Rate",
    "text": "Calculating Vaccination Rate\nNow we will be calculating the Monthly vaccination rate for each of the months. We will append the calculated rate to the end of the data frame.\n\nvaccinated_july <- july_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_aug <- aug_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_sept <- sept_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_oct <- oct_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_nov <- nov_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_dec <- dec_2021 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_jan <- jan_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_feb <- feb_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_mar <- mar_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_apr <- apr_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_may <- may_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)\n\nvaccinated_jun <- jun_2022 %>% \n  mutate(vaccinated_rate = (`SASARAN` - `BELUM VAKSIN`)/`SASARAN`)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#importing-geospatial-data",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#importing-geospatial-data",
    "title": "Take Home Exercise 2",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\nNow that we have finish calculating the vaccination rate, we can now import the sf data frame.\nIn the code chunk below, we will be filtering the relevant columns. We will also be removing the 2 outer islands with NA in the rows. We also will be applying st_transform() to transform the data into the correct coordinate system.\n\njakarta_sf <- st_read(dsn = \"data/geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\") %>%\n  select(0:9) %>%\n  drop_na() %>% \n  filter(KAB_KOTA != \"KEPULAUAN SERIBU\") %>%\n  st_transform(crs = 23845)\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\hxchen-2019\\birdie\\lessons\\Take-home\\Take-home_ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#merging-the-sf-data-frame",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#merging-the-sf-data-frame",
    "title": "Take Home Exercise 2",
    "section": "Merging the SF data frame",
    "text": "Merging the SF data frame\nNow that we can do that we can now perform the merging of the sf data frame with the vaccination data frame. We will be joining by KODE_DESA and KODE KELURAHAN for simplicity.\n\njakarta_july <- left_join(jakarta_sf, vaccinated_july,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_aug <- left_join(jakarta_sf, vaccinated_aug,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_sept <- left_join(jakarta_sf, vaccinated_sept,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_oct <- left_join(jakarta_sf, vaccinated_oct,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_nov <- left_join(jakarta_sf, vaccinated_nov,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_dec <- left_join(jakarta_sf, vaccinated_dec,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_jan <- left_join(jakarta_sf, vaccinated_jan,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_feb <- left_join(jakarta_sf, vaccinated_feb,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_mar <- left_join(jakarta_sf, vaccinated_mar,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_apr <- left_join(jakarta_sf, vaccinated_apr,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_may <- left_join(jakarta_sf, vaccinated_may,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))\n\njakarta_jun <- left_join(jakarta_sf, vaccinated_jun,\n                             by = c(\"KODE_DESA\" = \"KODE KELURAHAN\"))"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#choropleth-maps-july-2021-to-june-2022",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#choropleth-maps-july-2021-to-june-2022",
    "title": "Take Home Exercise 2",
    "section": "Choropleth Maps July 2021 to June 2022",
    "text": "Choropleth Maps July 2021 to June 2022\n\n\n\n\n\n\nNote\n\n\n\nWe will be setting the style of classification into “quantile” and setting the bins to 5.\n\n\nJuly 2021\n\njuly_choro <- tm_shape(jakarta_july)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\njuly_choro\n\n\n\n\nAugust 2021\n\naug_choro <- tm_shape(jakarta_aug)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Aug 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\naug_choro\n\n\n\n\nSeptember 2021\n\nsept_choro <- tm_shape(jakarta_sept)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Sept 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nsept_choro\n\n\n\n\nOctober 2021\n\noct_choro <- tm_shape(jakarta_oct)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Oct 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\noct_choro\n\n\n\n\nNovember 2021\n\nnov_choro <- tm_shape(jakarta_nov)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Nov 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\nnov_choro\n\n\n\n\nDecember 2021\n\ndec_choro <- tm_shape(jakarta_dec)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Dec 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\ndec_choro\n\n\n\n\nJanuary 2022\n\njan_choro <- tm_shape(jakarta_jan)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Jan 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\njan_choro\n\n\n\n\nFebruary 2022\n\nfeb_choro <- tm_shape(jakarta_feb)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Feb 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_grid(alpha =0.2)\n\nfeb_choro\n\n\n\n\nMarch 2022\n\nmar_choro <- tm_shape(jakarta_mar)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Mar 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nmar_choro\n\n\n\n\nApril 2022\n\napr_choro <- tm_shape(jakarta_apr)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Apr 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\napr_choro\n\n\n\n\nMay 2022\n\nmay_choro <- tm_shape(jakarta_may)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate May 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\nmay_choro\n\n\n\n\nJune 2022\n\njune_choro <- tm_shape(jakarta_jun)+\n  tm_fill(\"vaccinated_rate\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          n = 5,\n          title = \"Vaccination Rate\") +\n  tm_layout(main.title = \"Jakarta Vaccination Rate Jun 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\njune_choro"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#analysis-of-choropleth-map",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#analysis-of-choropleth-map",
    "title": "Take Home Exercise 2",
    "section": "Analysis of Choropleth Map",
    "text": "Analysis of Choropleth Map\nTo make things easier for us to analyse, we will be arranging the maps into a grid of maps for us to see the pattern better.\n\ntmap_arrange(july_choro, aug_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(sept_choro, oct_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(nov_choro, dec_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(jan_choro, feb_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(mar_choro, apr_choro, asp = 1, ncol=2)\n\n\n\ntmap_arrange(may_choro, june_choro, asp = 1, ncol=2)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on the choropleths map plotted for the region of Jakarta, one pattern that was reviewed was that throughout the study period, the monthly vaccination rate of the entire region increases. The Monthly vaccination rate increases sharply up till the month of December before it slowed down significantly. Overall the vaccination rate throughout the region of Jakarta increases with the minimum rate increasing from 0.227 to 0781.\nHowever, one thing to note is the rate of change of vaccination rate differs significantly for each sub district. A district can fluctuate between upper quantile and the lower quantile throughout the period. This can be seen from the 2 district in the north-western region having the highest vaccination rates, but progress to a lower quantile throughout the month. The districts located at the south of Jakarta and a few districts near the north-east remains largely stable at the highest quantile throughout the period."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#step-1-deriving-contiguity-weights.",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#step-1-deriving-contiguity-weights.",
    "title": "Take Home Exercise 2",
    "section": "Step 1: Deriving contiguity weights.",
    "text": "Step 1: Deriving contiguity weights.\nBefore we calculate the Local Gi* Analysis, we would need to calculate the contiguity weights. This can be done with the code chunk below. The code chunk add 2 new columns, one of which is the idenitify nearest neighbors and the contain the weights matrix.\nWe will make use of the following functions from sfdep package:\n\nst_contiguity(): Identify the nearest neighbors. Find out more here.\nst_inverse_distance(): Calculate the inverse distance weighs. Find out more about here.\n\nThe code chunk below will compute the weight matrix for all the months:\n\nwm_jul <- jakarta_july %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_aug <- jakarta_aug %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_sept <- jakarta_sept %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_oct <- jakarta_oct %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_nov <- jakarta_nov %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_dec <- jakarta_dec %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_jan <- jakarta_jan %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_feb <- jakarta_feb %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_mar <- jakarta_mar %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_apr <- jakarta_apr %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_may <- jakarta_may %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface.\n\nwm_jun <- jakarta_jun %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n! Polygon provided. Using point on surface."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#step-2-calculating-gi-statistic",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#step-2-calculating-gi-statistic",
    "title": "Take Home Exercise 2",
    "section": "Step 2: Calculating Gi* Statistic",
    "text": "Step 2: Calculating Gi* Statistic\nNow that we have the computed the weights of each for each of the vaccination rate, we can calculate the local Gi* as well. In this step, we will be performing a monte carlo simulation on the values as well.\nWe will be making use of the following function from the sfdep package:\n\nlocal_gstar_perm(): Calculate the Gi* values with simulation. It takes the target variable, nearest neighbor and weight matrix and the number of simulation. Find out more here.\n\nThe code chunk below will compute the weight matrix for all the months:\n\nHCSA_jul <- wm_jul %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_aug <- wm_aug %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_sept <- wm_sept %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_oct <- wm_oct %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_nov <- wm_nov %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_dec <- wm_dec %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_jan <- wm_jan %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_feb <- wm_feb %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_mar <- wm_mar %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_apr <- wm_apr %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_may <- wm_may %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)\n\nHCSA_jun <- wm_jun %>% \n  mutate(local_Gi = local_gstar_perm(\n    vaccinated_rate, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_Gi)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#step-3-visualization-gi",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#step-3-visualization-gi",
    "title": "Take Home Exercise 2",
    "section": "Step 3: Visualization Gi*",
    "text": "Step 3: Visualization Gi*\nNow that we have calculated the Gi* values we can visualize it\n\nLooking at the Visualisation\n\nmap1 <- tm_shape(HCSA_jul)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"Gi* of July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\nmap1\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nJust looking at the map above does not seems to indicate, but that is not enough for us to make a conclusion. We would need to compare it with the p-value. Lets compare it with the p-value map\n\np_map <- tm_shape(HCSA_jul)+\n    tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) \n\ntmap_arrange(map1, p_map, ncol = 2)\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nWarning: Values have found that are less than the lowest break\n\n\nWarning: Values have found that are higher than the highest break\n\n\nVariable(s) \"p_value\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nWe will need to plot out the areas that are of significant p-value. In this case we are selecting 95% significant level as selecting 99% will give an error.\n\nHCSA_jul_sig <- HCSA_jul  %>%\n  filter(p_sim < 0.05)\n\n\nHCSA_jul_map <- tm_shape(HCSA_jul)+\n  tm_polygons()+\n  tm_shape(HCSA_jul_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA July 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_jul_map\n\nVariable(s) \"gi_star\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nNow that we are able to better see the Cold Spot and Hot Spot, we can better come to a conclusion.\n\n\nFiltering of significant value\nThe code chunk below will filter out the value of significant value less than 0.05.\n\nHCSA_aug_sig <- HCSA_aug  %>%\n  filter(p_sim < 0.05)\n\nHCSA_sept_sig <- HCSA_sept  %>%\n  filter(p_sim < 0.05)\n\nHCSA_oct_sig <- HCSA_oct  %>%\n  filter(p_sim < 0.05)\n\nHCSA_nov_sig <- HCSA_nov  %>%\n  filter(p_sim < 0.05)\n\nHCSA_dec_sig <- HCSA_dec  %>%\n  filter(p_sim < 0.05)\n\nHCSA_jan_sig <- HCSA_jan  %>%\n  filter(p_sim < 0.05)\n\nHCSA_feb_sig <- HCSA_feb  %>%\n  filter(p_sim < 0.05)\n\nHCSA_mar_sig <- HCSA_mar  %>%\n  filter(p_sim < 0.05)\n\nHCSA_apr_sig <- HCSA_apr  %>%\n  filter(p_sim < 0.05)\n\nHCSA_may_sig <- HCSA_may  %>%\n  filter(p_sim < 0.05)\n\nHCSA_jun_sig <- HCSA_jun  %>%\n  filter(p_sim < 0.05)\n\n\n\nVisualizing Hot Spot and Cold Spot Area\n\n\n\n\n\n\nNote\n\n\n\nAll the maps here are images and not rendered as the map changes due to the use of monte carlo simulation. The map you generated might be different.\n\n\nJuly 2021\n\nHCSA_jul_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 5 cold spot areas and 5 hot spot areas for July 2021.\n\n\nAugust 2021\n\nHCSA_aug_map <- tm_shape(HCSA_aug)+\n  tm_polygons()+\n  tm_shape(HCSA_aug_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA August 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_aug_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 3 hot spot areas for August 2021\n\n\nSeptember 2021\n\nHCSA_sept_map <- tm_shape(HCSA_sept)+\n  tm_polygons()+\n  tm_shape(HCSA_sept_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA September 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_sept_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 5 cold spot areas and 4 hot spot areas for September 2021\n\n\nOctober 2021\n\nHCSA_oct_map <- tm_shape(HCSA_oct)+\n  tm_polygons()+\n  tm_shape(HCSA_oct_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA October 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_oct_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 6 cold spot areas and 6 hot spot areas for October 2021\n\n\nNovember 2021\n\nHCSA_nov_map <- tm_shape(HCSA_nov)+\n  tm_polygons()+\n  tm_shape(HCSA_nov_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA November 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_nov_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 5 cold spot areas and 5 hot spot areas for November 2021.\n\n\nDecember 2021\n\nHCSA_dec_map <- tm_shape(HCSA_dec)+\n  tm_polygons()+\n  tm_shape(HCSA_dec_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA December 2021\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_dec_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 3 hot spot areas for December 2021\n\n\nJanuary 2022\n\nHCSA_jan_map <- tm_shape(HCSA_jan)+\n  tm_polygons()+\n  tm_shape(HCSA_jan_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA January 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_jan_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 4 hot spot areas for January 2022\n\n\nFebruary 2022\n\nHCSA_feb_map <- tm_shape(HCSA_feb)+\n  tm_polygons()+\n  tm_shape(HCSA_feb_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA February 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_feb_map\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 6 hot spot areas for February 2022.\n\n\nMarch 2022\n\nHCSA_mar_map <- tm_shape(HCSA_mar)+\n  tm_polygons()+\n  tm_shape(HCSA_mar_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA March 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_mar_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 4 hot spot areas for March 2022\n\n\nApril 2022\n\nHCSA_apr_map <- tm_shape(HCSA_apr)+\n  tm_polygons()+\n  tm_shape(HCSA_apr_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA April 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_apr_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 6 hot spot areas for April 2022.\n\n\nMay 2022\n\nHCSA_may_map <- tm_shape(HCSA_may)+\n  tm_polygons()+\n  tm_shape(HCSA_may_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA May 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_may_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 5 hot spot areas for May 2022.\n\n\nJune 2022\n\nHCSA_jun_map <- tm_shape(HCSA_jun)+\n  tm_polygons()+\n  tm_shape(HCSA_jun_sig)+\n  tm_fill(\"gi_star\")+  \n  tm_layout(main.title = \"HCSA June 2022\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nHCSA_jun_map\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBased on a 95% confidence interval, we can conclude that there is a total of 4 cold spot areas and 6 hot spot areas for June 2022."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#preparing-the-data-for-ehsa",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#preparing-the-data-for-ehsa",
    "title": "Take Home Exercise 2",
    "section": "Preparing the Data for EHSA",
    "text": "Preparing the Data for EHSA\nAs our Vaccination Rate Data is found in a different files we would need to combine them together to form a data set that we can use.\n\ndf <- tibble(\"month\" = as.Date(\"01/07/2021\", \"%m/%d/%Y\"), \n             \"DESA\" = jakarta_july %>% pull(DESA),\n             \"vaccinated_rate\" = jakarta_july %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/08/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_aug %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_aug %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/09/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_sept %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_sept %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/10/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_oct %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_oct %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/11/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_nov %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_nov %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/12/2021\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_dec %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_dec %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/01/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_jan %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_jan %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/02/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_feb %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_feb %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"02/03/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_mar %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_mar %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/04/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_apr %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_apr %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/05/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_may %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_may %>% pull(vaccinated_rate))\n\ndf <- df %>% add_row(\"month\" = as.Date(\"01/06/2022\", \"%m/%d/%Y\"), \n               \"DESA\" = jakarta_jun %>% pull(DESA),\n               \"vaccinated_rate\" = jakarta_jun %>% pull(vaccinated_rate))\n\nLets take a look at our new data frame with all the information.\n\ndf\n\n# A tibble: 3,132 × 3\n   month      DESA               vaccinated_rate\n   <date>     <chr>                        <dbl>\n 1 2021-01-07 KEAGUNGAN                    0.327\n 2 2021-01-07 GLODOK                       0.505\n 3 2021-01-07 HARAPAN MULIA                0.318\n 4 2021-01-07 CEMPAKA BARU                 0.326\n 5 2021-01-07 PASAR BARU                   0.479\n 6 2021-01-07 KARANG ANYAR                 0.348\n 7 2021-01-07 MANGGA DUA SELATAN           0.351\n 8 2021-01-07 PETOJO UTARA                 0.347\n 9 2021-01-07 SENEN                        0.396\n10 2021-01-07 BUNGUR                       0.374\n# … with 3,122 more rows\n\n\nAs we have not filter out the Outer Islands in the original sf data frame we can perform the task here:\n\nJakarta_Area_cleaned <- Jakarta_Area %>%\n  filter(KAB_KOTA != \"KEPULAUAN SERIBU\")"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#performing-man-kendall-test",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#performing-man-kendall-test",
    "title": "Take Home Exercise 2",
    "section": "Performing Man-Kendall Test",
    "text": "Performing Man-Kendall Test\n\nStep 1: Calculating Space Time Cube\nOnce we have both the time series data of the Vaccination Rate and the filtered sf data frame, we can create a space time cube using the spacetime() function. Find out more here.\n\nvaccinated_rate_st <- spacetime(df, Jakarta_Area_cleaned,\n                      .loc_col = \"DESA\",\n                      .time_col = \"month\")\n\nWe will then verified that the time series cube is correct.\n\nis_spacetime_cube(vaccinated_rate_st)\n\n[1] TRUE\n\n\n\n\nStep 2: Calculating the Spatial Weights\nIn the code chunk below, multiple actions are being done. Firstly, we will need to activate the geometry context of the space time cube, and we will create 2 new rows for the nearest neighbors and weight matrix (similar to what was done above). The difference is that because this exist as a space time cube, we will need to incorporate the weight matrix and nearest neighbors to each time slice. The following new functions are used:\n\nactivate(): activate the geometry context\nset_nbs() and set_wts(): create a new column in the data context with the same name as the column in the geometry context. Find out more here.\n\n\nvaccinated_rate_nb <- vaccinated_rate_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\n! Polygon provided. Using point on surface.\n\n\nWarning in st_point_on_surface.sfc(geometry): st_point_on_surface may not give\ncorrect results for longitude/latitude data\n\n\n\n\nStep 3: Computing Gi*\nThis code chunk below calculate calculate the local Gi* for each district and month. It is similar in code to that of computing Gi* individually.\n\ngi_stars <- vaccinated_rate_nb %>% \n  group_by(month) %>% \n  mutate(gi_star = local_gstar_perm(\n    vaccinated_rate, nb, wt)) %>% \n  tidyr::unnest(gi_star)\n\n\n\nStep 4: Performing Man-Kendall Test\nIn this step, we will be performing Man-Kendall Test on each of sub district. In this case we are using the MannKendall() from the Kendall package.\n\nMannKendall(): It is a test for monotonic trend in a time series z[t] based on the Kendall rank correlation of z[t] and t. Find out more here.\n\n\nehsa <- gi_stars %>%\n  group_by(DESA) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)"
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#temporal-trends",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#temporal-trends",
    "title": "Take Home Exercise 2",
    "section": "Temporal Trends",
    "text": "Temporal Trends\nIn this section, we will be performing a deep dive into 3 different sub district and we will be analyzing the temporal trends in the area. In this case 3 sub district were selected:\n\nGEDONG: A sub district of PASAR REBO, which became a hot spot from November 2011\nKOJA: A sub district of KOJA, which is consistently a cold spot\nGELORA: A sub district of TANAH ABANG, central Jakarta with a lot of fluctuation\n\nWe will be plotting the various Graphs of the Gi* value for each of the region.\nFor this Kendall-Mann Test, we are evaluating the following Hypothesis\n\nNull hypothesis: There is no monotonic trend in the series.\nAlternate hypothesis: A trend exists. This trend can be positive, negative, or non-null.\n\n\nTemporal Trends of GEDONG sub district\n\ngedong <- gi_stars %>% \n  ungroup() %>% \n  filter(DESA == \"GEDONG\") |> \n  select(DESA, month, gi_star)\n\n\ngedong_plot <- ggplot(data = gedong, \n       aes(x = month, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(gedong_plot)\n\n\n\n\n\n\ngedong %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n    tau      sl     S     D  varS\n  <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 0.727 0.00127    48  66.0  213.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the above result, the sI is the p-value, while the S value represents the direction of the trend.\nAs the p-value < 0.05, we must reject the null Hypothesis. we can conclude that is a significant upward trend, based with a confidence of 95%. In other words, this is a sign of emerging hot spot.\n\n\n\n\nTemporal Trends of KOJA sub district\n\nkoja <- gi_stars %>% \n  ungroup() %>% \n  filter(DESA == \"KOJA\") |> \n  select(DESA, month, gi_star)\n\n\nkoja_plot <- ggplot(data = koja, \n       aes(x = month, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(koja_plot)\n\n\n\n\n\n\nkoja %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau    sl     S     D  varS\n   <dbl> <dbl> <dbl> <dbl> <dbl>\n1 0.0303 0.945     2  66.0  213.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the above result, the sI is the p-value, while the S value represents the direction of the trend.\nAs the p-value > 0.05, we must not reject the null Hypothesis. We can conclude that is a upward but insignificant trend, based with a confidence of 95%.\n\n\n\n\nTemporal Trends of GELORA sub district\n\ngelora <- gi_stars %>% \n  ungroup() %>% \n  filter(DESA == \"GELORA\") |> \n  select(DESA, month, gi_star)\n\n\ngelora_plot <- ggplot(data = gelora, \n       aes(x = month, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(gelora_plot)\n\n\n\n\n\n\ngelora %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau       sl     S     D  varS\n   <dbl>    <dbl> <dbl> <dbl> <dbl>\n1 -0.818 0.000279   -54  66.0  213.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the above result, the sI is the p-value, while the S value represents the direction of the trend.\nAs the p-value < 0.05, we must reject the null Hypothesis. We can conclude that is a significant downward trend, based with a confidence of 95%. In other words, this is an emerging cold spot."
  },
  {
    "objectID": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#plotting-an-ehsa-map",
    "href": "lessons/Take-home/Take-home_ex3/Take-home_ex3.html#plotting-an-ehsa-map",
    "title": "Take Home Exercise 2",
    "section": "Plotting an EHSA Map",
    "text": "Plotting an EHSA Map\nNow that we can infer the trend from some smaller district lets look at the entire Jakarta as a whole.\n\nViewing Classification Hot and Cold Spots\nWe can make use of the emerging_hotspot_analysis() function. It takes the space time cube and combines the Gi* statistic with the Mann-Kendall test to determine if there is a temporal trend associated with local clustering of hot and cold spots. Find out more here.\n\nehsa_analysis <- emerging_hotspot_analysis(\n  x = vaccinated_rate_st, \n  .var = \"vaccinated_rate\", \n  k = 1, \n  nsim = 99\n)\n\nAfter we have completed that, lets take a look at the data.\n\nehsa_analysis\n\n# A tibble: 261 × 4\n   location             tau  p_value classification    \n   <chr>              <dbl>    <dbl> <chr>             \n 1 KEAGUNGAN          0.576 0.0112   oscilating hotspot\n 2 GLODOK             0.606 0.00749  sporadic coldspot \n 3 HARAPAN MULIA      0.636 0.00493  sporadic coldspot \n 4 CEMPAKA BARU       0.636 0.00493  sporadic coldspot \n 5 PASAR BARU         0.848 0.000162 oscilating hotspot\n 6 KARANG ANYAR       0.576 0.0112   oscilating hotspot\n 7 MANGGA DUA SELATAN 0.848 0.000162 oscilating hotspot\n 8 PETOJO UTARA       0.606 0.00749  sporadic coldspot \n 9 SENEN              0.273 0.244    sporadic coldspot \n10 BUNGUR             0.636 0.00493  sporadic coldspot \n# … with 251 more rows\n\n\nNow, we can see a bar graph of the different classification.\n\nehsa_plot <- ggplot(data = ehsa_analysis,\n       aes(x = classification)) +\n  geom_bar()\n\nggplotly(ehsa_plot)\n\n\n\n\n\n\n\nPlotting EHSA Data\n\n\n\n\n\n\nNote\n\n\n\nOne Assumption I made in the plotting the EHSA data is that a Mann-Kendall Test is usually done to determine if the area is an Emerging Hot Spot or Cold Spot. Mann-Kendall Test uses the G* values to calculate the\n\n\nBefore we can plot the EHSA data, we once again need to combine the data with a sf data frame.\n\njakarta_ehsa <- left_join(Jakarta_Area_cleaned, ehsa_analysis,\n                          by = c(\"DESA\" = \"location\"))\n\nNow that we have the necessary data, we will need to filter out the classification that are not significant.\n\nehsa_sig <- jakarta_ehsa  %>%\n  filter(p_value < 0.05)\n\nWe can now plot the graph.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(jakarta_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAt a 95% confidence level, based on the Heat Map above, we can determine that most of the areas in Jakarta are oscillating hot spot. This is followed by most areas being sporadic cold spot and then oscillating cold spot and finally no pattern being the detected.\nThe gray out area are areas that are insignificant.\nAn oscillating hot spot is expected to dominate most of the areas of the region as it make sense that most of the area have an history of being a cold spot most of the time, as government have not ramp up vaccination efforts there. An oscillating cold spot likewise is expected to dominate with opposite taking placing, with the government slowing down their efforts there. What is surprising is that there are sporadic cold spot takes second place, as this indicate, that the effort spend there is constantly shifting, most likely to the areas of oscillating hot spot as all sporadic cold spot borders at least one oscillating hot spot."
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html",
    "href": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html",
    "title": "Hands On Exercise 8",
    "section": "",
    "text": "pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#reading-the-data",
    "href": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#reading-the-data",
    "title": "Hands On Exercise 8",
    "section": "2 Reading the Data",
    "text": "2 Reading the Data\n\n2.1 Importing the Data\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/school/Documents/GitHub/Week1/lessons/Hands-on/Hands-on-Ex8/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nAfterwards, we would need to update the CRS infomration to the correct code\n\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\nLets check to verify the newly transformed fcode\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNoticed that the EPSG is now 3414, which is what we want./\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\nThe step above is not really necessary but it is good to have.\n\n\n2.2 Importing Aspatial Dataset\nWe will now be importing the Aspatial Dataset\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nWith glimpse we can take alook at the data strucure\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             <dbl> 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            <dbl> 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             <dbl> 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nWell the data does look a but confusing why not we summarize the data for easier viewing.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n2.3 Converting Aspatial to SF object\nWe have done the steps before in the previous Hands On Exercise there is no need to do so again.\n\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLI…¹ AREA_…²   AGE PROX_…³ PROX_…⁴ PROX_…⁵ PROX_…⁶ PROX_…⁷ PROX_…⁸\n     <dbl>   <dbl>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1   118635 3000000     309    30    7.94   0.166   2.52     6.62   1.77   0.0584\n2   288420 3880000     290    32    6.61   0.280   1.93     7.51   0.545  0.616 \n3   267833 3325000     248    33    6.90   0.429   0.502    6.46   0.378  0.141 \n4   258380 4250000     127     7    4.04   0.395   1.99     4.91   1.68   0.382 \n5   467169 1400000     145    28   11.8    0.119   1.12     6.41   0.565  0.461 \n6   466472 1320000     139    22   10.3    0.125   0.789    5.09   0.781  0.0994\n# … with 12 more variables: PROX_MRT <dbl>, PROX_PARK <dbl>,\n#   PROX_PRIMARY_SCH <dbl>, PROX_TOP_PRIMARY_SCH <dbl>,\n#   PROX_SHOPPING_MALL <dbl>, PROX_SUPERMARKET <dbl>, PROX_BUS_STOP <dbl>,\n#   NO_Of_UNITS <dbl>, FAMILY_FRIENDLY <dbl>, FREEHOLD <dbl>,\n#   LEASEHOLD_99YR <dbl>, geometry <POINT [m]>, and abbreviated variable names\n#   ¹​SELLING_PRICE, ²​AREA_SQM, ³​PROX_CBD, ⁴​PROX_CHILDCARE, ⁵​PROX_ELDERLYCARE,\n#   ⁶​PROX_URA_GROWTH_AREA, ⁷​PROX_HAWKER_MARKET, ⁸​PROX_KINDERGARTEN"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#eda",
    "href": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#eda",
    "title": "Hands On Exercise 8",
    "section": "3 EDA",
    "text": "3 EDA\nNow that we have a sf data frame we can visualise our dataset and perfomr some EDA on it.\n\n3.1 EDA using statistical graphics\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nNotice how the data is skewed. One method of normalizeing skewed data is to perform log transformation.\n\ncondo_resale.sf <- condo_resale.sf %>%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nLets take a look at the log data\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nNotice how the logs is now distributed more nicely\n\nAREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggpubr::ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n3.2 Drawing Statistical Point Map\nWe would want to reveal the geospatial distribution of the prices.\nBut we would prefer to have an interactive map\n\ntmap_mode(\"view\")\n\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\n  tmap_options(check.and.fix = TRUE) +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nWe would need to set it to plot mode again to prevent other maps from being interactive.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#hedonic-pricing-modeling",
    "href": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#hedonic-pricing-modeling",
    "title": "Hands On Exercise 8",
    "section": "4 Hedonic Pricing Modeling",
    "text": "4 Hedonic Pricing Modeling\nWe can build it using lm() of R base\n\n4.1 Simple Linear Regression method.\n\ncondo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nNow that we have a linear regression method, we can take a look at the common stats.\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: < 2.2e-16\n\n\nfrom the values above we can actually inteprete the formula as\ny = -258121.1 + 14719\nIt is easier to view it in a scatterplot\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nHowever, a simple linear regression model is not the best in explanaing the model at all. We would need to make use of Multiple Linear Regression Model."
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#multiple-linear-regression-method",
    "href": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#multiple-linear-regression-method",
    "title": "Hands On Exercise 8",
    "section": "5 Multiple Linear Regression Method",
    "text": "5 Multiple Linear Regression Method\nBefore we use multiple linear regression method we need to see how correlated the factors are first.\nThis can be done with the use of a corrplot\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\nNotice how Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them. For now we will choose Freehold over the other.\n\n5.1 Building a hedonic pricing model using multiple linear regression method\n\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  < 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  < 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  < 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  < 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: < 2.2e-16\n\n\nThe above data is now complete, but it is clear that not all the factors are statistically significant, we would need to filter those variable out.\n\n\n5.2 olsrr method.\nWe will first filter those variable out first, before we calibrate with the olsrr method\n\ncondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                             Model Summary                               \n------------------------------------------------------------------------\nR                       0.807       RMSE                     755957.289 \nR-Squared               0.651       Coef. Var                    43.168 \nAdj. R-Squared          0.647       MSE                571471422208.591 \nPred R-Squared          0.638       MAE                      414819.628 \n------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n5.3 GTsummary method.\nAnother method that we can use as well would be making use of the gtsummary package as well.\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWith this packages, the model statistic can be included as well.\n\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = <0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n5.4 Checking for multicolinearity\nOLSRR provide quite alot of useful methods to help us build better models.\nIn this case, we are testing for multicolinearity\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n5.5 Testing for Non-Linearity\nAnother assumption of linear regression is that the dependent variables are linearly related to the independent variables.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n5.6 Test for Normality\nAnother assumption in Linear Regression is that all the variables in normal.\nWe can test this out with\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model resembles a normal distribution.\nAnother way to view it is to see it in a table.\n\nols_test_normality(condo.mlr1)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n5.7 Testing for Spatial Autocorrelation.\nAs we are testing with geographically reference attributes, we also need to visialised the residual of the hedonic pricing model.\nBefore we could do that we need to convert it to the spatial point data frame\n\nmlr.output <- as.data.frame(condo.mlr1$residuals)\n\nWe will join it with the condo_resale.sf object\n\ncondo_resale.res.sf <- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %>%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\nwe will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame\n\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNow we can take a look at it\n\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThere seems to be sign of spatial auto correlation.\n\n5.7.1 Moran’s I Test\nWe will need to perform Moran’s I test to prove out observationb\n\nnb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nWe will then convert it into spatial weight\n\nnb_lw <- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNow we can perform a Moran test\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThis is the conclusion.\nThe Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#building-hedonic-pricing-models-using-gwmodel.",
    "href": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#building-hedonic-pricing-models-using-gwmodel.",
    "title": "Hands On Exercise 8",
    "section": "6 Building Hedonic Pricing Models using GWmodel.",
    "text": "6 Building Hedonic Pricing Models using GWmodel.\n\n6.1 Building Fixed Bandwidth GWR Model\nWe would firt need to compute a fixed bandwidth first. This can be done with the bw.gwr package.\n\n6.1.1 Calculating Fixed Bandwidth\nThere are two main methods CV cross-validation approach and AIC corrected (AICc) approach. In this case we are using the CV approach.\n\nbw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.37828e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.341 CV score: 4.721292e+14 \nFixed bandwidth: 971.3407 CV score: 4.721292e+14 \nFixed bandwidth: 971.3409 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \n\n\nIn this case the recommended bandwifth is 971.3408 meters. It is in meters because ESGR is measured in meters.\n\n\n6.1.2 GWModel Method - fixed bandwidth\nWe can now calibrate the model using fixed bandwidth and kernel gaussian. We can choose other kernel.\n\ngwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nWe can take alook at the model output\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-03-11 23:10:12 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3408 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4645e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9008e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5206e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793007\n   AREA_SQM                 21575\n   AGE                     434200\n   PROX_CBD               2704591\n   PROX_CHILDCARE         1654088\n   PROX_ELDERLYCARE      38867786\n   PROX_URA_GROWTH_AREA  78515685\n   PROX_MRT               3124310\n   PROX_PARK             18122416\n   PROX_PRIMARY_SCH       4637495\n   PROX_SHOPPING_MALL     1529951\n   PROX_BUS_STOP         11342166\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073633\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3803 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6197 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534071e+14 \n   R-square value:  0.8909911 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2023-03-11 23:10:13 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\n6.2 Building Adaptive Bandwidth GWR Model\n\n6.2.1 Computing the adaptive Bandwidth\nIt is similar to that of fixed bandwidth except the adaptive argument has changed to TRUE.\n\nbw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\n\n\n6.2.2 GWModel Method - Adaptive\n\ngwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\nWe can display it as well\n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-03-11 23:10:21 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2023-03-11 23:10:23 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61."
  },
  {
    "objectID": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#visualising-gwr-output",
    "href": "lessons/Hands-on/Hands-on-Ex8/Hands-on-Ex8.html#visualising-gwr-output",
    "title": "Hands On Exercise 8",
    "section": "7 Visualising GWR Output",
    "text": "7 Visualising GWR Output\nWe will be making use of the Adaptive Ban\n\n7.1 Converting to sf dataframe\n\ncondo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%\n  st_transform(crs=3414)\n\ncondo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)\ncondo_resale.sf.adaptive.svy21  \n\nSimple feature collection with 1436 features and 51 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14940.85 ymin: 24765.67 xmax: 43352.45 ymax: 48382.81\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n    Intercept  AREA_SQM        AGE  PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n1   2050011.7  9561.892  -9514.634 -120681.9      319266.92       -393417.79\n2   1633128.2 16576.853 -58185.479 -149434.2      441102.18        325188.74\n3   3433608.2 13091.861 -26707.386 -259397.8     -120116.82        535855.81\n4    234358.9 20730.601 -93308.988 2426853.7      480825.28        314783.72\n5   2285804.9  6722.836 -17608.018 -316835.5       90764.78       -137384.61\n6  -3568877.4  6039.581 -26535.592  327306.1     -152531.19       -700392.85\n7  -2874842.4 16843.575 -59166.727 -983577.2     -177810.50       -122384.02\n8   2038086.0  6905.135 -17681.897 -285076.6       70259.40        -96012.78\n9   1718478.4  9580.703 -14401.128  105803.4     -657698.02       -123276.00\n10  3457054.0 14072.011 -31579.884 -234895.4       79961.45        548581.04\n   PROX_URA_GROWTH_AREA    PROX_MRT  PROX_PARK PROX_PRIMARY_SCH\n1            -159980.20  -299742.96 -172104.47        242668.03\n2            -142290.39 -2510522.23  523379.72       1106830.66\n3            -253621.21  -936853.28  209099.85        571462.33\n4           -2679297.89 -2039479.50 -759153.26       3127477.21\n5             303714.81   -44567.05  -10284.62         30413.56\n6             -28051.25   733566.47 1511488.92        320878.23\n7            1397676.38 -2745430.34  710114.74       1786570.95\n8             269368.71   -14552.99   73533.34         53359.73\n9            -361974.72  -476785.32 -132067.59        -40128.92\n10           -150024.38 -1503835.53  574155.47        108996.67\n   PROX_SHOPPING_MALL PROX_BUS_STOP  NO_Of_UNITS FAMILY_FRIENDLY  FREEHOLD\n1          300881.390     1210615.4  104.8290640       -9075.370  303955.6\n2          -87693.378     1843587.2 -288.3441183      310074.664  396221.3\n3         -126732.712     1411924.9   -9.5532945        5949.746  168821.7\n4          -29593.342     7225577.5 -161.3551620     1556178.531 1212515.6\n5           -7490.586      677577.0   42.2659674       58986.951  328175.2\n6          258583.881     1086012.6 -214.3671271      201992.641  471873.1\n7         -384251.210     5094060.5   -0.9212521      359659.512  408871.9\n8          -39634.902      735767.1   30.1741069       55602.506  347075.0\n9          276718.757     2815772.4  675.1615559      -30453.297  503872.8\n10        -454726.822     2123557.0  -21.3044311     -100935.586  213324.6\n         y    yhat    residual CV_Score Stud_residual Intercept_SE AREA_SQM_SE\n1  3000000 2886532   113468.16        0    0.38207013     516105.5    823.2860\n2  3880000 3466801   413198.52        0    1.01433140     488083.5    825.2380\n3  3325000 3616527  -291527.20        0   -0.83780678     963711.4    988.2240\n4  4250000 5435482 -1185481.63        0   -2.84614670     444185.5    617.4007\n5  1400000 1388166    11834.26        0    0.03404453    2119620.6   1376.2778\n6  1320000 1516702  -196701.94        0   -0.72065800   28572883.7   2348.0091\n7  3410000 3266881   143118.77        0    0.41291992     679546.6    893.5893\n8  1420000 1431955   -11955.27        0   -0.03033109    2217773.1   1415.2604\n9  2025000 1832799   192200.83        0    0.52018109     814281.8    943.8434\n10 2550000 2223364   326635.53        0    1.10559735    2410252.0   1271.4073\n      AGE_SE PROX_CBD_SE PROX_CHILDCARE_SE PROX_ELDERLYCARE_SE\n1   5889.782    37411.22          319111.1           120633.34\n2   6226.916    23615.06          299705.3            84546.69\n3   6510.236    56103.77          349128.5           129687.07\n4   6010.511   469337.41          304965.2           127150.69\n5   8180.361   410644.47          698720.6           327371.55\n6  14601.909  5272846.47         1141599.8          1653002.19\n7   8970.629   346164.20          530101.1           148598.71\n8   8661.309   438035.69          742532.8           399221.05\n9  11791.208    89148.35          704630.7           329683.30\n10  9941.980   173532.77          500976.2           281876.74\n   PROX_URA_GROWTH_AREA_SE PROX_MRT_SE PROX_PARK_SE PROX_PRIMARY_SCH_SE\n1                 56207.39    185181.3     205499.6            152400.7\n2                 76956.50    281133.9     229358.7            165150.7\n3                 95774.60    275483.7     314124.3            196662.6\n4                470762.12    279877.1     227249.4            240878.9\n5                474339.56    363830.0     364580.9            249087.7\n6               5496627.21    730453.2    1741712.0            683265.5\n7                371692.97    375511.9     297400.9            344602.8\n8                517977.91    423155.4     440984.4            261251.2\n9                153436.22    285325.4     304998.4            278258.5\n10               239182.57    571355.7     599131.8            331284.8\n   PROX_SHOPPING_MALL_SE PROX_BUS_STOP_SE NO_Of_UNITS_SE FAMILY_FRIENDLY_SE\n1               109268.8         600668.6       218.1258           131474.7\n2                98906.8         410222.1       208.9410           114989.1\n3               119913.3         464156.7       210.9828           146607.2\n4               177104.1         562810.8       361.7767           108726.6\n5               301032.9         740922.4       299.5034           160663.7\n6              2931208.6        1418333.3       602.5571           331727.0\n7               249969.5         821236.4       532.1978           129241.2\n8               351634.0         775038.4       338.6777           171895.1\n9               289872.7         850095.5       439.9037           220223.4\n10              265529.7         631399.2       259.0169           189125.5\n   FREEHOLD_SE Intercept_TV AREA_SQM_TV     AGE_TV PROX_CBD_TV\n1     115954.0    3.9720784   11.614302  -1.615447 -3.22582173\n2     130110.0    3.3460017   20.087361  -9.344188 -6.32792021\n3     141031.5    3.5629010   13.247868  -4.102368 -4.62353528\n4     138239.1    0.5276150   33.577223 -15.524302  5.17080808\n5     210641.1    1.0784029    4.884795  -2.152474 -0.77155660\n6     374347.3   -0.1249043    2.572214  -1.817269  0.06207388\n7     182216.9   -4.2305303   18.849348  -6.595605 -2.84136028\n8     216649.4    0.9189786    4.879056  -2.041481 -0.65080678\n9     220473.7    2.1104224   10.150733  -1.221345  1.18682383\n10    206346.2    1.4343123   11.068059  -3.176418 -1.35360852\n   PROX_CHILDCARE_TV PROX_ELDERLYCARE_TV PROX_URA_GROWTH_AREA_TV PROX_MRT_TV\n1         1.00048819          -3.2612693            -2.846248368 -1.61864578\n2         1.47178634           3.8462625            -1.848971738 -8.92998600\n3        -0.34404755           4.1319138            -2.648105057 -3.40075727\n4         1.57665606           2.4756745            -5.691404992 -7.28705261\n5         0.12990138          -0.4196596             0.640289855 -0.12249416\n6        -0.13361179          -0.4237096            -0.005103357  1.00426206\n7        -0.33542751          -0.8235874             3.760298131 -7.31116712\n8         0.09462126          -0.2405003             0.520038994 -0.03439159\n9        -0.93339393          -0.3739225            -2.359121712 -1.67102293\n10        0.15961128           1.9461735            -0.627237944 -2.63204802\n   PROX_PARK_TV PROX_PRIMARY_SCH_TV PROX_SHOPPING_MALL_TV PROX_BUS_STOP_TV\n1   -0.83749312           1.5923022            2.75358842        2.0154464\n2    2.28192684           6.7019454           -0.88662640        4.4941192\n3    0.66565951           2.9058009           -1.05686949        3.0419145\n4   -3.34061770          12.9836105           -0.16709578       12.8383775\n5   -0.02820944           0.1220998           -0.02488294        0.9145046\n6    0.86781794           0.4696245            0.08821750        0.7656963\n7    2.38773567           5.1844351           -1.53719231        6.2029165\n8    0.16674816           0.2042469           -0.11271635        0.9493299\n9   -0.43301073          -0.1442145            0.95462153        3.3123012\n10   0.95831249           0.3290120           -1.71252687        3.3632555\n   NO_Of_UNITS_TV FAMILY_FRIENDLY_TV FREEHOLD_TV  Local_R2\n1     0.480589953        -0.06902748    2.621347 0.8846744\n2    -1.380026395         2.69655779    3.045280 0.8899773\n3    -0.045279967         0.04058290    1.197050 0.8947007\n4    -0.446007570        14.31276425    8.771149 0.9073605\n5     0.141120178         0.36714544    1.557983 0.9510057\n6    -0.355762335         0.60891234    1.260522 0.9247586\n7    -0.001731033         2.78285441    2.243875 0.8310458\n8     0.089093858         0.32346758    1.602012 0.9463936\n9     1.534793921        -0.13828365    2.285410 0.8380365\n10   -0.082251138        -0.53369623    1.033819 0.9080753\n                    geometry\n1  POINT (22085.12 29951.54)\n2   POINT (25656.84 34546.2)\n3   POINT (23963.99 32890.8)\n4  POINT (27044.28 32319.77)\n5  POINT (41042.56 33743.64)\n6   POINT (39717.04 32943.1)\n7   POINT (28419.1 33513.37)\n8  POINT (40763.57 33879.61)\n9  POINT (23595.63 28884.78)\n10 POINT (24586.56 33194.31)\n\ngwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                <dbl> 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ LOG_SELLING_PRICE       <dbl> 14.91412, 15.17135, 15.01698, 15.26243, 14.151…\n$ MLR_RES                 <dbl> -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               <dbl> 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM.1              <dbl> 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE.1                   <dbl> -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD.1              <dbl> -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE.1        <dbl> 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE.1      <dbl> -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA.1  <dbl> -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT.1              <dbl> -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK.1             <dbl> -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH.1      <dbl> 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL.1    <dbl> 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP.1         <dbl> 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS.1           <dbl> 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY.1       <dbl> -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD.1              <dbl> 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    <dbl> 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                <dbl> 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           <dbl> 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            <dbl> 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             <dbl> 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  <dbl> 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             <dbl> 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       <dbl> 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     <dbl> 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE <dbl> 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             <dbl> 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            <dbl> 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     <dbl> 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   <dbl> 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        <dbl> 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          <dbl> 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      <dbl> 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             <dbl> 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            <dbl> 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             <dbl> 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  <dbl> -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             <dbl> -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       <dbl> 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     <dbl> -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV <dbl> -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             <dbl> -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            <dbl> -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     <dbl> 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   <dbl> 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        <dbl> 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          <dbl> 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      <dbl> -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             <dbl> 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                <dbl> 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ coords.x1               <dbl> 22085.12, 25656.84, 23963.99, 27044.28, 41042.…\n$ coords.x2               <dbl> 29951.54, 34546.20, 32890.80, 32319.77, 33743.…\n$ geometry                <POINT [m]> POINT (22085.12 29951.54), POINT (25656.…\n\n\nWe can take a look at the summary as well.\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n7.2 Visualising Local R2\n\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n7.3 Visualising coefficient estimates\n\ntmap_mode(\"view\")\nAREA_SQM_SE <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n7.4 By URA Planning Region\n\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)\n\n\n\ntmap_mode(\"plot\")"
  }
]