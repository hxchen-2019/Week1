---
title: "Hands On Exercise 8"
author: "Hao Xian"
date: "6 March 2023"
date-modified: '`r Sys.Date()`'
format: html
number-sections: true
execute: 
  echo: true
  eval: true
  message: false
  warning: false
  
editor: visual
---

## Loading the packages

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```

## Reading the Data

### Importing the Data

```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

Afterwards, we would need to update the CRS infomration to the correct code

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

Lets check to verify the newly transformed fcode

```{r}
st_crs(mpsz_svy21)
```

Noticed that the EPSG is now 3414, which is what we want./

```{r}
st_bbox(mpsz_svy21) #view extent
```

The step above is not really necessary but it is good to have.

### Importing Aspatial Dataset

We will now be importing the Aspatial Dataset

```{r}
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")
```

With glimpse we can take alook at the data strucure

```{r}
glimpse(condo_resale)
```

Well the data does look a but confusing why not we summarize the data for easier viewing.

```{r}
summary(condo_resale)
```

### Converting Aspatial to SF object

We have done the steps before in the previous Hands On Exercise there is no need to do so again.

```{r}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
```

```{r}
head(condo_resale.sf)
```

## EDA

Now that we have a sf data frame we can visualise our dataset and perfomr some EDA on it.

### EDA using statistical graphics

```{r}
ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Notice how the data is skewed. One method of normalizeing skewed data is to perform log transformation.

```{r}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

Lets take a look at the log data

```{r}
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Notice how the logs is now distributed more nicely

```{r}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggpubr::ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

### Drawing Statistical Point Map

We would want to reveal the geospatial distribution of the prices.

But we would prefer to have an interactive map

```{r}
tmap_mode("view")

tm_shape(mpsz_svy21)+
  tm_polygons() +
  tmap_options(check.and.fix = TRUE) +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

We would need to set it to plot mode again to prevent other maps from being interactive.

```{r}
tmap_mode("plot")
```

## Hedonic Pricing Modeling

We can build it using lm() of R base

### Simple Linear Regression method.

```{r}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
```

Now that we have a linear regression method, we can take a look at the common stats.

```{r}
summary(condo.slr)
```

from the values above we can actually inteprete the formula as

y = -258121.1 + 14719

It is easier to view it in a scatterplot

```{r}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

However, a simple linear regression model is not the best in explanaing the model at all. We would need to make use of Multiple Linear Regression Model.

## Multiple Linear Regression Method

Before we use multiple linear regression method we need to see how correlated the factors are first.

This can be done with the use of a corrplot

```{r}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

Notice how ***Freehold*** is highly correlated to ***LEASE_99YEAR***. In view of this, it is wiser to only include either one of them. For now we will choose ***Freehold*** over the other.

### Building a hedonic pricing model using multiple linear regression method

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)
summary(condo.mlr)
```

The above data is now complete, but it is clear that not all the factors are statistically significant, we would need to filter those variable out.

### olsrr method.

We will first filter those variable out first, before we calibrate with the olsrr method

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)
ols_regress(condo.mlr1)
```

### GTsummary method.

Another method that we can use as well would be making use of the gtsummary package as well.

```{r}
tbl_regression(condo.mlr1, intercept = TRUE)
```

With this packages, the model statistic can be included as well.

```{r}
tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

### Checking for multicolinearity

OLSRR provide quite alot of useful methods to help us build better models.

In this case, we are testing for multicolinearity

```{r}
ols_vif_tol(condo.mlr1)
```

Since the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.

### Testing for Non-Linearity

Another assumption of linear regression is that the dependent variables are linearly related to the independent variables.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

The figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.

### Test for Normality

Another assumption in Linear Regression is that all the variables in normal.

We can test this out with

```{r}
ols_plot_resid_hist(condo.mlr1)
```

The figure reveals that the residual of the multiple linear regression model resembles a normal distribution.

Another way to view it is to see it in a table.

```{r}
ols_test_normality(condo.mlr1)
```

The summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.

### Testing for Spatial Autocorrelation.

As we are testing with geographically reference attributes, we also need to visialised the residual of the hedonic pricing model.

Before we could do that we need to convert it to the spatial point data frame

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)
```

We will join it with the condo_resale.sf object

```{r}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

we will convert *condo_resale.res.sf* from simple feature object into a SpatialPointsDataFrame

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

Now we can take a look at it

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
tmap_mode("plot")
```

There seems to be sign of spatial auto correlation.

#### Moran's I Test

We will need to perform Moran's I test to prove out observationb

```{r}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

We will then convert it into spatial weight

```{r}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

Now we can perform a Moran test

```{r}
lm.morantest(condo.mlr1, nb_lw)
```

This is the conclusion.

The Global Moran\'s I test for residual spatial autocorrelation shows that it\'s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.

Since the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.

## Building Hedonic Pricing Models using GWmodel.

### Building Fixed Bandwidth GWR Model

We would firt need to compute a fixed bandwidth first. This can be done with the bw.gwr package.

#### Calculating Fixed Bandwidth

There are two main methods CV cross-validation approach and AIC corrected (AICc) approach. In this case we are using the CV approach.

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                     FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

In this case the recommended bandwifth is 971.3408 meters. It is in meters because ESGR is measured in meters.

#### GWModel Method - fixed bandwidth

We can now calibrate the model using fixed bandwidth and kernel gaussian. We can choose other kernel.

```{r}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                         FAMILY_FRIENDLY + FREEHOLD, 
                       data=condo_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

We can take alook at the model output

```{r}
gwr.fixed
```

The report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.

### Building Adaptive Bandwidth GWR Model

#### Computing the adaptive Bandwidth

It is similar to that of fixed bandwidth except the `adaptive` argument has changed to **TRUE**.

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + 
                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + 
                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + 
                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

#### GWModel Method - Adaptive

```{r}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
```

We can display it as well

```{r}
gwr.adaptive
```

The report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.

## Visualising GWR Output

We will be making use of the Adaptive Ban

### Converting to sf dataframe

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)

condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
condo_resale.sf.adaptive.svy21  

gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))

glimpse(condo_resale.sf.adaptive)
```

We can take a look at the summary as well.

```{r}
summary(gwr.adaptive$SDF$yhat)
```

### Visualising Local R2

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

tmap_mode("plot")
```

### Visualising coefficient estimates

```{r}
tmap_mode("view")
AREA_SQM_SE <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

AREA_SQM_TV <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=2,
             sync = TRUE)

tmap_mode("plot")
```

### By URA Planning Region

```{r}
tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(condo_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)

tmap_mode("plot")
```
