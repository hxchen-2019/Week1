---
title: "Take Home Exercise 3"
author: "Hao Xian"
date: "16 February 2023"
date-modified: '`r Sys.Date()`'
execute: 
  echo: true
  eval: true
  warning: true
  message: false

editor: visual
---

# The Task At Hand

::: callout-important
This part is taken from Professor Kam Tin Seong [Take Home Exercise 3.](https://is415-ay2022-23t2.netlify.app/th_ex3.html)
:::

Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

## The Task

::: callout-important
This part is taken from Professor Kam Tin Seong [Take Home Exercise 3.](https://is415-ay2022-23t2.netlify.app/th_ex3.html)
:::

In this take-home exercise, you are tasked to predict HDB resale prices at the sub-market level (i.e. HDB 3-room, HDB 4-room and HDB 5-room) for the month of January and February 2023 in Singapore. The predictive models must be built by using by using conventional OLS method and GWR methods. You are also required to compare the performance of the conventional OLS method versus the geographical weighted methods.

# Loading the Packages

**The R packages we'll use for this analysis are:**

-   [**sf**](https://cran.r-project.org/web/packages/sf/index.html): used for importing, managing, and processing geospatial data

-   [**tidyverse**](https://www.tidyverse.org/): a collection of packages for data science tasks

    -   **readr** for importing delimited files (.csv)

    -   **tidyr** for manipulating and tidying data

    -   **dplyr** for wrangling and transforming data

    -   **ggplot2** for visualising data

-   [**tmap**](https://cran.r-project.org/web/packages/tmap/index.html): used for creating thematic maps, such as choropleth and bubble maps

-   [**spdep**](https://cran.r-project.org/web/packages/spdep/index.html): used to create spatial weights matrix objects, global and local spatial autocorrelation statistics and related calculations (e.g. spatially lag attributes)

-   [**onemapsgapi**](https://cran.r-project.org/web/packages/onemapsgapi/index.html): used to query Singapore-specific spatial data, alongside additional functionalities.

-   \[**httr**\]((https://cran.r-project.org/web/packages/httr/)**:** used to make API calls, such as a GET request

-   [**units**](https://cran.r-project.org/web/packages/units/index.html): used to for manipulating numeric vectors that have physical measurement units associated with them

-   [**matrixStats**](https://cran.r-project.org/web/packages/matrixStats/index.html): a set of high-performing functions for operating on rows and columns of matrices

-   [**jsonlite**](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html): a JSON parser that can convert from JSON to the appropraite R data types

-   [**rgdal**](https://cran.r-project.org/web/packages/rgdal/index.html)**:** provides bindings to the 'Geospatial' Data Abstraction Library

**In addition, these R packages are specific to building + visualising hedonic pricing models:**

-   [**olsrr**](https://cran.r-project.org/web/packages/olsrr/index.html): used for building least squares regression models

-   [**coorplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) + [**ggpubr**](https://cran.r-project.org/web/packages/ggpubr/index.html): both are used for multivariate data visualisation & analysis

-   [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/index.html): provides a collection of localised spatial statistical methods, such as summary statistics, principal components analysis, discriminant analysis and various forms of GW regression

-   [**SpatialML**](https://cran.r-project.org/web/packages/SpatialML/index.html)**:** Implements a spatial extension of the random forest algorithm

**Lastly, here are the extra R packages that aren't necessary for the analysis itself, but help us go the extra mile with visualisations and presentation of our analysis:**

-   [**devtools**](https://cran.r-project.org/web/packages/devtools/index.html): used for installing any R packages which is not available in RCRAN. In this context, it'll be used to download the [**xaringanExtra**](https://pkg.garrickadenbuie.com/xaringanExtra/#/) package for [panelsets](https://pkg.garrickadenbuie.com/xaringanExtra/#/panelset)

-   [**kableExtra**](https://haozhu233.github.io/kableExtra/): an extension of kable, used for table customisation

-   [**plotly**](https://plotly.com/r/): used for creating interactive web graphics, and can be used in conjunction with ggplot2 with the `ggplotly()` function

-   [**ggthemes**](https://cran.r-project.org/web/packages/ggthemes/index.html): an extension of ggplot2, with more advanced themes for plotting

```{r}
pacman::p_load(sf, tidyverse, tmap, spdep, httr,
             onemapsgapi, units, matrixStats, readxl, jsonlite,
             olsrr, corrplot, ggpubr, GWmodel, SpatialML, tidymodels, 
             devtools, kableExtra, plotly, ggthemes, onemapsgapi, rgdal)
```

# The Data

## List of Data

| Type                     | Name                                         | Format | Source                                                                                                      |
|--------------------------|----------------------------------------------|--------|-------------------------------------------------------------------------------------------------------------|
| Aspatial                 | Resale Flat Price                            | .csv   | [data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)                                               |
| Geospatial               | Singapore National Boundary                  | .shp   | [data.gov.sg](https://data.gov.sg/dataset/national-map-polygon)                                             |
| Geospatial               | Master Plan 2019 Subzone Boundary (Web)      | .shp   | Prof Kam                                                                                                    |
| Geospatial               | MRT Exit Point                               | .shp   | [LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=train)      |
| Geospatial               | Bus Stop Locations Aug 2023                  | .shp   | [LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=bus%20stop) |
| Geospatial - Extracted   | Childcare Services                           | .shp   | [OneMap API](https://www.onemap.gov.sg/docs/)                                                               |
| Geospatial - Extracted   | Eldercare Services                           | .shp   | [OneMap API](https://www.onemap.gov.sg/docs/)                                                               |
| Geospatial - Extracted   | Hawker Centres                               | .shp   | [OneMap API](https://www.onemap.gov.sg/docs/)                                                               |
| Geospatial - Extracted   | Kindergartens                                | .shp   | [OneMap API](https://www.onemap.gov.sg/docs/)                                                               |
| Geospatial - Extracted   | Parks                                        | .shp   | [OneMap API](https://www.onemap.gov.sg/docs/)                                                               |
| Geospatial - Extracted   | Supermarkets                                 | .kml   | [OneMap API](https://www.onemap.gov.sg/main/v2/essentialamenities)                                          |
| Geospatial - Extracted   | Primary Schools                              | .pdf   | [MOE Website](https://www.moe.gov.sg/about-us/organisation-structure/sd/school-clusters)                    |
| Geospatial - Selfsourced | List of Shopping Mall                        | .html  | [Wikipedia](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore)                              |
| Geospatial - Selfsourced | Pharmacy                                     | .kml   | [OneMap API](https://www.onemap.gov.sg/main/v2/essentialamenities)                                          |
| Geospatial - Selfsourced | Integrated Screening Programme (ISP) Clinics | .kml   | [OneMap API](https://www.onemap.gov.sg/main/v2/essentialamenities)                                          |

# Preparing the Aspatial Data

## Importing the Resale Data

```{r}
resale <- read_csv("data/aspatial/resale-flat-prices.csv")
glimpse(resale)
```

Taking a look at the data we have noticed that the data set contains 11 columns with 148576 observations. They have the following columns: ***months, town, flat_type, block, street_name, storey_range, floor_area_sqm, flat_model, remaining_lease, resale_price.*** We are only interested in the following period from: *1st January 2021 to 31st December 2022* and *January 2023 to February 2022*.

### Filtering the Aspatial Data

We are making use of the filter function from the dplyr package to help us filter out the data. Find out more [here](https://dplyr.tidyverse.org/reference/filter.html). As there is more steps to be done on the aspatial data before it can be used for geographically weighted regression, we will split the data set into the period: 1*st January 2021 to February 2022*, for now.

```{r}
resale_flat_full <-  filter(resale,flat_type == "4 ROOM") %>% 
              filter(month >= "2021-01" & month <= "2023-02")
```

Once we have split it, we can make use of the unique function of Base R to ensure that all the data extracted out are correct. Find out more [here](https://rdrr.io/r/base/unique.html).

```{r}
unique(resale_flat_full$flat_type)
```

```{r}
unique(resale_flat_full$month)
```

From the result above, we can confirm that the data for data set is extracted correctly.

## Transforming the Resale Data

::: callout-important
The following steps are made with reference to: Take Home Exercise 3 done by: NOR AISYAH BINTE AJIT. Check out her work [here](https://aisyahajit2018-is415.netlify.app/posts/2021-11-07-take-home-exercise-3/).
:::

Now that we have correctly filter out the dataset that we wish to use, we are left with another problem. Lets have a look at our data as an example for me to better illustrate the problem.

```{r}
head(resale_flat_full)
```

In our analysis, we are looking at the following key_factors:

-   Area of the unit

-   Floor level

-   Remaining lease

-   Age of the unit

With regards to the key factor. Notice how there are 4 key issues that we need to addressed:

1.  **No geospatial data:** There is no geospatial data for us to plot out the points. This is worrying as the geospatial data is needed for us to perform geographically weighted regression. Fortunately, the data frame provided 2 columns that are critical in retrieving the coordinates of the flat, however it is found in ***2 different columns:*** **block, street_name**. We would need to concatenate them together into to search for their coordinates.

2.  **remaining_lease is recorded as a string:** The remaining leases data is found as a string, when it should be an continuos variable. It is current written as a string currently, which will be treated as a categorical data instead, as such we would need to convert it into the correct format first.

3.  **storey_range is given as a range:** In our dataset, the floor of the exact unique is not given, but rather a range is given this could be a huge potential issue as this would mean thatthe data will treated as a categorical data. We would need to convert it into the correct format first

4.  **No age**: There is no age in our dataset, which would mean that we would need to solve this issue as well.

### Retrieving Postal Codes and Coordinates of the address

As mentioned before, one of the key issues that we would need to perform is to retrieve all the relevant data such as postal code and coordinates of the address that is needed for later analysis.

The steps are as followed:

1.  Combining Block and Street Name to form an address

2.  Filtering out Unique Address

3.  Retrieving coordinates from OneMap.API

4.  Inspecting the Result and Fixing the Issues

#### Step 1: Combining Block and Street Name to form an address

In this step, we will be combining the street_name and block to form an address.

We can make use of the `paste()` function from base R to concatenate the two data together. Find out more [here](https://rdrr.io/r/base/paste.html).

Afterwards, we will placed the data in a new columns called address in the dataframe by using the `mutate()` function from dplyr. Find out more about [here](https://dplyr.tidyverse.org/reference/mutate.html).

```{r}
resale_flat_full <- resale_flat_full %>%
  mutate(resale_flat_full, address = paste(block,street_name))

head(resale_flat_full)
```

#### Step 2: Filtering out Unique Address.

This step is performed in order to minimize the amount of API Call that we need to perform. Furthermore, this also makes it easier for us to see which of the address will result in an error.

We will first get the unique address out first before sorting the data. This can be done with the `sort()` from base R. Find out more [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/sort).

```{r}
add_list <- sort(unique(resale_flat_full$address))
```

#### Step 3: Retrieiving Coordinates from OneMap API

To retrieve the coordinates from OneMap API, it will be easier to create a function to retrieve all the coordinate instead. To do so, lets take a deep dive into the OneMap API. Documentation can be found [here](https://www.onemap.gov.sg/docs/#onemap-rest-apis). In this case, we will be making use of the OneMap API search API to retrieve the necessary coordinates.

According to the documentation, the request requires the following.

-   **searchVal:** Keywords entered by user that is used to filter out the results.

-   **returnGeom {Y/N}:** Checks if user wants to return the geometry.

-   **getAddrDetails {Y/N}**: Checks if user wants to return address details for a point.

-   **pageNum**: Specifies the page to retrieve your search results from. *This is optional (We will not be using this in this case)*

A provided example link would be something like this: *https://developers.onemap.sg/commonapi/search?searchVal=revenue&returnGeom=Y&getAddrDetails=Y&pageNum=1*

This will be the following response ***(Taken from OneMap API)***:

![](images/image-2146457878.png)

We are only interested in the LATITUDE and LONGITUDE data in this case.

Now that we understand the API better, we will now create a function that will help us sort through all the data. The following code chunk below does a number of critical steps:

1.  We will create a data frame called postal_coords that will store all the data frame.

2.  We will make use of the `GET()` function from httr package to make a get request call. Find out more [here](https://httr.r-lib.org/reference/GET.html).

3.  We will create a data frame called new role to store all the coordinates

4.  We also need to check the number of responses returned and append to the main data frame accordingly. This is because there are a few conditions to take note of

    -   The number of results can be greater than one or none at all. (*indicated by found* in the JSON).

    -   The results returned can have no postal code (*which we will not consider as valid)*

    -   We will take the first result with a valid postal code as the correct coordinates.

5.  Lastly, we will append the returned response (**new_row**) with the necessary fields to the main dataframe (**postal_coords**) using `rbind()` function of base R package. Find out more [here](https://rdrr.io/r/base/cbind.html).

All of this can be found in the code chunk below:

```{r}
get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){
    #print(i)

    r <- GET('https://developers.onemap.sg/commonapi/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  return(postal_coords)
}
```

With the function define we can call the function to generate out the data frame of the postal codes.

::: callout-note
Please note that this function does take some time to run.
:::

```{r}
#| eval: false
coords <- get_coords(add_list)
```

#### Step 4: Inspecting the Result and Fixing the Issues

Remember all the issues that we can face when making the api call, we would need to check to make sure that all the data is accounted for.

We can make use of the `is.na()` function to check which street address contains any NA values

```{r}
#| eval: false
coords[(is.na(coords$postal) | is.na(coords$latitude) | is.na(coords$longitude) | coords$postal=="NIL"), ]
```

Based on the data frame above, we have noticed that there seems to be 2 address with no postal code

-   215 CHOA CHU KANG

-   216 CHOA CHU KANG

When searching directly with OneMap API instead, we found that OneMap API classified them as the same building instead with the results being ***"BLK 216 AND 215 CHOA CHU KANG CENTRAL".*** Furthermore a brief check on the website: property indicates the postal code as

-   [680215](https://www.propertyguru.com.sg/singapore-property-listing/hdb/choa-chu-kang/choa-chu-kang-central_104043/215)

-   [680216](https://www.propertyguru.com.sg/singapore-property-listing/hdb/choa-chu-kang/choa-chu-kang-central_104043/216)

We shall proceed with keeping them as the same as there is the latitude and longitude data.

### Transforming Remaining Lease

In this section, we will be transforming the remaining lease into an integer.

First, we will split the remaining lease into years and months columns for calculation. To do so, we will make use of `str_sub()` function from tidyverse. Find out more [here](https://stringr.tidyverse.org/reference/str_sub.html).

we will convert the string into an integer using the `as.integer()` function from base R. Find out more [here](https://rdrr.io/r/base/integer.html).

```{r}
resale_flat_full <- resale_flat_full %>%
  mutate(resale_flat_full, remaining_lease_yr = as.integer(str_sub(remaining_lease, 0, 2))) %>%
  mutate(resale_flat_full, remaining_lease_mth = as.integer(str_sub(remaining_lease, 9, 11)))
```

We will then convert the NA values of the remaining_lease_mth columns into 0. We can make use of the `is.na()` function from base R. Find out more [here](https://rdrr.io/r/base/NA.html).

Followed by changing the remaining_lease_yr into months.

Finally, we can sum the two columns together with `rowSums()` function from base R. Find out more [here](https://rdrr.io/r/base/colSums.html).

```{r}
resale_flat_full$remaining_lease_mth[is.na(resale_flat_full$remaining_lease_mth)] <- 0
resale_flat_full$remaining_lease_yr <- resale_flat_full$remaining_lease_yr * 12
resale_flat_full <- resale_flat_full %>% 
  mutate(resale_flat_full, remaining_lease_mths = rowSums(resale_flat_full[, c("remaining_lease_yr", "remaining_lease_mth")]))
```

### Transforming Storey Range

Categorical variables require special attention in regression analysis because, unlike continuous variables, they *cannot* be entered into the regression equation just as they are. A method we can used to convert categorical variables into continuous variable is called "treatment" coding, or "dummy" coding. This method involve converting the categorical variable into a reference number.

However, we must understand that ***storey range*** is an ordinal data, in other words, each categorical has an order, or in this case, low to high. Using a dummy variable might not make as much sense in this case. Hence, we will assign a higher value to higher floors. The reasoning behind it is that a higher floor offers more privacy, better security and lower noise pollution due to the high height.

We will first create a dataframe to store all the unique storey range and at the same time sort them. After they are sorted we will create a list of encoding based on the unique values then merging them to form a data frame

```{r}
storeys <- sort(unique(resale_flat_full$storey_range))
storey_order <- 1:length(storeys)
storey_range_order <- data.frame(storeys, storey_order)
```

```{r}
head(storey_range_order)
```

From the above results, we can see that:

-   01 TO 03 is assigned the value: 1

-   04 TO 06 is assigned the value: 2

-   07 TO 09 is assigned the value: 3

-   10 TO 12 is assigned the value: 4

-   13 TO 15 is assigned the value: 5

-   16 TO 18 is assigned the value: 6

Hence, the storey range are in the correct order

### Transforming Age

Age is one of the key factors that we are using in our analysis. However, there is no direct reference to age of the HDB. One method we can use is to infer the age of the building based on the remaining lease. It is well known that Singapore HDB are leased to us for 99 years as such we can calculate the age based on the difference between the total lease and remaining lease.

```{r}
resale_flat_full <- resale_flat_full %>% 
  mutate(resale_flat_full, age = 99 * 12 - resale_flat_full$remaining_lease_mths)
```

```{r}
head(resale_flat_full)
```

### Combining all the data and storing as RDS

We will now combine all the relevant fields to a data frame using `left_join()` function of dplyr package. Find out more [here](https://dplyr.tidyverse.org/reference/mutate-joins.html).

```{r}
#| eval: false
resale_flat_full <- left_join(resale_flat_full, storey_range_order, by= c("storey_range" = "storeys"))

rs_coords <- left_join(resale_flat_full, coords, by = c('address' = 'address'))
```

We can then store the data frame into a rds for use later.

```{r}
#| eval: false
rs_coords_rds <- write_rds(rs_coords, "data/aspatial/rds/rs_coords.rds")
```

## Viewing RDS Data

### Reading RDS

Instead of performing all the steps above again, once you save the data frame in a rds format, you can read the rds format again. Notice how the data frame is saved exactly as it is.

```{r}
rs_coords <- read_rds("data/aspatial/rds/rs_coords.rds")
head(rs_coords)
```

### Converting to Sf Object

Since the coordinate columns are Latitude & Longitude which are in decimal degrees, the projected CRS will be WGS84. We will need to assign them the respective EPSG code 4326 first before transforming it to 3414 which is the EPSG code for SVY21.

We will first use the `st_as_sf()` function of sf package to convert the data frame into sf object. Find out more [here](https://www.rdocumentation.org/packages/sf/versions/1.0-9/topics/st_as_sf).

Followed by the `st_transform()` function of sf package to transform the coordinates of the sf object.

```{r}
rs_coords_sf <- st_as_sf(rs_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)

rs_coords_sf
```

### Checking for invalid geometries

The step below is not exactly mandatory, but it is good to check if the sf object is valid or not.

```{r}
length(which(st_is_valid(rs_coords_sf) == FALSE))
```

### Plotting the HDB Resale points

```{r}
tmap_mode("view")
tm_shape(rs_coords_sf)+
  tm_dots(col="blue", size = 0.02)
tmap_mode("plot")
```

# Preparing the Locational Data

In this section, we will be retrieving all the other co location factors from the OneMap API.

::: callout-important
The following steps are made with reference to: Take Home Exercise 3 done by: MEGAN SIM TZE YEN. Check out her work [here](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/#structural-factors).
:::

## Extracting the Data from OneMap API

Before we do a deep dive into examining the data, the first step that we need to do first is to extract out all the relevant data that we need. But first we need to see what OneMap API allows us to extract first.

Before we start off, I will first be introducing to you the onemapsgapi package that we will be using. You can learn more about the package [here](https://rdrr.io/cran/onemapsgapi/). You will need to sign up for an account so that they will generate a unique token for you to use. I have already pre-loaded my token into a variable called ***token***.

### Step 1: Searching for Themes

The first step that we can do is to see what themes are available for us to use, with the code chunk below. In this case, we will be using the function `search_themes()` function to help us view all the available list. Find out more [here](https://rdrr.io/cran/onemapsgapi/man/search_themes.html).

```{r}
#| echo: false
token <- "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOjEwMDA2LCJ1c2VyX2lkIjoxMDAwNiwiZW1haWwiOiJoeGNoZW4wMEBnbWFpbC5jb20iLCJmb3JldmVyIjpmYWxzZSwiaXNzIjoiaHR0cDpcL1wvb20yLmRmZS5vbmVtYXAuc2dcL2FwaVwvdjJcL3VzZXJcL3Nlc3Npb24iLCJpYXQiOjE2Nzk4MjQ4NTQsImV4cCI6MTY4MDI1Njg1NCwibmJmIjoxNjc5ODI0ODU0LCJqdGkiOiI5ZmEyYjA3NTRlZDZiYjI2MDEzZmE2NzI0MGMxNmI2NSJ9.J2MvVpZ3V9MFmmCgvUgfytgBtf356VQA68Uoz9xw_Nw"

```

```{r}
# Please replace token with the values of your own token.
avail_themes <- search_themes(token)
avail_themes
```

As you can see there seems to be a huge amount of themes available for us to use, which in this case I am interested only in a few namely, **Eldercare Services, Childcare Services, Hawker Centres, Parks, Kindergartens.** As you have noticed, there seems to be some themes that have repeated themselves, we would need to do a deep dive to find out more about what each theme contains.

### Step 2: A Closer Look at each theme

One ability of the `search_themes()` function is that you can make a query as well. This way there is no need to see all the other information that is not necessary for our evaluation.

[**Eldercare**]{.underline}

```{r}
avail_themes_elder <- search_themes(token, "elder")
avail_themes_elder
```

There is only one available theme relating to Eldercare, so this theme will be use. The query is `eldercare`

[**Childcare**]{.underline}

```{r}
avail_themes_child <- search_themes(token, "childcare")
avail_themes_child
```

There is only one available theme relating to ChildCare, so this theme will be use. The query is `childcare`

[**Hawker Centres**]{.underline}

```{r}
avail_themes_hawker <- search_themes(token, "hawker")
avail_themes_hawker
```

In this case, there appears to be 3 different themes with the name "Hawker". In this case, we can ignore the healthier_hawker_centres, as it category is in health rather than environment.

```{r}
hawkercentre_new <- get_theme(token, "hawkercentre_new")
hawkercentre <- get_theme(token, "hawkercentre")
head(hawkercentre_new)
head(hawkercentre)
```

If we were to compare to 2 data frame, both data set has the same number of observations, with the only difference being that hawkercentre_new theme has more variables as compared to that of the hawkercentre. However, the difference in variables does seems to add value to the data frame at all and will be drop as those variables are not essential. Using either of the themes should have be fine, but I will be making use of the former for this exercise.

The query picked is `hawkercentre_new`

[**Parks**]{.underline}

```{r}
avail_themes_parks <- search_themes(token, "parks")
avail_themes_parks 
```

Based on the dataframe, above there seems to be a total of 25 different themes that matches the name "parks", however this is due to the fact that it is the result also returns the results from the "National Parks Boards". There are two themes that stands out the most here: "Parks" and "NParks Parks and Nature Reserve". If we were to look into their category we found that the former belongs to recreation, while the latter belongs to environment. Seeing as we're trying to relate the locational factors to the pricing of resale housing units, it makes more sense to go with the former!

The query chosen is `nationalparks`

[**Kindergarten**]{.underline}

```{r}
avail_themes_kindergarten <- search_themes(token, "kinder")
avail_themes_kindergarten
```

There is only one available theme relating to Kindergartens, so this theme will be use. The query is `kindergartens`

[**Clinic**]{.underline}

```{r}
avail_themes_kindergarten <- search_themes(token, "health")
avail_themes_kindergarten
```

There is only one available theme relating to Clinic, so this theme will be use. The query is `moh_isp_clinics`

Another theme of interest could also be Pharmacy, so this theme will be use. The query is `registered_pharmacy`

[**Others**]{.underline}

One thing to note is that we are no longer able to access some information that Megan has access to due to the update of the API such as the name of the primary school as well, which we will address in the other section.

### Step 3: Saving the Data into a data frame

In order to facilitate in the retrieving of themes, I have created the following function to aid me in retrieving the themes and saving them as a data frame.

We will be making use of the `get_theme()` function from the onemapsgapi package to retrieve the relevant and output it in a tibble frame. Read more [here](https://cran.r-project.org/web/packages/onemapsgapi/vignettes/onemapsgapi_vignette.html).

Afterwards, we will convert the tibble to simple features dataframe. All the themes for this project use Lat and Lng as the latitude and longitude respectively, and our project coordinates system should be in the WGS84 system, aka ESPG code 4326.

To ensure that we can access the data again, we will save it into asf into a shapefile, which we can do with `st_write()` function.

```{r}
#| eval: false
save_theme_sf <- function(themename){
  
  themetibble <- get_theme(token, themename) %>%
    select(c("NAME", "Lat", "Lng") )
  themesf <- st_as_sf(themetibble, coords=c("Lng", "Lat"), crs=4326)
  themename_file <- paste("data/extracted/", themename, ".shp", sep="")
  st_write(themesf, themename_file)
} 
```

```{r}
#| eval: false
save_theme_sf("eldercare")
save_theme_sf("childcare")
save_theme_sf("hawkercentre_new")
save_theme_sf("nationalparks")
save_theme_sf("kindergartens")
save_theme_sf("moh_isp_clinics")
save_theme_sf("registered_pharmacy")
```

### Step 4: Visualizing the Points (Optional)

Now that we have loaded all the points we can simply load the other points

```{r}
eldercare <-st_read(dsn="data/extracted", layer = "eldercare") %>% 
  st_transform(crs=3414)
hawkercentre_new <-st_read(dsn="data/extracted", layer = "hawkercentre_new") %>% 
  st_transform(crs=3414)
childcare <-st_read(dsn="data/extracted", layer = "childcare") %>% 
  st_transform(crs=3414)
nationalparks <-st_read(dsn="data/extracted", layer = "nationalparks") %>% 
  st_transform(crs=3414)
kindergartens <-st_read(dsn="data/extracted", layer = "kindergartens") %>% 
  st_transform(crs=3414)
moh_isp_clinics <-st_read(dsn="data/extracted", layer = "moh_isp_clinics") %>% 
  st_transform(crs=3414)
registered_pharmacy <-st_read(dsn="data/extracted", layer = "registered_pharmacy") %>% 
  st_transform(crs=3414)
```

```{r}
tmap_mode("view")
tm_shape(eldercare)+
  tm_dots(col="blue", size = 0.02) +
tm_shape(hawkercentre_new) +
  tm_dots(col="red", size = 0.02) +
tm_shape(childcare)+
  tm_dots(col="yellow", size = 0.02) +
tm_shape(nationalparks)+
  tm_dots(col="green", size = 0.02) +
tm_shape(kindergartens)+
  tm_dots(col="purple", size = 0.02) +
tm_shape(moh_isp_clinics)+
  tm_dots(col="grey", size = 0.02) +
tm_shape(registered_pharmacy)+
  tm_dots(col="darkgreen", size = 0.02)
tmap_mode("plot")
```

The map above looks about right.

## Extracting the Other Data

Now that we have handled those data, one issue that we have face is that there is no data for most the Shopping Mall or Primary School.

[**Shopping Mall Data**]{.underline}

Unlike Megan's Time where both Shopping Mall Data can be found in OneMap API, we need will need to make to do with the list from Wikipedia, which might not necessary be the most accurate, however, it appears to be the best list of data that we have.

You can find the link [here](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore). `https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore`

I have extracted the all the data from the wikipedia and clean it up manually.

```{r}
shopping_mall_list = c("100 AM","313@Somerset","Aperia","Balestier Hill Shopping Centre","Bugis Cube","Bugis Junction","Bugis+","Capitol Piazza","Cathay Cineleisure Orchard","Clarke Quay Central","The Centrepoint","City Square Mall","City Gate Mall","CityLink Mall","Duo","Far East Plaza","Funan","Great World City","HDB Hub","Holland Village Shopping Mall","ION Orchard","Junction 8","Knightsbridge","Liat Towers","Lucky Plaza","Marina Bay Sands","The Shoppes at Marina Bay Sands","Marina Bay Link Mall","Marina Square","Millenia Walk","Mustafa Shopping Centre","Ngee Ann City","Orchard Central","Orchard Gateway","Orchard Plaza","Midpoint Orchard","Palais Renaissance","People's Park Centre","People's Park Complex","Plaza Singapura","PoMo","Raffles City","Scotts Square","Shaw House and Centre","Sim Lim Square","Singapore Shopping Centre","The South Beach","Square 2","Sunshine Plaza","Suntec City","Tanglin Mall","Tanjong Pagar Centre","Tekka Centre","The Adelphi","The Paragon","Tiong Bahru Plaza","The Poiz","Thomson Plaza","United Square","Thomson V","Velocity@Novena Square","Wheelock Place","Wisma Atria","Zhongshan Mall","Bedok Mall","Century Square","Our Tampines Hub","Changi City Point","Downtown East","Djitsun Mall Bedok","Eastpoint Mall","Jewel Changi Airport","KINEX","Katong Shopping Centre","Katong Square","Kallang Wave Mall","Leisure Park Kallang","i12 Katong","Parkway Parade","Paya Lebar Square","Paya Lebar Quarter","Roxy Square","Singpost Centre","Tampines 1","Tampines Mall","White Sands","City Plaza","Elias Mall","Loyang Point","888 Plaza","Admiralty Place","AMK Hub","Canberra Plaza","Causeway Point","Woodlands Civic Centre","Broadway Plaza","Djitsun Mall","Jubilee Square","Junction 8","Junction Nine","Marsiling Mall","Northpoint City","Sembawang Shopping Centre","Sun Plaza","Vista Point","Wisteria Mall","Woodlands Mart","Woodlands North Plaza","Waterway Point","Compass One","Hougang Mall","Heartland Mall","NEX","Buangkok Square","Greenwich V","Hougang 1","Hougang Green Shopping Mall","Hougang Rivercourt","myVillage At Serangoon Garden","Northshore Plaza","Oasis Terraces","Punggol Plaza","Rivervale Mall","Rivervale Plaza","The Seletar Mall","Upper Serangoon Shopping Centre","Beauty World Centre","Beauty World Plaza","Bukit Panjang Plaza","Bukit Timah Plaza","Fajar Shopping Centre","Greenridge Shopping Centre","Hillion Mall","HillV2","Junction 10","Keat Hong Shopping Centre","Limbang Shopping Centre","Lot One","Rail Mall","Sunshine Place","Teck Whye Shopping Centre","West Mall","Yew Tee Point","Yew Tee Square","VivoCity","HarbourFront Centre","Alexandra Retail Centre","321 Clementi","The Clementi Mall","IMM","JCube","Jem","Westgate","Jurong Point","Pioneer Mall","The Star Vista","Alexandra Central","Anchorpoint","OD Mall","Boon Lay Shopping Centre","Grantral Mall","Fairprice Hub","Gek Poh Shopping Centre","Rochester Mall","Taman Jurong Shopping Centre","West Coast Plaza","Queensway Shopping Centre")
```

Remember the function that we have created to retrieve the hdb, we can make use of it here as well to find their coordinates.

::: callout-note
Note that this function does take some time to run.
:::

```{r}
shop_coords <- get_coords(shopping_mall_list)
```

```{r}
shop_coords
```

Now then we can check which Mall does not have the necessary data

```{r}
shop_coords[(is.na(shop_coords$postal) | is.na(shop_coords$latitude) | is.na(shop_coords$longitude) | shop_coords$postal=="NIL"), ]
```

From the results above, we can see that there are 7 addresses that does not contain any information at all:

-   Clarke Quay Central

-   City Gate Mall

-   Holland Village Shopping Mall

-   Mustafa Shopping Centre

-   PoMo

-   Shaw House and Centre

-   OD Mall

I research deeper into the situation and found reverse search the malls to find their Postal Code, afterwards I will search out the name based on the One Map API and found the following:

-   Clarke Quay Central is called `The Central`

-   City Gate Mall is called `City Gate`

-   Holland Village Shopping Mall is called `Holland Road Shopping Centre`

-   Mustafa Shopping Centre is called `Mustafa Centre`

-   PoMO is called `GR.ID`

-   Shaw House and Centre is called `Shaw Centre`

-   OD Mall is called `The GrandStand`

I will append the names in the list below as it is easier:

```{r}
new_shopping_mall_list = c("100 AM","313@Somerset","Aperia","Balestier Hill Shopping Centre","Bugis Cube","Bugis Junction","Bugis+","Capitol Piazza","Cathay Cineleisure Orchard","The Central","The Centrepoint","City Square Mall","City Gate","CityLink Mall","Duo","Far East Plaza","Funan","Great World City","HDB Hub","Holland Road Shopping Centre","ION Orchard","Junction 8","Knightsbridge","Liat Towers","Lucky Plaza","Marina Bay Sands","The Shoppes at Marina Bay Sands","Marina Bay Link Mall","Marina Square","Millenia Walk","Mustafa Centre","Ngee Ann City","Orchard Central","Orchard Gateway","Orchard Plaza","Midpoint Orchard","Palais Renaissance","People's Park Centre","People's Park Complex","Plaza Singapura","GR.ID","Raffles City","Scotts Square","Shaw Centre","Sim Lim Square","Singapore Shopping Centre","The South Beach","Square 2","Sunshine Plaza","Suntec City","Tanglin Mall","Tanjong Pagar Centre","Tekka Centre","The Adelphi","The Paragon","Tiong Bahru Plaza","The Poiz","Thomson Plaza","United Square","Thomson V","Velocity@Novena Square","Wheelock Place","Wisma Atria","Zhongshan Mall","Bedok Mall","Century Square","Our Tampines Hub","Changi City Point","Downtown East","Djitsun Mall Bedok","Eastpoint Mall","Jewel Changi Airport","KINEX","Katong Shopping Centre","Katong Square","Kallang Wave Mall","Leisure Park Kallang","i12 Katong","Parkway Parade","Paya Lebar Square","Paya Lebar Quarter","Roxy Square","Singpost Centre","Tampines 1","Tampines Mall","White Sands","City Plaza","Elias Mall","Loyang Point","888 Plaza","Admiralty Place","AMK Hub","Canberra Plaza","Causeway Point","Woodlands Civic Centre","Broadway Plaza","Djitsun Mall","Jubilee Square","Junction 8","Junction Nine","Marsiling Mall","Northpoint City","Sembawang Shopping Centre","Sun Plaza","Vista Point","Wisteria Mall","Woodlands Mart","Woodlands North Plaza","Waterway Point","Compass One","Hougang Mall","Heartland Mall","NEX","Buangkok Square","Greenwich V","Hougang 1","Hougang Green Shopping Mall","Hougang Rivercourt","myVillage At Serangoon Garden","Northshore Plaza","Oasis Terraces","Punggol Plaza","Rivervale Mall","Rivervale Plaza","The Seletar Mall","Upper Serangoon Shopping Centre","Beauty World Centre","Beauty World Plaza","Bukit Panjang Plaza","Bukit Timah Plaza","Fajar Shopping Centre","Greenridge Shopping Centre","Hillion Mall","HillV2","Junction 10","Keat Hong Shopping Centre","Limbang Shopping Centre","Lot One","Rail Mall","Sunshine Place","Teck Whye Shopping Centre","West Mall","Yew Tee Point","Yew Tee Square","VivoCity","HarbourFront Centre","Alexandra Retail Centre","321 Clementi","The Clementi Mall","IMM","JCube","Jem","Westgate","Jurong Point","Pioneer Mall","The Star Vista","Alexandra Central","Anchorpoint","The GrandStand","Boon Lay Shopping Centre","Grantral Mall","Fairprice Hub","Gek Poh Shopping Centre","Rochester Mall","Taman Jurong Shopping Centre","West Coast Plaza","Queensway Shopping Centre")
```

```{r}
shop_coords_new <- get_coords(new_shopping_mall_list) 
shop_coords_new 
```

We will check again to make sure that the data is correct

```{r}
shop_coords_new[(is.na(shop_coords_new$postal) | is.na(shop_coords_new$latitude) | is.na(shop_coords_new$longitude) | shop_coords_new$postal=="NIL"), ]
```

Since we have confirm that the data is clean, we will just rename the columns to make it easier to understand.

```{r}
rename(shop_coords_new, "name" = "address")
```

We will save it as a shape file so that we can access it much more easily

```{r}
shopping_sf <- st_as_sf(shop_coords_new, coords=c("longitude", "latitude"), crs=4326)
```

```{r}
#| eval: false
st_write(shopping_sf, "data/extracted/shopping_mall.shp")
```

[**Primary School**]{.underline}

Unlike Megan's Time where Primary School Data can be found in OneMap API and is found in Data.gov, we need will need to make to do with the list from MOE, which appears to be the best list of data that we have.

You can find the link [here](https://www.moe.gov.sg/about-us/organisation-structure/sd/school-clusters). `https://www.moe.gov.sg/about-us/organisation-structure/sd/school-clusters`

I have extracted the all the data from the MOE and clean it up manually.

```{r}
primary_sch_list = c("North Vista Primary School","Palm View Primary School","Rivervale Primary School","Seng Kang Primary School","Xinmin Primary School","Ahmad Ibrahim Primary School","Chongfu School","Endeavour Primary School","Jiemin Primary School","Northland Primary School","Northoaks Primary School","Xishan Primary School","Singapore Chinese Girls' School (Primary)","Anchor Green Primary School","Compassvale Primary School","Edgefield Primary School","Fernvale Primary School","Hougang Primary School","Yio Chu Kang Primary School","Catholic High School (Primary)","Anchor Green Primary School","Compassvale Primary School","Edgefield Primary School","Fernvale Primary School","Hougang Primary School","Yio Chu Kang Primary School","Catholic High School (Primary)","Greendale Primary School","Horizon Primary School","Mee Toh School","Montfort Junior School","Nan Chiau Primary School","North Spring Primary School","Maris Stella High School (Primary)","Admiralty Primary School")

primary_sch_list_2=c("Evergreen Primary School","Greenwood Primary School","Marsiling Primary School","Qihua Primary School","Woodgrove Primary School","Woodlands Ring Primary School","Anderson Primary School","Huamin Primary School","Naval Base Primary School","North View Primary School","Peiying Primary School","Sembawang Primary School","Yishun Primary School","CHIJ St Nicholas (Primary)","Ang Mo Kio Primary School","CHIJ Our Lady of Nativity","Holy Innocents' Primary School","Jing Shan Primary School","Mayflower Primary School","Punggol Primary School","Blangah Rise Primary School","Fairfield Methodist School (Primary)","New Town Primary School","Pei Tong Primary School","Queenstown Primary School","Anglo-Chinese Primary School","CHIJ (Toa Payoh) Primary School","First Toa Payoh Primary School","Kheng Cheng School","Marymount Convent School","Pei Chun Public School","Raffles Girls’ Primary School","Alexandra Primary School","Cantonment Primary School","CHIJ (Kellock)","Gan Eng Seng Primary School","Hong Wen School","Radin Mas Primary School","River Valley Primary School","Zhangde Primary School","Anglo-Chinese Junior","Bendemeer Primary School","Farrer Park Primary School","St. Andrew's Junior School","St. Joseph's Institution Junior","St. Margaret's Primary School","Cedar Primary School","CHIJ Our Lady of Good Counsel","St Gabriel’s Primary School","Xinghua Primary School","Yangzheng Primary School","Zhonghua Primary School","Ai Tong School","Kuo Chuan Presbyterian Primary School","Teck Ghee Primary School","Townsville Primary School","Elias Park Primary School","Meridian Primary School","Northshore Primary School","Punggol Cove Primary School","Punggol Green Primary School","Punggol View Primary School","Valour Primary School","Bedok Green Primary School","Junyuan Primary School","Poi Ching School","Red Swastika School","Temasek Primary School","Yu Neng Primary School","Angsana Primary School","Chongzheng Primary School","East Spring Primary School","Fern Green Primary School","Gongshang Primary School","Sengkang Green Primary School","Springdale Primary School","Yumin Primary School","Changkat Primary School","Damai Primary School","Kong Hwa School","St Anthony’s Canossian Primary School","Telok Kurau Primary School","Canossa Catholic Primary School","Fengshan Primary School","Geylang Methodist School (Primary)","Haig Girls’ School","Paya Lebar Methodist Girls’ School (Primary)","Tanjong Katong Primary School","Casuarina Primary School","Oasis Primary School","Park View Primary School","Pasir Ris Primary School","St. Hilda’s Primary School","Tampines North Primary School","Tampines Primary School","Waterway Primary School","White Sands Primary School","CHIJ (Katong) Primary","Maha Bodhi School","Ngee Ann Primary School","Opera Estate Primary School","St Stephen's Primary School","Tao Nan School","Clementi Primary School","Henry Park Primary School","Nan Hua Primary School","Qifa Primary School","Yuhua Primary School","Chua Chu Kang Primary School","Concord Primary School","De La Salle School","Nanyang Primary School","South View Primary School","Unity Primary School","Corporation Primary School","Frontier Primary School","Jurong West Primary School","Pioneer Primary School","West Grove Primary School","Xingnan Primary School","Methodist Girls' School (Primary)","Bukit View Primary School","Dazhong Primary School","Jurong Primary School","Keming Primary School","Lianhua Primary School","St. Anthony's Primary School","Beacon Primary School","Greenridge Primary School","Pei Hwa Presbyterian Primary School","Teck Whye Primary School","West View Primary School","Zhenghua Primary School","Boon Lay Garden Primary School","Bukit Panjang Primary School","CHIJ Our Lady Queen of Peace","Kranji Primary School","West Spring Primary School","Westwood Primary School","Yew Tee Primary School","Bukit Timah Primary School","Fuhua Primary School","Lakeside Primary School","Princess Elizabeth Primary School","Rulang Primary School","Shuqun Primary School")
```

We will make use of the same function to get the list of Primary School Coordinates of Primary School

```{r}
primary_sch_list_coor <- get_coords(primary_sch_list) 
primary_sch_list_coor2 <- get_coords(primary_sch_list_2) 
```

Once we Again we will check if both have any error

```{r}
primary_sch_list_coor[(is.na(primary_sch_list_coor$postal) | is.na(primary_sch_list_coor$latitude) | is.na(primary_sch_list_coor$longitude) | primary_sch_list_coor$postal=="NIL"), ]
```

```{r}
primary_sch_list_coor2[(is.na(primary_sch_list_coor2$postal) | is.na(primary_sch_list_coor2$latitude) | is.na(primary_sch_list_coor2$longitude) | primary_sch_list_coor2$postal=="NIL"), ]
```

From the results above, we can see that there are 7 addresses that does not contain any information at all:

-   Maris Stella High School (Primary)

-   CHIJ St Nicholas (Primary)

-   CHIJ (Toa Payoh) Primary School

-   St Gabriel's Primary School

-   St Anthony's Canossian Primary School

-   St. Hilda's Primary School

-   St Stephen's Primary School

I research deeper into the situation and found reverse search the malls to find their Postal Code, afterwards I will search out the name based on the One Map API and found the following:

-   Maris Stella High School (Primary) is called `Maris Stella High School`

-   CHIJ St Nicholas (Primary) is called `CHIJ St Nicholas`

-   CHIJ (Toa Payoh) Primary School is called `CHIJ Primary (Toa Payoh)`

-   St Gabriel's Primary School is called `Saint Gabriel Primary School`

-   St Anthony's Canossian Primary School is called `Saint Anthony Canossian Primary School`

-   St. Hilda's Primary School is called `Saint Hilda Primary School`

-   St Stephen's Primary School is called `Saint Stephen School`

We will just replace it in the names in the list again.

```{r}
primary_sch_list = c("North Vista Primary School","Palm View Primary School","Rivervale Primary School","Seng Kang Primary School","Xinmin Primary School","Ahmad Ibrahim Primary School","Chongfu School","Endeavour Primary School","Jiemin Primary School","Northland Primary School","Northoaks Primary School","Xishan Primary School","Singapore Chinese Girls' School (Primary)","Anchor Green Primary School","Compassvale Primary School","Edgefield Primary School","Fernvale Primary School","Hougang Primary School","Yio Chu Kang Primary School","Catholic High School (Primary)","Anchor Green Primary School","Compassvale Primary School","Edgefield Primary School","Fernvale Primary School","Hougang Primary School","Yio Chu Kang Primary School","Catholic High School (Primary)","Greendale Primary School","Horizon Primary School","Mee Toh School","Montfort Junior School","Nan Chiau Primary School","North Spring Primary School","Maris Stella High School","Admiralty Primary School")

primary_sch_list_2=c("Evergreen Primary School","Greenwood Primary School","Marsiling Primary School","Qihua Primary School","Woodgrove Primary School","Woodlands Ring Primary School","Anderson Primary School","Huamin Primary School","Naval Base Primary School","North View Primary School","Peiying Primary School","Sembawang Primary School","Yishun Primary School","CHIJ St Nicholas","Ang Mo Kio Primary School","CHIJ Our Lady of Nativity","Holy Innocents' Primary School","Jing Shan Primary School","Mayflower Primary School","Punggol Primary School","Blangah Rise Primary School","Fairfield Methodist School (Primary)","New Town Primary School","Pei Tong Primary School","Queenstown Primary School","Anglo-Chinese Primary School","CHIJ Primary (Toa Payoh)","First Toa Payoh Primary School","Kheng Cheng School","Marymount Convent School","Pei Chun Public School","Raffles Girls’ Primary School","Alexandra Primary School","Cantonment Primary School","CHIJ (Kellock)","Gan Eng Seng Primary School","Hong Wen School","Radin Mas Primary School","River Valley Primary School","Zhangde Primary School","Anglo-Chinese Junior","Bendemeer Primary School","Farrer Park Primary School","St. Andrew's Junior School","St. Joseph's Institution Junior","St. Margaret's Primary School","Cedar Primary School","CHIJ Our Lady of Good Counsel","Saint Gabriel Primary School","Xinghua Primary School","Yangzheng Primary School","Zhonghua Primary School","Ai Tong School","Kuo Chuan Presbyterian Primary School","Teck Ghee Primary School","Townsville Primary School","Elias Park Primary School","Meridian Primary School","Northshore Primary School","Punggol Cove Primary School","Punggol Green Primary School","Punggol View Primary School","Valour Primary School","Bedok Green Primary School","Junyuan Primary School","Poi Ching School","Red Swastika School","Temasek Primary School","Yu Neng Primary School","Angsana Primary School","Chongzheng Primary School","East Spring Primary School","Fern Green Primary School","Gongshang Primary School","Sengkang Green Primary School","Springdale Primary School","Yumin Primary School","Changkat Primary School","Damai Primary School","Kong Hwa School","Saint Anthony Canossian Primary School","Telok Kurau Primary School","Canossa Catholic Primary School","Fengshan Primary School","Geylang Methodist School (Primary)","Haig Girls’ School","Paya Lebar Methodist Girls’ School (Primary)","Tanjong Katong Primary School","Casuarina Primary School","Oasis Primary School","Park View Primary School","Pasir Ris Primary School","Saint Hilda Primary School","Tampines North Primary School","Tampines Primary School","Waterway Primary School","White Sands Primary School","CHIJ (Katong) Primary","Maha Bodhi School","Ngee Ann Primary School","Opera Estate Primary School","Saint Stephen School","Tao Nan School","Clementi Primary School","Henry Park Primary School","Nan Hua Primary School","Qifa Primary School","Yuhua Primary School","Chua Chu Kang Primary School","Concord Primary School","De La Salle School","Nanyang Primary School","South View Primary School","Unity Primary School","Corporation Primary School","Frontier Primary School","Jurong West Primary School","Pioneer Primary School","West Grove Primary School","Xingnan Primary School","Methodist Girls' School (Primary)","Bukit View Primary School","Dazhong Primary School","Jurong Primary School","Keming Primary School","Lianhua Primary School","St Anthony's Primary School","Beacon Primary School","Greenridge Primary School","Pei Hwa Presbyterian Primary School","Teck Whye Primary School","West View Primary School","Zhenghua Primary School","Boon Lay Garden Primary School","Bukit Panjang Primary School","CHIJ Our Lady Queen of Peace","Kranji Primary School","West Spring Primary School","Westwood Primary School","Yew Tee Primary School","Bukit Timah Primary School","Fuhua Primary School","Lakeside Primary School","Princess Elizabeth Primary School","Rulang Primary School","Shuqun Primary School")
```

```{r}
primary_sch_list_coor <- get_coords(primary_sch_list) 
primary_sch_list_coor2 <- get_coords(primary_sch_list_2) 
```

We will need to check again to make sure that there is no missing data

```{r}
primary_sch_list_coor[(is.na(primary_sch_list_coor$postal) | is.na(primary_sch_list_coor$latitude) | is.na(primary_sch_list_coor$longitude) | primary_sch_list_coor$postal=="NIL"), ]
```

```{r}
primary_sch_list_coor2[(is.na(primary_sch_list_coor2$postal) | is.na(primary_sch_list_coor2$latitude) | is.na(primary_sch_list_coor2$longitude) | primary_sch_list_coor2$postal=="NIL"), ]
```

Since we have confirm that the data is clean, we will just rename the columns to make it easier to understand.

```{r}
primary_sch_list_coor_full <- rbind(primary_sch_list_coor, primary_sch_list_coor2)
```

```{r}
rename(primary_sch_list_coor_full, "name" = "address")
```

We can write it into a shape file for us to access everything again.

```{r}
primary_sch_sf <- st_as_sf(primary_sch_list_coor_full, coords=c("longitude", "latitude"), crs=4326)
```

```{r}
#| eval: false
st_write(primary_sch_sf, "data/extracted/primary_school.shp")
```

[**Good Primary School**]{.underline}

Education and academic institutions are an especially important locational factors for families with children, or expect to have children. Since it has already been proven that [distance affects priority admission](https://www.moe.gov.sg/primary/p1-registration/distance), the number of good Primary School around the area is important as well. For this analysis, our focus will be on the primary-school level of education, we would need to understand what is defined as "Good Primary School". MOE does not released a list of Primary Schools ranked based on their result as such, there is now way to determine how the Primary School are actually ranked.

One possible example of the ranking would be the [schlah's Primary School Rankings](https://schlah.com/primary-schools) from the 2020. Although the list is a bit dated, It does offer a transparent method on how they determined what is considered a "Good Primary School" primarily through:

-   Popularity in Primary 1 (P1) Registration: 20%

-   Gifted Education Programme (GEP): 20%

-   Special Assistance Plan (SAP): 15%

-   Achievements in the Singapore Youth Festival Arts Presentation: 15%

-   Representation in the Singapore National School Games: 15%

-   Singapore Uniformed Groups Unit Recognition: 15%

For my analysis, I will consider the Top 10 Primary School as Good Primary School:

![](images/image-476731336.png)

```{r}
good_primary_school <- c("Nanyang Primary School","Tao Nan School","Catholic High School (Primary)","Nan Hua Primary School","Saint Hilda Primary School","Henry Park Primary School","Anglo-Chinese Primary School","Raffles Girls’ Primary School","Pei Hwa Presbyterian Primary School","CHIJ St Nicholas")
```

We will once again make use of the OneMap API to get the theme

```{r}
good_primary_school_coor <- get_coords(good_primary_school) 
```

We will need to check again to make sure that there is no missing data.

```{r}
good_primary_school_coor[(is.na(good_primary_school_coor$postal) | is.na(good_primary_school_coor$latitude) | is.na(good_primary_school_coor$longitude) | good_primary_school_coor$postal=="NIL"), ]
```

Since we have confirm that the data is cleaned we can save it as a shape file to reference again.

```{r}
rename(good_primary_school_coor, "name" = "address")
```

```{r}
good_primary_sch_sf <- st_as_sf(good_primary_school_coor, coords=c("longitude", "latitude"), crs=4326)
```

```{r}
#| eval: false
st_write(good_primary_sch_sf, "data/extracted/good_primary_school.shp")
```

[**CBD**]{.underline}

Lastly, we need to factor in the proximity to the Central Business District - in the Downtown Core. It's located in the southwest of Singapore. As such, let's take the coordinates of Downtown Core to be the coordinates of the CBD, we will store it as a CBD as well.

```{r}
#| eval: false
lat <- 1.287953
lng <- 103.851784

cbd_sf <- data.frame(lat, lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs=4326)

st_write(cbd_sf, "data/extracted/cbd.shp")
```

# Data Wrangling (GeoSpatial)

## Importing GeoSpatial Data

[**Reading SG Boundaries**]{.underline}

```{r}
sg_sf <- st_read(dsn = "data/geospatial/boundaries", layer="CostalOutline")
```

```{r}
mpsz_sf <- st_read(dsn = "data/geospatial/boundaries", layer = "MPSZ-2019")
```

[**Reading Train Station and Bus Stop**]{.underline}

```{r}
rail_network_sf <- st_read(dsn="data/geospatial/TrainStationExit", layer="Train_Station_Exit_Layer")
```

```{r}
bus_sf <- st_read(dsn="data/geospatial/BusStop_Feb2023", layer="BusStop")
```

[**Reading SuperMarkets**]{.underline}

```{r}
supermarket_sf <- st_read("data/geospatial/supermarkets/supermarkets.kml") 
```

Notice how the supermarket has a Z coordinates. The Z coordinates only contains 0 values and hence we would need to drop it.

## Importing Extracted Data

[**Reading Childcare**]{.underline}

```{r}
childcare_sf <- st_read(dsn="data/extracted", layer="childcare")
```

[**Reading Elder Care**]{.underline}

```{r}
eldercare_sf <- st_read(dsn="data/extracted", layer="eldercare")
```

[**Reading Hawker Center**]{.underline}

```{r}
hawkercentre_sf <- st_read(dsn="data/extracted", layer="hawkercentre_new")
```

[**Reading Kindergartens**]{.underline}

```{r}
kindergarten_sf <- st_read(dsn="data/extracted", layer="kindergartens") 
```

[**Reading Schools**]{.underline}

```{r}
primary_sf <- st_read(dsn="data/extracted", layer="primary_school") 
```

```{r}
good_primary_sf <- st_read(dsn="data/extracted", layer="good_primary_school") 
```

[**Reading Shopping Mall**]{.underline}

```{r}
shopping_sf <- st_read(dsn="data/extracted", layer="shopping_mall") 
```

[**Reading Parks**]{.underline}

```{r}
parks_sf <- st_read(dsn="data/extracted", layer="nationalparks") 
```

[**Reading CBD**]{.underline}

```{r}
cbd_sf <- st_read(dsn="data/extracted", layer="cbd")
```

## Importing Self-Source Data

[**Clinics**]{.underline}

```{r}
pharm_sf <- st_read(dsn="data/extracted", layer="registered_pharmacy") 
```

[**ISP Clinics**]{.underline}

```{r}
icp_sf <- st_read(dsn="data/extracted", layer="moh_isp_clinics") 
```

## Data Pre-processing

::: callout-important
The following steps are made with reference to: Take Home Exercise 3 done by: MEGAN SIM TZE YEN. Check out her work [here](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/#structural-factors).
:::

Once we have loaded all our Geo Spatial Data we would need to make some changes

1.  Remove Z-Dimension (for supermarket_sf)

2.  Removing unnecessary columns

3.  Check for invalid geometries

4.  Check for missing values

### Removing Z-Dimension

In order to drop the Z Dimension, we can make use of the `st_zm()` from the sf package to remove it. Find out more [here](https://r-spatial.github.io/sf/reference/st_zm.html).

```{r}
supermarket_sf <- st_zm(supermarket_sf)
```

### Removing Unnecessary columns

In my case, since I have ensured that most of my extracted data only has the most important features, there is no need for me to clear

### Invalid Geometry

I have check through most of the other Spatial Dataframe and found only those below to have invalid geometry

```{r}
length(which(st_is_valid(sg_sf) == FALSE))
```

```{r}
length(which(st_is_valid(mpsz_sf) == FALSE))
```

According to Megan, the `st_make_valid()` method from sf package can be used to make the geometry valid. Find out more [here](https://r-spatial.github.io/sf/reference/valid.html).

```{r}
sg_sf <- st_make_valid(sg_sf)
length(which(st_is_valid(sg_sf) == FALSE))
```

```{r}
mpsz_sf <- st_make_valid(mpsz_sf)
length(which(st_is_valid(mpsz_sf) == FALSE))
```

### Missing Value

Once again, I have check through all the spatial dataframe and found only bus having NA Values

```{r}
bus_sf[rowSums(is.na(bus_sf))!=0,]
```

Based on the information above, it would seem that BUS_ROOF_N and LOC_DESC columns has NA values. This is not really too important as the key identifier is still present. It is optional if you want to fix it or not but I will just leave it it.

```{r}

# bus_sf <- na.omit(bus_sf,c("BYS_ROOF_N","LOC_DESC"))
# bus_sf[rowSums(is.na(bus_sf))!=0,]
```

### Verifying Coordinates System and Fixing it

We can make use of st_crs() function to check our coordinate system

```{r}
st_crs(sg_sf)
```

Our projected CRS should be SVY21 (ESPG Code 3414), but for our given data, the current ESPG Codes are 9001. Well that means that we need to assign the appropriate EPSG Code. In addition, all of our self-sourced/extracted datasets are in WG84 (ESPG Code 4326) as well.

We can check each one by one, or we can just assign all of them at one go with the code chunk below. One thing to note is the `st_set_crs()` can be used for CRS with SVY21, however in this case I am using `st_transform()` as it works the same as well, while for those in WG84, we would need to use `st_transform().`

```{r}
# with st_set_crs(), we can assign the appropriate ESPG Code
sg_sf <- st_transform(sg_sf, 3414)
rail_network_sf <- st_transform(rail_network_sf, 3414)
bus_sf <- st_transform(bus_sf, 3414)

# with st_transform(), we can change from one CRS to another
mpsz_sf <- st_transform(mpsz_sf, 3414)
childcare_sf <- st_transform(childcare_sf, crs=3414)
eldercare_sf <- st_transform(eldercare_sf, crs=3414)
hawkercentre_sf <- st_transform(hawkercentre_sf, crs=3414)
kindergarten_sf <- st_transform(kindergarten_sf, crs=3414)
parks_sf <- st_transform(parks_sf, crs=3414)
supermarket_sf <- st_transform(supermarket_sf, crs=3414)
cbd_sf <- st_transform(cbd_sf, crs=3414)

pharm_sf <- st_transform(pharm_sf, crs=3414)
icp_sf <- st_transform(icp_sf, crs=3414)
primary_sf <- st_transform(primary_sf, crs=3414)
good_primary_sf <- st_transform(good_primary_sf, crs=3414)
shopping_sf <- st_transform(shopping_sf, crs=3414)
```

We can check one again to make sure

```{r}
st_crs(sg_sf)
```

## Visualization

We can try to visualize them to make it easier for us to see.

```{r}
plot(st_geometry(mpsz_sf))
```

We can also visualize all the points to have a better understanding of the situation

```{r}
tmap_mode("view")
tm_shape(bus_sf)+
  tm_dots(col="blue", size = 0.02) +
tm_shape(childcare_sf) +
  tm_dots(col="red", size = 0.02) +
tm_shape(eldercare_sf)+
  tm_dots(col="yellow", size = 0.02) +
tm_shape(good_primary_sf)+
  tm_dots(col="green", size = 0.02) +
tm_shape(hawkercentre_sf)+
  tm_dots(col="purple", size = 0.02) +
tm_shape(icp_sf)+
  tm_dots(col="grey", size = 0.02) +
tm_shape(kindergarten_sf) +
  tm_dots(col="darkgreen", size = 0.02) +
tm_shape(parks_sf)+
  tm_dots(col="lightblue", size = 0.02) +
tm_shape(pharm_sf)+
  tm_dots(col="lightgreen", size = 0.02) +
tm_shape(primary_sf)+
  tm_dots(col="violet", size = 0.02) +
tm_shape(rail_network_sf)+
  tm_dots(col="orange", size = 0.02) +
tm_shape(shopping_sf)+
  tm_dots(col="pink", size = 0.02) +
tm_shape(supermarket_sf)+
  tm_dots(col="white", size = 0.02) +
tm_shape(cbd_sf)+
  tm_dots(col="violet", size = 0.02) 
tmap_mode("plot")
```

# Data Wrangling (Aspatial Data)

Remember the RDS Data that we have prepared earlier, we will put it out again for us to use again. Once again we need to convert it to a sf data frame for us to use as well.

```{r}
rs_coords <- read_rds("data/aspatial/rds/rs_coords.rds")

rs_coords_sf <- st_as_sf(rs_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)

rs_coords_sf
```

### Calculating Proximity

One of the things we need to find is the proximity to particular facilities as one of factors.

Megan has kindly provided the function for us to use above. First, she use compute with `st_distance(),` to compute the proximity to the facility and find the closest facility (shortest distance) with the `rowMins()` function of our matrixStats package. The values will be appended to the data frame as a new column.

```{r}
proximity <- function(df1, df2, varname) {
  dist_matrix <- st_distance(df1, df2) %>%
    drop_units()
  df1[,varname] <- rowMins(dist_matrix)
  return(df1)
}
```

```{r}
rs_coords_sf <- 
  # the columns will be truncated later on when viewing 
  # so we're limiting ourselves to two-character columns for ease of viewing between
  proximity(rs_coords_sf, cbd_sf, "PROX_CBD") %>%
  proximity(., childcare_sf, "PROX_CHILDCARE") %>%
  proximity(., eldercare_sf, "PROX_ELDERCARE") %>%
  proximity(., hawkercentre_sf, "PROX_HAWKER") %>%
  proximity(., rail_network_sf, "PROX_MRT") %>%
  proximity(., parks_sf, "PROX_PARK") %>%
  proximity(., good_primary_sf, "PROX_TOPPRISCH") %>%
  proximity(., shopping_sf, "PROX_MALL") %>%
  proximity(., supermarket_sf, "PROX_SPRMKT") %>%
  proximity(., icp_sf, "PROX_CLINIC") %>%
  proximity(., pharm_sf, "PROX_PHARMACY")
```

```{r}
rs_coords_sf
```

### Facility Count withing Radius Calculation

Other than proximity, which calculates the shortest distance, we also want to find the *number* of facilities within a particular radius as another factors.

Megan has kindly provided the function for us to use above. First, she use `st_distance()` to compute the distance between the flats and the desired facilities, and then sum up the observations with `rowSums()`. The values will be appended to the data frame as a new column.

```{r}
num_radius <- function(df1, df2, varname, radius) {
  dist_matrix <- st_distance(df1, df2) %>%
    drop_units() %>%
    as.data.frame()
  df1[,varname] <- rowSums(dist_matrix <= radius)
  return(df1)
}
```

```{r}
rs_coords_sf <-
  num_radius(rs_coords_sf, kindergarten_sf, "NUM_KNDRGTN", 350) %>%
  num_radius(., childcare_sf, "NUM_CHILDCARE", 350) %>%
  num_radius(., bus_sf, "NUM_BUS_STOP", 350) %>%
  num_radius(., icp_sf, "NUM_ISP_CLIN", 350) %>%
  num_radius(., pharm_sf, "NUM_CHAS_CLIN", 350) %>%
  num_radius(., primary_sf, "NUM_PRI_SCH", 1000)
```

```{r}
rs_coords_sf
```

## Saving the Data into RDS

Well we do not really need all the columns, and we also do not want to read all the files again, we can save it as a rds format so that we can access it again.

Before we save it into RDS, we can clean up columns that we do not really need.

```{r}
rs_coords_final <- rs_coords_sf %>%
  select(1, 7, 11, 15:17, 19:36) %>%
  rename("AREA_SQM" = "floor_area_sqm", 
         "LEASE_YRS" = "remaining_lease_mths", 
         "PRICE"= "resale_price",
         "AGE"= "age",
         "STOREY_ORDER" = "storey_order") %>%
  relocate("PRICE") %>%
  relocate(geometry, .after = last_col())
```

```{r}
rs_coords_final
```

```{r}
#| eval: false
write_rds(rs_coords_final, "data/rds/rs_coords_final.rds")
```

# Exploratory Data Analysis

After the lengthy preparation fo the data above, we can now perform our EDA. This is not really the highlight of this exercise as such we will not go too in dept into it.

We will first load the data we have prepared in first.

```{r}
resale_sf <- read_rds("data/rds/rs_coords_final.rds")
```

## Statistical Graphics

One way to look at how skewed the data is to plot out the graph to see how skewed the data.

In this case we are making use of `ggplot` to help us plot out the. Find out more [here](https://ggplot2.tidyverse.org/reference/ggplot.html).

```{r}
ggplot(data=resale_sf, aes(x=`PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
    labs(title = "Distribution of Resale Prices",
         x = "Resale Prices",
         y = 'Frequency')

```

Notice how the data is skewed. One common method we can used to balance the data would be the log the data.

```{r}
resale_sf <- resale_sf %>%
  mutate(`LOG_PRICE` = log(PRICE))

ggplot(data = resale_sf, aes(x=`LOG_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue") +
  labs(title = "Distribution of Resale Prices (Log)",
       x = "Resale Prices",
       y = 'Frequency')
```

Notice how when we logged the value the distribution is relatively less skewed. Do note that we will still be using `RESALE_PRICE` in the later parts of our analysis.

That is just for resale prices, but what about the rest of the other factors. We can make use of `ggarrange()` to arrange the plot properly. Find out more [here](https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html).

```{r}
AREA_SQM <- ggplot(data = resale_sf, aes(x = `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

LEASE_YRS <- ggplot(data = resale_sf, aes(x = `LEASE_YRS`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_CBD <- ggplot(data = resale_sf, aes(x = `PROX_CBD`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_CHILDCARE <- ggplot(data = resale_sf, aes(x = `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_ELDERCARE <- ggplot(data = resale_sf, aes(x = `PROX_ELDERCARE`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_HAWKER <- ggplot(data = resale_sf, aes(x = `PROX_HAWKER`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_MRT <- ggplot(data = resale_sf, aes(x = `PROX_MRT`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_PARK <- ggplot(data = resale_sf, aes(x = `PROX_PARK`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_TOPPRISCH <- ggplot(data = resale_sf, aes(x = `PROX_TOPPRISCH`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_MALL <- ggplot(data = resale_sf, aes(x = `PROX_MALL`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_SPRMKT <- ggplot(data = resale_sf, aes(x = `PROX_SPRMKT`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

PROX_CLINIC <- ggplot(data = resale_sf, aes(x = `PROX_CLINIC`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')
  
PROX_PHARMACY <- ggplot(data = resale_sf, aes(x = `PROX_PHARMACY`)) + 
  geom_histogram(bins=20, color="black", fill = 'lightblue')

ggarrange(AREA_SQM, LEASE_YRS, PROX_CBD, PROX_CHILDCARE, PROX_ELDERCARE, PROX_HAWKER, PROX_MRT, PROX_PARK, PROX_TOPPRISCH, PROX_MALL, PROX_SPRMKT, PROX_CLINIC, PROX_PHARMACY, ncol = 3, nrow = 4)
```

Despite the difference in skewness of each factors, we will not be making any changes to the data set as we are building a predictive model instead.

```{r}
tmap_mode("view")
tmap_options(check.and.fix = TRUE)
tm_shape(resale_sf) +  
  tm_dots(col = "PRICE",
          alpha = 0.6,
          style="quantile") +
  # sets minimum zoom level to 11, sets maximum zoom level to 14
  tm_view(set.zoom.limits = c(11,14))

tmap_mode("plot")
```

From the map, we can observe that the density of flats with higher resale prices tend to be more concentrated in the central area of Singapore, which is really closer to our expectation.

# Building a Predictive Model

Now that we have completed our EDA, we can focus on the main aspect of this Take Home Exercise. It is stated that our training data is from 2021 to 2022, and our test data is from 2022. We will first need to split the data correctly

## Preparing the Data

Once again we can load the data frame from our rds file again before we perform the split.

```{r}
resale_sf <- read_rds("data/rds/rs_coords_final.rds")

train_data_full <-  filter(resale_sf, month >= "2021-01" & month <= "2022-12")
test_data_full <-  filter(resale_sf, month >= "2023-01" & month <= "2023-02")
```

We will save it as rds so that we can access it again without having to split it again.

```{r}
write_rds(train_data_full, "data/rds/train_data_full.rds")
write_rds(test_data_full, "data/rds/test_data_full.rds")
```

## Checking For Multiple Colinearly

First we will need to drop the geometry data first and also the month data as it is not really necessary, in fact it will actually hinder with the preparation in some models.

This can be done with a number of ways. Find out more [here](https://www.listendata.com/2015/06/r-keep-drop-columns-from-data-frame.html).

In this case we are making use of the `st_drop_geometry()` from sf package to help us. Read more [here](https://r-spatial.github.io/sf/reference/st_geometry.html)

```{r}
resale_nogeo <- resale_sf %>%
  st_drop_geometry() 

resale_nogeo <- subset(resale_nogeo, select = -c(month))
```

We can then plot out the correlation matrix with the corrplot. Find out more [here](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html).

```{r}
corrplot::corrplot(cor(resale_nogeo[, 2:22]), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.5, 
                   method = "number", 
                   type = "upper")
```

Matrix reorder is very important for mining the hidden structure and pattern in the matrix. Based on the documentationm, There are four methods in corrplot (indicated by the. parameter order), named "AOE", "FPC", "hclust", "alphabet". In the code chunk above, AOE order is used. According to Prof, It orders the variables by using the *angular order of the eigenvectors* method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

![From: https://www.researchgate.net/figure/The-scale-of-Pearsons-Correlation-Coefficient_tbl1_340940666](images/The-scale-of-Pearsons-Correlation-Coefficient.png)

In this case we are using a benchmark of +-0.8 as a benchmark for collinearly. As you can see age is highly correlated with LEASE_YRS as the age is derived from the LEASE_YRS as well such we can just drop AGE.

```{r}
resale_nogeo <- subset(resale_nogeo, select = -c(AGE))
```

## Building a Simple Multiple Linear Regression Model

### Preparing the Data

We can once again retrieve all our data to prepare for the fitting of the data. We will need to remove the geometry as well with `st_drop_geometry().`We will also need to remove age and month as they are not necessary.

```{r}
train_data_full <- read_rds("data/rds/train_data_full.rds")
test_data_full <- read_rds("data/rds/test_data_full.rds")

train_data_full <- subset(train_data_full, select = -c(month, AGE))
test_data_full <- subset(test_data_full, select = -c(month, AGE))

train_data_full <- train_data_full %>%
  st_drop_geometry() 

test_data_full <- test_data_full %>%
  st_drop_geometry() 
```

### Fitting the Model

```{r}
#| eval: false
resale.mlr <- lm(formula = PRICE ~ AREA_SQM +
                  LEASE_YRS + STOREY_ORDER +
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_TOPPRISCH + PROX_MALL + 
                  PROX_SPRMKT + PROX_CLINIC + PROX_PHARMACY + NUM_KNDRGTN +
                  NUM_CHILDCARE + NUM_BUS_STOP +
                  NUM_ISP_CLIN + NUM_CHAS_CLIN + NUM_PRI_SCH,
                data=train_data_full)
```

```{r}
#| eval: false
write_rds(resale.mlr, "data/model/resale_lr.rds")
```

```{r}
resale.mlr <- read_rds("data/model/resale_lr.rds")
summary(resale.mlr)
```

The summary of the Multiple Linear Regression Model tells us many information, and as usually I will be using a significance level of 95%.

-   The R-squared of 0.7449 reveals that the simple regression model built is able to explain about 74% of the resale prices.

-   Proximity Mall and Proximity to Clinic has a p-value greater than 0.05, as such we will cannot reject the null hypothesis that mean is a good estimator of *PRICE*. This will allow us to infer that those factors in a multiple linear regression model above is a not good estimator of *PRICE*.

### Building a Better Regression Model

It is clear that not all the independent variables are statistically significant, hence we can revised the model by removing those variables which are not statistically significant.

Now, we are ready to calibrate the revised model. Notice how difficult it is to glimpse information from the above summary table. One Method for us to improve the reporting is to use the `ols_regress()` from the olsrr package. Find out more [here](https://www.rdocumentation.org/packages/olsrr/versions/0.5.3/topics/ols_regress).

```{r}
resale.lr.filtered <- lm(formula = PRICE ~ AREA_SQM +
                  LEASE_YRS + STOREY_ORDER +
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_TOPPRISCH + 
                  PROX_SPRMKT + PROX_PHARMACY + NUM_KNDRGTN +
                  NUM_CHILDCARE + NUM_BUS_STOP +
                  NUM_ISP_CLIN + NUM_CHAS_CLIN + NUM_PRI_SCH,
                 data=train_data_full)

ols_regress(resale.lr.filtered)
```

```{r}
#| eval: false
write_rds(resale.lr.filtered, "data/model/resale_lr_filtered.rds")
```

```{r}
resale.lr.filtered <- read_rds("data/model/resale_lr_filtered.rds")
summary(resale.lr.filtered)
```

The summary of the Multiple Linear Regression Model tells us many information, and as usually I will be using a significance level of 95%.

-   The R-squared of 0.7449 reveals that the simple regression model built is able to explain about 74% of the resale prices.

-   There appears to be almost no change even with the removed factors.

### Checking the Validity of Linear Regression Model

[**Checking of Collinearity**]{.underline}

The `ols_vif_tol()` of **olsrr** package is used to test if there are sign of multi collinearity. Find out more [here](https://olsrr.rsquaredacademy.com/reference/ols_coll_diag.html)

```{r}
ols_vif_tol(resale.lr.filtered)
```

Since the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.

[**Test for Non Linearity**]{.underline}

We need to test the assumption that linearity and additivity of the relationship between dependent and independent variables.

In the code chunk below, the `ols_plot_resid_fit()` of **olsrr** package is used to perform linearity assumption test. Find out more [here](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html).

```{r}
ols_plot_resid_fit(resale.lr.filtered)
```

Since most of the data points are scattered around the 0 line, hence we can conclude that the relationships between the dependent variable and independent variables are linear.

[**Test for Normality**]{.underline}

We can uses `ols_plot_resid_hist()` of *olsrr* package to perform normality assumption test. Find out more [here](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_hist.html).

```{r}
ols_plot_resid_hist(resale.lr.filtered)
```

The figure reveals that the residual of the multiple linear regression model resemble normal distribution.

## Building Geographically Weighted Linear Regression (GWR)

### Preparing the Data

We can once again retrieve all our data to prepare for the fitting of the data. One difference to note is that we are not going to remove the geographically coordinates. We will also need to remove age and month as they are not necessary.

```{r}
train_data_full <- read_rds("data/rds/train_data_full.rds")
test_data_full <- read_rds("data/rds/test_data_full.rds")

train_data_sub <- subset(train_data_full, select = -c(month, AGE))
```

Another method thing that we need to do would be to convert the data into a Spatial Data frame as the Linear Regression need Spatial Data.

This can be done with the `as_Spatial()` function from sf package. Find out more [here](https://www.rdocumentation.org/packages/sf/versions/0.3-4/topics/as_Spatial).

```{r}
train_data_sub_sp <- as_Spatial(train_data_sub)
train_data_sub_sp 
```

### Building the Adaptive BandWidth

There are two type of bandwidth we can calculate for GWR:fixed and adaptive. Both are calculated using, `bw.gwr()` of GWModel package can be used to determine the optimal fixed bandwidth to use in the model. Find out more [here](https://cran.r-project.org/web/packages/GWmodel/GWmodel.pdf).

Since we have learn that the adaptive bandwidth is better than the fixed bandwidth during our take home exercise, I will be using the adaptive bandwidth. The only difference is that ***adaptive*** is set to FALSE for fixed bandwidth

We define the stopping rule using ***approach*** argument. According to the documentaion there are two possible approaches to determine the stopping rule, CV cross-validation approach and AIC corrected (AICc) approach. CV is the default method.

::: callout-note
Calculation of Bandwidth is computational long and intensive. This calculation took me 4 hours to find the optimal bandwidth
:::

```{r}
#| eval: false
bw_adaptive <- bw.gwr(PRICE ~ AREA_SQM +
                  LEASE_YRS + STOREY_ORDER +
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_TOPPRISCH + PROX_MALL + 
                  PROX_SPRMKT + PROX_CLINIC + PROX_PHARMACY + NUM_KNDRGTN +
                  NUM_CHILDCARE + NUM_BUS_STOP +
                  NUM_ISP_CLIN + NUM_CHAS_CLIN + NUM_PRI_SCH,
                  data=train_data_sub_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

We can save it again so that we can use it again, later and there is no need to recalculate

```{r}
#| eval: false
write_rds(bw_adaptive, "data/model/bw_adaptive.rds")
```

### Building a Adaptive bandwidth GWR

We will first load the calculate bandwidth first.

```{r}
bw_adaptive <- read_rds("data/model/bw_adaptive.rds")
print(bw_adaptive)
```

As you can see our bandwidth is 164 meters.

Now, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel. `gwr.basic()` of GWModel package can be used to build the model. Find out more [here](https://cran.r-project.org/web/packages/GWmodel/GWmodel.pdf).

The GWModel does offer other function to build different GWR Model, such as `gwr.bootstrap`. However in this case we are building a simple GWR Model, as the other models have different considerations.

```{r}
#| eval: false
gwr_adaptive <- gwr.basic(formula = PRICE ~ AREA_SQM +
                  LEASE_YRS + STOREY_ORDER +
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_TOPPRISCH + PROX_MALL + 
                  PROX_SPRMKT + PROX_CLINIC + PROX_PHARMACY + NUM_KNDRGTN +
                  NUM_CHILDCARE + NUM_BUS_STOP +
                  NUM_ISP_CLIN + NUM_CHAS_CLIN + NUM_PRI_SCH,
                  data=train_data_sub_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive= TRUE,
                          longlat = FALSE)
```

```{r}
#| eval: false
write_rds(gwr_adaptive, "data/model/gwr_adaptive.rds")
```

We can view the model by reading the RDS package and see its summary.

```{r}
gwr_adaptive <- read_rds("data/model/gwr_adaptive.rds")
gwr_adaptive
```

Based on the following Summary, here are the following summary

-   The R-squared of 0.918 reveals that the simple regression model built is able to explain about 91% of the resale prices.

-   This is significantly higher than the standard OLS Model.

### Building a Better GWR

```{r}
#| eval: false
gwr_adaptive_filtered <- gwr.basic(formula = PRICE ~ AREA_SQM +
                  LEASE_YRS + STOREY_ORDER +
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_TOPPRISCH +
                  PROX_SPRMKT + PROX_PHARMACY + NUM_KNDRGTN +
                  NUM_CHILDCARE + NUM_BUS_STOP +
                  NUM_ISP_CLIN + NUM_CHAS_CLIN + NUM_PRI_SCH,
                  data=train_data_sub_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive= TRUE,
                          longlat = FALSE)
```

```{r}
#| eval: false
write_rds(gwr_adaptive, "data/model/gwr_adaptive_filtered.rds")
```

```{r}
gwr_adaptive_filtered <- read_rds("data/model/gwr_adaptive_filtered.rds")
gwr_adaptive_filtered
```

Based on the following Summary, here are the following summary

-   The R-squared of 0.916 reveals that the simple regression model built is able to explain about 91% of the resale prices.

-   This is significantly higher than the standard OLS Model, however is lower than the original GWR model.

## Building a **Random Forest Model**

### Preparing the Data

We can once again retrieve all our data to prepare for the fitting of the data. One difference to note is that we are not going to remove the geographically coordinates. We will also need to remove age and month as they are not necessary.

::: callout-warning
Random Forest Model is extremely computational intensive, as such a smaller model set was used (6 months)
:::

```{r}
train_data_full <- read_rds("data/rds/train_data_full.rds")
test_data_full <- read_rds("data/rds/test_data_full.rds")

train_data_sub <-  filter(train_data_full, month >= "2022-06" & month <= "2022-12")

train_data_rf <- subset(train_data_sub, select = -c(month, AGE))
test_data_rf <- subset(test_data_full, select = -c(month, AGE))
```

We will first extract all the coordinates data from and writing them into a rds for future used.

```{r}
coords_train <- st_coordinates(train_data_rf)
coords_test <- st_coordinates(test_data_rf)
```

```{r}
#| eval: false
coords_train <- write_rds(coords_train, "data/model/coords_train.rds" )
coords_test <- write_rds(coords_test, "data/model/coords_test.rds" )
```

We can then drop it from the training data.

```{r}
train_data_rf <- train_data_rf %>% 
  st_drop_geometry()
```

```{r}
train_data_rf
```

### Building A Random Forest Model

We can build a random forest using `ranger()` from Ranger package. Find out more [here](https://www.rdocumentation.org/packages/ranger/versions/0.14.1/topics/ranger).

Due to the random nature of Random Forest Model, we will need to set a seed in order to ensure it can be reproduce. One thing to note is that I have set the number of tress to 100. More reason will be explained below.

```{r}
#| eval: false
set.seed(1234)
rf <- ranger(PRICE ~ AREA_SQM +
                  LEASE_YRS + STOREY_ORDER +
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_TOPPRISCH + PROX_MALL + 
                  PROX_SPRMKT + PROX_CLINIC + PROX_PHARMACY + NUM_KNDRGTN +
                  NUM_CHILDCARE + NUM_BUS_STOP +
                  NUM_ISP_CLIN + NUM_CHAS_CLIN + NUM_PRI_SCH,
             num.trees = 100,
             data=train_data_rf)
```

```{r}
#| eval: false
write_rds(rf, "data/model/random_forest.rds")
```

```{r}
rf <- read_rds("data/model/random_forest.rds")
rf
```

Based on the following Summary, here are the following summary

-   The R-squared of 0.941 reveals that the simple regression model built is able to explain about 94% of the resale prices.

-   This is significantly higher than the standard OLS Model and GWR model.

## Building A GeoGraphically Weighted Random Forest

### Preparing the data

We can once again retrieve all our data to prepare for the fitting of the data. We will also need to remove age and month as they are not necessary. All steps are the same as above.

```{r}
#| eval: false
train_data_full <- read_rds("data/rds/train_data_full.rds")
test_data_full <- read_rds("data/rds/test_data_full.rds")

train_data_sub <-  filter(train_data_full, month >= "2022-06" & month <= "2022-12")

train_data_rf <- subset(train_data_sub, select = -c(month, AGE))
test_data_rf <- subset(test_data_full, select = -c(month, AGE))

train_data_rf <- train_data_rf %>% 
  st_drop_geometry()
```

### Building Adaptive Bandwidth

There are two type of bandwidth we can calculate for Random Forest and adaptive. Both are calculated using, `grf.bw()` of SpatialML package can be used to determine the optimal fixed bandwidth to use in the model. Find out more [here](https://cran.r-project.org/web/packages/SpatialML/SpatialML.pdf).

For it to be adaptive we need to set the kernel to "adaptive".

::: callout-warning
This calculation is computationally Intensive and it took about 12 hours to gain a result. The number of trees is set at 100 to reduce the computational time.
:::

```{r}
#| eval: false
bw_grf <- grf.bw(formula = PRICE ~ AREA_SQM +
                  LEASE_YRS + STOREY_ORDER +
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_TOPPRISCH + PROX_MALL + 
                  PROX_SPRMKT + PROX_CLINIC + PROX_PHARMACY + NUM_KNDRGTN +
                  NUM_CHILDCARE + NUM_BUS_STOP +
                  NUM_ISP_CLIN + NUM_CHAS_CLIN + NUM_PRI_SCH,
                  data=train_data_rf,
                  kernel="adaptive",
                  trees = 100,
                  weighted = TRUE,
                  coords=coords_train)
```

We can save it again so that we can use it again, later and there is no need to recalculate

```{r}
#| eval: false
write_rds(bw_grf, "data/model/gwr_bw_adaptive.rds")
```

### Building An Adaptive random Forest

We will first load our calculated bandwidth first

```{r}
bw_grf <- read_rds("data/model/gwr_bw_adaptive.rds")
print(bw_grf)
```

As you can see our bandwidth is 356 meters.

Due to the random nature of Random Forest Model, we will need to set a seed in order to ensure it can be reproduce.

Now, we can go ahead to calibrate the geographically weighted random forest pricing model by using adaptive bandwidth. `grf()` of SpatialML package can be used to determine the optimal fixed bandwidth to use in the model. Find out more [here](https://cran.r-project.org/web/packages/SpatialML/SpatialML.pdf).

The SpatialML currently offer only one function to build different GWR Model.

::: callout-warning
The building of model will take some time as well. This is shorter than calculating bandwidth but will still take time.
:::

```{r}
#| eval: false
set.seed(1234)
gwRF_adaptive <- grf(formula = PRICE ~ AREA_SQM +
                  LEASE_YRS + STOREY_ORDER +
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_TOPPRISCH + PROX_MALL + 
                  PROX_SPRMKT + PROX_CLINIC + PROX_PHARMACY + NUM_KNDRGTN +
                  NUM_CHILDCARE + NUM_BUS_STOP +
                  NUM_ISP_CLIN + NUM_CHAS_CLIN + NUM_PRI_SCH,
                     dframe=train_data_rf, 
                     bw=bw_grf,
                    ntree = 100,
                     kernel="adaptive",
                     coords=coords_train)
```

We can save the model and used it later.

```{r}
#| eval: false
write_rds(gwRF_adaptive, "data/model/gwRF_adaptive.rds")
```

Lets take a look at it

```{r}
#| eval: false
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

::: callout-warning
Viewing it directly might result in RStudio Hanging.
:::

```{r}
#| eval: false
gwRF_adaptive
```

![](images/Screenshot%202023-03-26%20at%202.32.13%20PM.png)

Based on the following Summary, here are the following summary

-   The R-squared of 0.944 reveals that the simple regression model built is able to explain about 94% of the resale prices.

-   This is significantly higher than the standard OLS Model and GWR model, however these model seems to be slightly better than the standard Random Forest Methods.

### Why not 500 Trees and Full Data

There is no significant issues running in the standard Random Forest but when used on the GRF, it results in the following error.

![](images/Screenshot%202023-03-26%20at%209.57.44%20AM-01.png)

Which mean that my computer is unable to handle the tree due to it being out of memory.

The issue with using the full data set is that it takes too long. For my computer, just running the calculation of the bandwidth and generating the model takes 12 hours and above and it is still running.

# Predicting the Data set

## Geographically Weighted Regression Forest

### Preparing the Data and the Model

We will first load the model first

```{r}
#| eval: false
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

We will then be used to combine the test data with its corresponding coordinates data.

```{r}
#| eval: false
test_data_full <- read_rds("data/rds/test_data_full.rds")
coords_test <- read_rds("data/model/coords_test.rds")

test_data_rf <- subset(test_data_full, select = -c(month, AGE))

test_data_rf <- cbind(test_data_rf, coords_test) %>%
  st_drop_geometry()
```

### Predicting the Values

`predict.grf()` of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier. Find out more [here](https://cran.r-project.org/web/packages/SpatialML/SpatialML.pdf).

```{r}
#| eval: false
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data_rf, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

We will save the file for future use

```{r}
#| eval: false
GRF_pred <- write_rds(gwRF_pred, "data/model/GRF_pred.rds")
```

## Random Forest Regression

### Preparing the Data and Model

We can prepare the data for Random Forest. One difference is that there is no need for the coordinates data.

```{r}
#| eval: false
rf <- read_rds("data/model/random_forest.rds")
test_data_full <- read_rds("data/rds/test_data_full.rds")

test_data_rf <- test_data_full %>% 
  st_drop_geometry()
```

### Predicting the Values

We can make use of `predict()` from Ranger to help us predict make the necessary prediction. Find out more [here](https://www.rdocumentation.org/packages/ranger/versions/0.14.1/topics/predict.ranger).

```{r}
#| eval: false
rf_pred <- predict(rf, test_data_rf)
```

```{r}
#| eval: false
write_rds(rf_pred$predictions, "data/model/rf_pred.rds")
```

## Simple Multiple Linear Regression

### Preparing the Data and Model

We can prepare the data for Simple Linear Regression Model. While we have prepared 2 models, we will be using the model with the higher MSE, **resale_lr_filtered.** The model with the low probability factor removed.

```{r}
#| eval: false
mlr <- read_rds("data/model/resale_lr_filtered.rds")
test_data_full <- read_rds("data/rds/test_data_full.rds")

test_data_rf <- subset(test_data_full, select = -c(month, AGE)) %>%
  st_drop_geometry()
```

### Predicting the Values

We can make use of `predict()` from stats to help us predict make the necessary prediction. Find out more [here](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/predict.lm).

As the predict function is similar in name to the Random Forest. We will need to indicate that the predict function we are using should be from stats.

```{r}
#| eval: false
mlr_pred <- stats::predict(mlr, test_data_rf)
```

```{r}
#| eval: false
write_rds(mlr_pred, "data/model/mlr_pred.rds")
```

## Geographically Weighted Linear Regression

### Preparing the Data

We can prepare the data for Simple Linear Regression Model. While we have prepared 2 models, we will be using the model with the higher MSE, **gwr_adaptive.** The model without the low probability factor removed.

For GWR, we would need to convert the test data into a Spatial Data frame. We also need to load the bandwidth as well.

```{r}
#| eval: false
test_data_full <- read_rds("data/rds/test_data_full.rds")

test_data_sub <- subset(test_data_full, select = -c(month, AGE))
test_data_sub_sp <- as_Spatial(test_data_sub)

```

### Predicting the Values

GWRModal does not have a predict function for a modal that is build. However, they have a built in `gwr.predict()`to built the model and predict at the same time. After numerous tries, I was not able to find out why there is a "No Regression Point" error.

What I found out was that if the training data is the same size as the test data, it will result in an output. Hence, Data from the test data frame is random sampled.

```{r}
#| eval: false
train_data_full <- read_rds("data/rds/train_data_full.rds")

train_data_sub <- subset(train_data_full, select = -c(month, AGE))
set.seed(1234)
train_data_sub <- sample_n(train_data_sub, 1846)
train_data_sub_sp <- as_Spatial(train_data_sub)
```

Once the training data has been sampled, it will be feed into `gwr.predict()`to built the model. Find out more [here](https://www.rdocumentation.org/packages/GWmodel/versions/2.2-9/topics/gwr.predict).

We need to provide the training data in the **predictdata**, and that is the only difference as compared to the linear regression model above.

```{r}
#| eval: false
gwr_predict <- gwr.predict(formula = PRICE ~ AREA_SQM +
                  LEASE_YRS + STOREY_ORDER +
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_TOPPRISCH + PROX_MALL + 
                  PROX_SPRMKT + PROX_CLINIC + PROX_PHARMACY + NUM_KNDRGTN +
                  NUM_CHILDCARE + NUM_BUS_STOP +
                  NUM_ISP_CLIN + NUM_CHAS_CLIN + NUM_PRI_SCH,
                  data = train_data_sub_sp,
                  predictdata = test_data_sub_sp,
                          bw = bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive= TRUE,
                          longlat = FALSE)
```

The prediction can be found in the following prediction portion of the dataframe.

```{r}
#| eval: false
gwr_predict_values <- gwr_predict$SDF$prediction
```

We will save both for use later

```{r}
#| eval: false
write_rds(gwr_predict, "data/model/gwr_pred_model.rds")
write_rds(gwr_predict_values, "data/model/gwr_pred.rds")
```

# Comparing Models

## Preparing the Data

We have save all the models out put as a vector value, so we will convert them into a data frame for further visualization and analysis.

We will read all the predicted value and convert them into a data frame

```{r}
#| eval: false
GRF_pred <- read_rds("data/model/GRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)

RF_pred <- read_rds("data/model/rf_pred.rds")
RF_pred_df <- as.data.frame(RF_pred)

MLR_pred <- read_rds("data/model/mlr_pred.rds")
MLR_pred_df <- as.data.frame(MLR_pred)

GWR_pred <- read_rds("data/model/gwr_pred.rds")
GWR_pred_df <- as.data.frame(GWR_pred)
```

We will load the test data again and merge them into one data frame for comparison as well.

```{r}
#| eval: false
test_data_full <- read_rds("data/rds/test_data_full.rds")
```

WE can use `cbind()` from Base R is used to append the predicted values onto test_data. Find out more [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/cbind).

```{r}
#| eval: false
test_data_predicted <- cbind(test_data_full, GRF_pred_df, RF_pred_df, MLR_pred_df,GWR_pred_df)
```

We can save it again to be used later

```{r}
#| eval: false
write_rds(test_data_predicted, "data/model/test_data_predicted.rds")
```

## Calculating the Measures

```{r}
test_data_predicted <- read_rds("data/model/test_data_predicted.rds")
```

One popular method of calculating the accuracy of the predicted model is to calculate the Root Mean Square or RMSE.

It allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, `postResample()` of Carat package is used to compute the RMSE and other factors. Find out more [here](https://topepo.github.io/caret/measuring-performance.html).

[**Geographically Weighted Random Forest**]{.underline}

```{r}
postResample(pred = test_data_predicted$GRF_pred, obs = test_data_predicted$PRICE)
```

[**Geographically Weighted Linear Regression**]{.underline}

```{r}
postResample(pred = test_data_predicted$GWR_pred, obs = test_data_predicted$PRICE)
```

[**Multiple Linear Regression**]{.underline}

```{r}
postResample(pred = test_data_predicted$MLR_pred, obs = test_data_predicted$PRICE)
```

[**Random Forest**]{.underline}

```{r}
postResample(pred = test_data_predicted$RF_pred, obs = test_data_predicted$PRICE)
```

## Viewing the Predicted Value

We can view the scatter plot of the predicted price against the actual price to find any outliers.

[**Geographically Weighted Random Forest**]{.underline}

```{r}
ggplotly(ggplot(data = test_data_predicted,
       aes(x = GRF_pred,
           y = PRICE)) +
  geom_point())
```

The graph points appears to be largely close to a diagonal line and there seems to be no significant outlier for Geographically Weighted Random Forest

[**Geographically Weighted Linear Regression**]{.underline}

```{r}
ggplotly(ggplot(data = test_data_predicted,
       aes(x = GWR_pred,
           y = PRICE)) +
  geom_point())
```

The graph points appears to be moderately close to a diagonal line and there seems to be no significant outlier for Geographically Weighted Linear Regression

[**Multiple Linear Regression**]{.underline}

```{r}
ggplotly(ggplot(data = test_data_predicted,
       aes(x = MLR_pred,
           y = PRICE)) +
  geom_point())
```

The graph points appears to be moderately close to a diagonal line and there seems to be no significant outlier for Multiple Linear Regression

[**Random Forest**]{.underline}

```{r}
ggplotly(ggplot(data = test_data_predicted,
       aes(x = RF_pred,
           y = PRICE)) +
  geom_point())
```

The graph points appears to be largely close to a diagonal line and there seems to be no significant outlier for Geographically Weighted Linear Regression

## Model Comparison

### Overview

::: callout-important
Geographically Weighted Regression has an overall smaller data set as compared to that of the multiple linear regression as I am not able to get the model to take in the entire training data.
:::

| Model                                              | RMSE  | R2    | MAE   |
|----------------------------------------------------|-------|-------|-------|
| Geographically Weighted Random Forest              | 35353 | 0.937 | 25209 |
| Random Forest                                      | 35682 | 0.943 | 24969 |
| Geographically Weighted Multiple Linear Regression | 70786 | 0.895 | 57959 |
| Multiple Linear Regression                         | 80393 | 0.808 | 64698 |

### Metrics

::: callout-note
RMSE and R2 interpretation is taken from [here](https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/#:~:text=Lower%20values%20of%20RMSE%20indicate,than%20one%20are%20often%20useful.).

MAE interpretation is taken from [here](https://www.sciencedirect.com/topics/engineering/mean-absolute-error#:~:text=The%20MAE%20score%20is%20measured,positive%20when%20calculating%20the%20MAE.).
:::

The following factors as indicated are evaluated as followed

-   **RMSE:** Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. The lower the value the better the model. The lower values of RMSE the better fit is. It's the most important criterion for fit if the main purpose of the model is prediction.

-   **R2:** It is the proportional improvement in prediction from the regression model, compared to the mean model. It indicates the goodness of fit of the model. The Higher the value the better it is.

-   **MAE:** Mean Absolute Error. In MAE, different errors are not weighted more or less, but the scores increase linearly with the increase in errors. The lower the value the better the model is.

## Explanation

Based on all the metrics indicated above, there are 2 main insights that we can gather from the results.

### Geographically Weighted Model Perform Better

Looking at the comparison between the 2 Models Group, we can see that Geographically Weighted Regression Random Forest outperforms that of Random Forest in RMSE but losing out in R2 and MAE. Unlike RMSE, MAE does not penalised large difference as compared to RMSE, but rather penalized based on the amount of error. We can infer that the Geographically Weighted Random Forest make more errors but they are of a smaller value as compared to Random Forest. Since RMSE is most important factor in prediction model, and the difference in the other factors are not too large, I can conclude that Geographically Weighted Regression Random Forest perform better.

The Geographically Weighted Regression perform better than the multiple linear regression in almost all metrics despite having only a smaller training data set.

### Random Forest Model Perform Better than Linear Regression Model.

Overall when looking at the metrics, We can conclude relatively easily that Random Forest out perform Linear Regression in almost all metric. In fact it should be noted that even Random Forest outperforms all the model in linear regression. The RMSE and MAE of the Random Forest Model is estimated to be about half of that of the Linear Regression Models, with the R2 metrics having a difference of 4%.

We can conclude easily that Random Forest Model are the prefered model in prediction.

## Overall

The best Model for prediction would be GeoGraphically Weighted Random Forest.

# Conclusion

Overall, we have evaluated the following.

-   Structural factors

    -   Area of the unit

    -   Floor level

    -   Remaining lease

    -   Age of the unit

-   Locational factors

    -   Proxomity to CBD

    -   Proximity to Childcare

    -   Proximity to eldercare

    -   Proximity to hawker centres

    -   Proximity to MRT

    -   Proximity to park

    -   Proximity to good primary school

    -   Proximity to shopping mall

    -   Proximity to supermarket

    -   Numbers of kindergartens within 350m

    -   Numbers of childcare centres within 350m

    -   Numbers of bus stop within 350m

    -   Numbers of primary school within 1km

-   Self Sourced Factors

    -   Number of ISP Clinic

    -   Proximity to ISP Clinic

    -   Number of Pharmacy

    -   Proximity to Pharmacy

The factors listed above are just some of the factors that I am able to find, most of the data seems to be taken down from Megan's time, which makes sourcing for information even more difficult than it already is. Furthermore, the preliminary work take the most time, and computing takes even more time than I expected, which make me better appreciate how much work it takes to clean a data set for it to be usable.

While I have concluded that the best Model for prediction would be Geographically Weighted Random Forest, I cannot say for certain that there are no other factors that will lead to the other models being better. Furthermore, due to certain limitations, I am unable to generate the proper geographically weighted linear regression model, which might have the potential to outperform the Random Forest Methods.

While this model can predict the pricing of the houses, the ever changing landscape might render it obsolete in the near future. I'm excited to see what other factors can be included and see a better model.

# Special Thanks

Special Thanks to Professor Kam for his guidance and help in this exercise.
