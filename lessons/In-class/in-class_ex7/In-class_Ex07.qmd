---
title: "In Class Exercise 7"
author: "Hao Xian"
date: "20 February 2023"
date-modified: '`r Sys.Date()`'
format: html
number-sections: true
execute: 
  echo: true
  eval: true
  message: false
  warning: false
  
editor: visual
---

::: callout-important
We will be using sfdep in In-Class-Excercise instead of spded in Hands On Excercise
:::

# Loading the packages

```{r}
pacman::p_load(sf, tmap, sfdep, tidyverse)
```

# The data

## Importing the Geospatial data

::: callout-note
This file is loaded as a sf dataframe
:::

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

## Importing GeoSpatial

::: callout-note
This input is recorded as tibble dataframe
:::

```{r}
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

```{r}
```

### Combining both data frame by using left join

::: callout-note
In order to retain the geospatial properties, the left data frame must be the sf data frame.

We need to make sure that the name is the same, it is Case-Sensitive!!
:::

```{r}
hunan_GDPPC <- left_join(hunan,hunan2012)%>%
  select(1:4, 7, 15)
```

### Plotting a Choropleth map

```{r}
tmap_mode("plot") 
tm_shape(hunan_GDPPC) +
  tm_fill("GDPPC",
          style = "quantile",
          palette = "Blues",
          title = "GDPPC") +
  tm_layout(main.title = "Distribution of GDP per capita by district, Hunan Province", 
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45,
            legend.width = 0.35,
            frame = TRUE) +
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha = 0.2)
```

## Identify area Neighbours

### Calculating Contiguit neighbour methods

::: callout-note
The code junk below uses st_contiguity is used to derive a contiguity neighbour list using Queen's method.

The code chunk below will store the results into a new sf data frame at the first column (.before = 1\_
:::

### Queens Methods

```{r}
nb_queen <- hunan_GDPPC %>% 
  mutate(nb = st_contiguity(geometry),
         .before = 1)
```

```{r}
summary(nb_queen$nb)
```

We can view the first 10 values

```{r}
nb_queen
```

Lets review the name of the neighbouring polygons

```{r}
nb_queen$County[c(2,3,4,57,85)]
```

### Rooks Methods

```{r}
nb_rook <- hunan_GDPPC %>% 
  mutate(nb = st_contiguity(geometry,
                            queen = FALSE),
         .before = 1)
```

### Indentifying Higher order neighbours

```{r}
nb2_queen <-  hunan_GDPPC %>% 
  mutate(nb = st_contiguity(geometry),
         nb2 = st_nb_lag_cumul(nb, 2),
         .before = 1)
```

```{r}
nb2_queen
```

## Computing contiguity weights

### Queens Method

```{r}
wm_q <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb,
                         style = "W"),
         .before = 1) 
```

```{r}
wm_q
```

## Computing Global Moran's I

::: callout-note
We usually do not perform a Moran Step by itself, we usually perform the global Moran Test
:::

```{r}
moranI <- global_moran(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)
```

## Performing Global Moran'I test

```{r}
global_moran_test(wm_q$GDPPC,
                  wm_q$nb,
                  wm_q$wt)
```

We have enough statistical evidence to reject the null hypothesis that the GDP is spatially independence. If we refer to the Moran I value is positive, therefore it shows signs that is it 00.

### Peroforming Global Moran's I permutation Test

::: callout-note
Must perform Set seed to ensure reproducibility.
:::

```{r}
set.seed(1234)
```

If the dataset is small, we can increase teh number of simulation for stability.

```{r}
global_moran_perm(wm_q$GDPPC,
                  wm_q$nb,
                  wm_q$wt,
                  nsim = 99)
```

## Computing Local Moran's I

Without the unnest, if you tried to plot it, we will not be able to plot it.

```{r}
lisa <- wm_q %>%
  mutate(local_moran = local_moran(GDPPC,nb,wt,nsim=99),
         .before = 1) %>%
  unnest(local_moran)
lisa
```

In General the Mean and the Pysal is the same, Use either Mean or Pysal. If we are using R, we use Mean. Pysal is the python method of processing.

It is the safest if we use mean. Median might be a better measure to use if the data does not follow a normal distribution.

## Visualising Local Morans' I

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("ii") +
  tm_borders(alpha = 0.5) + 
  tm_view(set.zoom.limits=c(6,8))
```

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("p_ii_sim") +
  tm_borders(alpha = 0.5) + 
  tm_view(set.zoom.limits=c(6,8))
```

We should use p_ii_sim or p_folded_sim, to plot the simulation, as those values are where the simulation are used.

```{r}
lisa_sig <- lisa %>%
  filter(p_ii <0.05)
```

This is not a good way to do so, can make use of the Hands-On Exercise Code to show how the grey area as insignificant areas.

There is no need to use LISA.

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
  tm_shape(lisa_sig) +
  tm_fill("mean") +
  tm_borders(alpha=0.4)
```

# Hot Spot and Cold Sppot Area Analysis

In general we will be using GStar

```{r}
HCSA <- wm_q %>%
  mutate(local_Gi = local_gstar_perm(GDPPC, nb, wt, nsim =99),
         .before = 1) %>%
    unnest(local_Gi)

HCSA
```

```{r}
tmap_mode("plot")
tm_shape(HCSA) +
  tm_fill("gi_star") +
  tm_borders(alpha = 0.5) + 
  tm_view(set.zoom.limits=c(6,8))
```

This map might not be that useful, we want to see those that are smaller 0.05. It is better to plot significant levels

```{r}
tmap_mode("plot")
tm_shape(HCSA) +
  tm_fill("p_sim") +
  tm_borders(alpha = 0.5) + 
  tm_view(set.zoom.limits=c(6,8))
```

## Peroforming Emerging Hotspot Analysis

We need to arrange to something similar to the Hunan_GDPPC for performing Emerging Hotspot Analysis.

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, tidyverse, zoo, Kendall)
```

## Importing the Darta

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

```{r}
GDPPC_st <- spacetime(GDPPC, hunan,
                      .loc_col = "County",
                      .time_col = "Year")
```

### Analysis

```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt=st_weights(nb)
         ) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

```{r}
gi_stars <-GDPPC_nb %>%
          group_by(Year) %>%
  mutate(gi_star = local_gstar_perm(GDPPC, nb, wt, nsim=99)) %>%
  tidyr::unnest(gi_star)
```

## Mann_Kendall Test

```{r}

cbg <- gi_stars %>%
  ungroup() %>%
  filter(County=="Changsha") |>
  select(County, Year, gi_star)
```

```{r}
ehsa <- emerging_hotspot_analysis(
  x=GDPPC_st,
  .var = "GDPPC",
  k =1,
  nsim = 99
)
```
